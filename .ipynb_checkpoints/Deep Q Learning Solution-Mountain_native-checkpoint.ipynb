{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from reinforcementlearning.lib import plotting\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_ACTIONS = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(2,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[2], dtype=tf.uint8)\n",
    "            self.output = self.input_state\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "\n",
    "        # Placeholders for our input\n",
    "        # Our input are 4 grayscale frames of shape 84, 84 each\n",
    "        self.X_pl = tf.placeholder(shape=[None, 2], dtype=tf.float32, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # Fully connected layers\n",
    "        fc1 = tf.contrib.layers.fully_connected(self.X_pl, 16)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        # Calculate the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", self.loss),\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, 4, 84, 84, 1]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "\n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, 4, 84, 84, 1]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.08420691 0.23392573]\n",
      " [0.         0.08420691 0.23392573]]\n",
      "96.84958\n"
     ]
    }
   ],
   "source": [
    "# For Testing....\n",
    "\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "e = Estimator(scope=\"test\")\n",
    "sp = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Example observation batch\n",
    "    observation = env.reset()\n",
    "    \n",
    "    \n",
    "    observations = np.array([observation] * 2)\n",
    "    \n",
    "    \n",
    "    # Test Prediction\n",
    "    print(e.predict(sess, observations))\n",
    "\n",
    "    # Test training step\n",
    "    y = np.array([10.0, 10.0])\n",
    "    a = np.array([1, 2])\n",
    "    print(e.update(sess, observations, a, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParametersCopier():\n",
    "    \"\"\"\n",
    "    Copy model parameters of one estimator to another.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, estimator1, estimator2):\n",
    "        \"\"\"\n",
    "        Defines copy-work operation graph.  \n",
    "        Args:\n",
    "          estimator1: Estimator to copy the paramters from\n",
    "          estimator2: Estimator to copy the parameters to\n",
    "        \"\"\"\n",
    "        e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "        e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "        e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "        e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "        self.update_ops = []\n",
    "        for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "            op = e2_v.assign(e1_v)\n",
    "            self.update_ops.append(op)\n",
    "            \n",
    "    def make(self, sess):\n",
    "        \"\"\"\n",
    "        Makes copy.\n",
    "        Args:\n",
    "            sess: Tensorflow session instance\n",
    "        \"\"\"\n",
    "        sess.run(self.update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Gamma discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "    \n",
    "    # Make model copier object\n",
    "    estimator_copy = ModelParametersCopier(q_estimator, target_estimator)\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "    \n",
    "    # For 'system/' summaries, usefull to check if currrent process looks healthy\n",
    "    current_process = psutil.Process()\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "\n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator,\n",
    "        len(VALID_ACTIONS))\n",
    "\n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    #state = state_processor.process(sess, state)\n",
    "    #state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "        #next_state = state_processor.process(sess, next_state)\n",
    "        #next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            #state = state_processor.process(sess, state)\n",
    "            #state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        #state = state_processor.process(sess, state)\n",
    "        #state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "\n",
    "            # Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                estimator_copy.make(sess)\n",
    "                print(\"\\nCopied model parameters to target network.\")\n",
    "\n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Take a step\n",
    "            action_probs = policy(sess, state, epsilon)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "            #next_state = state_processor.process(sess, next_state)\n",
    "            #next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "\n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            # Save transition to replay memory\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            # Sample a minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "\n",
    "            # Calculate q values and targets\n",
    "            q_values_next = target_estimator.predict(sess, next_states_batch)\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n",
    "\n",
    "            # Perform gradient descent update\n",
    "            states_batch = np.array(states_batch)\n",
    "            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=epsilon, tag=\"episode/epsilon\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], tag=\"episode/reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], tag=\"episode/length\")\n",
    "        episode_summary.value.add(simple_value=current_process.cpu_percent(), tag=\"system/cpu_usage_percent\")\n",
    "        episode_summary.value.add(simple_value=current_process.memory_percent(memtype=\"vms\"), tag=\"system/v_memeory_usage_percent\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, i_episode)\n",
    "        q_estimator.summary_writer.flush()\n",
    "        \n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay memory...\n",
      "\n",
      "Copied model parameters to target network.\n",
      "Step 199 (199) @ Episode 1/10000, loss: 0.9057908654212952\n",
      "Episode Reward: -200.0\n",
      "Step 199 (398) @ Episode 2/10000, loss: 0.9035135507583618\n",
      "Episode Reward: -200.0\n",
      "Step 199 (597) @ Episode 3/10000, loss: 0.9027080535888672\n",
      "Episode Reward: -200.0\n",
      "Step 199 (796) @ Episode 4/10000, loss: 0.9017006754875183\n",
      "Episode Reward: -200.0\n",
      "Step 199 (995) @ Episode 5/10000, loss: 0.9018107652664185\n",
      "Episode Reward: -200.0\n",
      "Step 199 (1194) @ Episode 6/10000, loss: 0.9020122289657593\n",
      "Episode Reward: -200.0\n",
      "Step 199 (1393) @ Episode 7/10000, loss: 0.8998410105705261\n",
      "Episode Reward: -200.0\n",
      "Step 199 (1592) @ Episode 8/10000, loss: 0.9016237258911133\n",
      "Episode Reward: -200.0\n",
      "Step 199 (1791) @ Episode 9/10000, loss: 0.8977310657501221\n",
      "Episode Reward: -200.0\n",
      "Step 199 (1990) @ Episode 10/10000, loss: 0.9024043083190918\n",
      "Episode Reward: -200.0\n",
      "Step 199 (2189) @ Episode 11/10000, loss: 0.9004780054092407\n",
      "Episode Reward: -200.0\n",
      "Step 199 (2388) @ Episode 12/10000, loss: 0.9006664752960205\n",
      "Episode Reward: -200.0\n",
      "Step 199 (2587) @ Episode 13/10000, loss: 0.9003483653068542\n",
      "Episode Reward: -200.0\n",
      "Step 199 (2786) @ Episode 14/10000, loss: 0.8970559835433965\n",
      "Episode Reward: -200.0\n",
      "Step 199 (2985) @ Episode 15/10000, loss: 0.9007972478866577\n",
      "Episode Reward: -200.0\n",
      "Step 199 (3184) @ Episode 16/10000, loss: 0.8994684219360352\n",
      "Episode Reward: -200.0\n",
      "Step 199 (3383) @ Episode 17/10000, loss: 0.8979750275611877\n",
      "Episode Reward: -200.0\n",
      "Step 199 (3582) @ Episode 18/10000, loss: 0.9028854370117188\n",
      "Episode Reward: -200.0\n",
      "Step 199 (3781) @ Episode 19/10000, loss: 0.9047565460205078\n",
      "Episode Reward: -200.0\n",
      "Step 199 (3980) @ Episode 20/10000, loss: 0.9001671075820923\n",
      "Episode Reward: -200.0\n",
      "Step 199 (4179) @ Episode 21/10000, loss: 0.8988732099533081\n",
      "Episode Reward: -200.0\n",
      "Step 199 (4378) @ Episode 22/10000, loss: 0.8984900712966919\n",
      "Episode Reward: -200.0\n",
      "Step 199 (4577) @ Episode 23/10000, loss: 0.9000042676925659\n",
      "Episode Reward: -200.0\n",
      "Step 199 (4776) @ Episode 24/10000, loss: 0.9033117294311523\n",
      "Episode Reward: -200.0\n",
      "Step 199 (4975) @ Episode 25/10000, loss: 0.9046691656112671\n",
      "Episode Reward: -200.0\n",
      "Step 199 (5174) @ Episode 26/10000, loss: 0.9001376628875732\n",
      "Episode Reward: -200.0\n",
      "Step 199 (5373) @ Episode 27/10000, loss: 0.9000421762466431\n",
      "Episode Reward: -200.0\n",
      "Step 199 (5572) @ Episode 28/10000, loss: 0.8997120261192322\n",
      "Episode Reward: -200.0\n",
      "Step 199 (5771) @ Episode 29/10000, loss: 0.9041022062301636\n",
      "Episode Reward: -200.0\n",
      "Step 199 (5970) @ Episode 30/10000, loss: 0.8965564966201782\n",
      "Episode Reward: -200.0\n",
      "Step 199 (6169) @ Episode 31/10000, loss: 0.9004226922988892\n",
      "Episode Reward: -200.0\n",
      "Step 199 (6368) @ Episode 32/10000, loss: 0.9027763605117798\n",
      "Episode Reward: -200.0\n",
      "Step 199 (6567) @ Episode 33/10000, loss: 0.9040232896804817\n",
      "Episode Reward: -200.0\n",
      "Step 199 (6766) @ Episode 34/10000, loss: 0.9017660021781921\n",
      "Episode Reward: -200.0\n",
      "Step 199 (6965) @ Episode 35/10000, loss: 0.9020206928253174\n",
      "Episode Reward: -200.0\n",
      "Step 199 (7164) @ Episode 36/10000, loss: 0.9001779556274414\n",
      "Episode Reward: -200.0\n",
      "Step 199 (7363) @ Episode 37/10000, loss: 0.9032377004623413\n",
      "Episode Reward: -200.0\n",
      "Step 199 (7562) @ Episode 38/10000, loss: 0.9021538496017456\n",
      "Episode Reward: -200.0\n",
      "Step 199 (7761) @ Episode 39/10000, loss: 0.9010540246963501\n",
      "Episode Reward: -200.0\n",
      "Step 199 (7960) @ Episode 40/10000, loss: 0.9012411832809448\n",
      "Episode Reward: -200.0\n",
      "Step 199 (8159) @ Episode 41/10000, loss: 0.8974486589431763\n",
      "Episode Reward: -200.0\n",
      "Step 199 (8358) @ Episode 42/10000, loss: 0.9026523232460022\n",
      "Episode Reward: -200.0\n",
      "Step 199 (8557) @ Episode 43/10000, loss: 0.8992822170257568\n",
      "Episode Reward: -200.0\n",
      "Step 199 (8756) @ Episode 44/10000, loss: 0.9031738638877869\n",
      "Episode Reward: -200.0\n",
      "Step 199 (8955) @ Episode 45/10000, loss: 0.9042437076568604\n",
      "Episode Reward: -200.0\n",
      "Step 199 (9154) @ Episode 46/10000, loss: 0.9027434587478638\n",
      "Episode Reward: -200.0\n",
      "Step 199 (9353) @ Episode 47/10000, loss: 0.9014639854431152\n",
      "Episode Reward: -200.0\n",
      "Step 199 (9552) @ Episode 48/10000, loss: 0.9028779268264774\n",
      "Episode Reward: -200.0\n",
      "Step 199 (9751) @ Episode 49/10000, loss: 0.9031024575233465\n",
      "Episode Reward: -200.0\n",
      "Step 199 (9950) @ Episode 50/10000, loss: 0.8996371030807495\n",
      "Episode Reward: -200.0\n",
      "Step 49 (9999) @ Episode 51/10000, loss: 0.9040186405181885\n",
      "Copied model parameters to target network.\n",
      "Step 199 (10149) @ Episode 51/10000, loss: 1.077870345115662\n",
      "Episode Reward: -200.0\n",
      "Step 199 (10348) @ Episode 52/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (10547) @ Episode 53/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (10746) @ Episode 54/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (10945) @ Episode 55/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (11144) @ Episode 56/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (11343) @ Episode 57/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (11542) @ Episode 58/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (11741) @ Episode 59/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (11940) @ Episode 60/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (12139) @ Episode 61/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (12338) @ Episode 62/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (12537) @ Episode 63/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (12736) @ Episode 64/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (12935) @ Episode 65/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (13134) @ Episode 66/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (13333) @ Episode 67/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (13532) @ Episode 68/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (13731) @ Episode 69/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (13930) @ Episode 70/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (14129) @ Episode 71/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (14328) @ Episode 72/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (14527) @ Episode 73/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (14726) @ Episode 74/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (14925) @ Episode 75/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (15124) @ Episode 76/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (15323) @ Episode 77/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (15522) @ Episode 78/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (15721) @ Episode 79/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (15920) @ Episode 80/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (16119) @ Episode 81/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (16318) @ Episode 82/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (16517) @ Episode 83/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (16716) @ Episode 84/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (16915) @ Episode 85/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (17114) @ Episode 86/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (17313) @ Episode 87/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (17512) @ Episode 88/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (17711) @ Episode 89/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (17910) @ Episode 90/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (18109) @ Episode 91/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (18308) @ Episode 92/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (18507) @ Episode 93/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (18706) @ Episode 94/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (18905) @ Episode 95/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (19104) @ Episode 96/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (19303) @ Episode 97/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (19502) @ Episode 98/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (19701) @ Episode 99/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (19900) @ Episode 100/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 99 (19999) @ Episode 101/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 199 (20099) @ Episode 101/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (20298) @ Episode 102/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (20497) @ Episode 103/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 199 (20696) @ Episode 104/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (20895) @ Episode 105/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (21094) @ Episode 106/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (21293) @ Episode 107/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (21492) @ Episode 108/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (21691) @ Episode 109/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (21890) @ Episode 110/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (22089) @ Episode 111/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (22288) @ Episode 112/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (22487) @ Episode 113/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (22686) @ Episode 114/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (22885) @ Episode 115/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (23084) @ Episode 116/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (23283) @ Episode 117/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (23482) @ Episode 118/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (23681) @ Episode 119/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (23880) @ Episode 120/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (24079) @ Episode 121/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (24278) @ Episode 122/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (24477) @ Episode 123/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (24676) @ Episode 124/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (24875) @ Episode 125/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (25074) @ Episode 126/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (25273) @ Episode 127/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (25472) @ Episode 128/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (25671) @ Episode 129/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (25870) @ Episode 130/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (26069) @ Episode 131/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (26268) @ Episode 132/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (26467) @ Episode 133/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (26666) @ Episode 134/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (26865) @ Episode 135/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (27064) @ Episode 136/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (27263) @ Episode 137/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (27462) @ Episode 138/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (27661) @ Episode 139/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (27860) @ Episode 140/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (28059) @ Episode 141/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (28258) @ Episode 142/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (28457) @ Episode 143/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (28656) @ Episode 144/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (28855) @ Episode 145/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (29054) @ Episode 146/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (29253) @ Episode 147/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (29452) @ Episode 148/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (29651) @ Episode 149/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (29850) @ Episode 150/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 149 (29999) @ Episode 151/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 199 (30049) @ Episode 151/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (30248) @ Episode 152/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (30447) @ Episode 153/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (30646) @ Episode 154/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (30845) @ Episode 155/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (31044) @ Episode 156/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (31243) @ Episode 157/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (31442) @ Episode 158/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (31641) @ Episode 159/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (31840) @ Episode 160/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (32039) @ Episode 161/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (32238) @ Episode 162/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (32437) @ Episode 163/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (32636) @ Episode 164/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (32835) @ Episode 165/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (33034) @ Episode 166/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (33233) @ Episode 167/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (33432) @ Episode 168/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (33631) @ Episode 169/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (33830) @ Episode 170/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (34029) @ Episode 171/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (34228) @ Episode 172/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (34427) @ Episode 173/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (34626) @ Episode 174/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (34825) @ Episode 175/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (35024) @ Episode 176/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (35223) @ Episode 177/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (35422) @ Episode 178/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (35621) @ Episode 179/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (35820) @ Episode 180/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (36019) @ Episode 181/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (36218) @ Episode 182/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (36417) @ Episode 183/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (36616) @ Episode 184/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (36815) @ Episode 185/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (37014) @ Episode 186/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (37213) @ Episode 187/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (37412) @ Episode 188/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (37611) @ Episode 189/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (37810) @ Episode 190/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (38009) @ Episode 191/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (38208) @ Episode 192/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (38407) @ Episode 193/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (38606) @ Episode 194/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (38805) @ Episode 195/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (39004) @ Episode 196/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (39203) @ Episode 197/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (39402) @ Episode 198/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (39601) @ Episode 199/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (39800) @ Episode 200/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (39999) @ Episode 201/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 0 (39999) @ Episode 202/10000, loss: None\n",
      "Copied model parameters to target network.\n",
      "Step 199 (40198) @ Episode 202/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (40397) @ Episode 203/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (40596) @ Episode 204/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (40795) @ Episode 205/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (40994) @ Episode 206/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (41193) @ Episode 207/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (41392) @ Episode 208/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (41591) @ Episode 209/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (41790) @ Episode 210/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (41989) @ Episode 211/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (42188) @ Episode 212/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (42387) @ Episode 213/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (42586) @ Episode 214/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (42785) @ Episode 215/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (42984) @ Episode 216/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 199 (43183) @ Episode 217/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (43382) @ Episode 218/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (43581) @ Episode 219/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (43780) @ Episode 220/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (43979) @ Episode 221/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (44178) @ Episode 222/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (44377) @ Episode 223/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (44576) @ Episode 224/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (44775) @ Episode 225/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (44974) @ Episode 226/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (45173) @ Episode 227/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (45372) @ Episode 228/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (45571) @ Episode 229/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (45770) @ Episode 230/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (45969) @ Episode 231/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (46168) @ Episode 232/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (46367) @ Episode 233/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (46566) @ Episode 234/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (46765) @ Episode 235/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (46964) @ Episode 236/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (47163) @ Episode 237/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (47362) @ Episode 238/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (47561) @ Episode 239/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (47760) @ Episode 240/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (47959) @ Episode 241/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (48158) @ Episode 242/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (48357) @ Episode 243/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (48556) @ Episode 244/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (48755) @ Episode 245/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (48954) @ Episode 246/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (49153) @ Episode 247/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (49352) @ Episode 248/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (49551) @ Episode 249/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (49750) @ Episode 250/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (49949) @ Episode 251/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 50 (49999) @ Episode 252/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 199 (50148) @ Episode 252/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (50347) @ Episode 253/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (50546) @ Episode 254/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (50745) @ Episode 255/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (50944) @ Episode 256/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (51143) @ Episode 257/10000, loss: 1.0998958110809326\n",
      "Episode Reward: -200.0\n",
      "Step 199 (51342) @ Episode 258/10000, loss: 1.00002384185791\n",
      "Episode Reward: -200.0\n",
      "Step 199 (51541) @ Episode 259/10000, loss: 1.0998959302902222\n",
      "Episode Reward: -200.0\n",
      "Step 199 (51740) @ Episode 260/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (51939) @ Episode 261/10000, loss: 1.0998958110809326\n",
      "Episode Reward: -200.0\n",
      "Step 199 (52138) @ Episode 262/10000, loss: 1.0999087452888489\n",
      "Episode Reward: -200.0\n",
      "Step 199 (52337) @ Episode 263/10000, loss: 1.098958110809326\n",
      "Episode Reward: -200.0\n",
      "Step 199 (52536) @ Episode 264/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (52735) @ Episode 265/10000, loss: 1.098959302902222\n",
      "Episode Reward: -200.0\n",
      "Step 199 (52934) @ Episode 266/10000, loss: 1.0998959302902222\n",
      "Episode Reward: -200.0\n",
      "Step 199 (53133) @ Episode 267/10000, loss: 1.099972581863403\n",
      "Episode Reward: -200.0\n",
      "Step 199 (53332) @ Episode 268/10000, loss: 1.0999101161956787\n",
      "Episode Reward: -200.0\n",
      "Step 199 (53531) @ Episode 269/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (53730) @ Episode 270/10000, loss: 1.0999242424964905\n",
      "Episode Reward: -200.0\n",
      "Step 199 (53929) @ Episode 271/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (54128) @ Episode 272/10000, loss: 1.0999913573265076\n",
      "Episode Reward: -200.0\n",
      "Step 199 (54327) @ Episode 273/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (54526) @ Episode 274/10000, loss: 1.09901294708252\n",
      "Episode Reward: -200.0\n",
      "Step 199 (54725) @ Episode 275/10000, loss: 1.0999528527259827\n",
      "Episode Reward: -200.0\n",
      "Step 199 (54924) @ Episode 276/10000, loss: 1.099972581863403\n",
      "Episode Reward: -200.0\n",
      "Step 199 (55123) @ Episode 277/10000, loss: 1.0999087452888489\n",
      "Episode Reward: -200.0\n",
      "Step 199 (55322) @ Episode 278/10000, loss: 1.099903678894043\n",
      "Episode Reward: -200.0\n",
      "Step 199 (55521) @ Episode 279/10000, loss: 1.0999242424964905\n",
      "Episode Reward: -200.0\n",
      "Step 199 (55720) @ Episode 280/10000, loss: 1.0999953508377075\n",
      "Episode Reward: -200.0\n",
      "Step 199 (55919) @ Episode 281/10000, loss: 1.0999242424964905\n",
      "Episode Reward: -200.0\n",
      "Step 199 (56118) @ Episode 282/10000, loss: 1.0999101161956787\n",
      "Episode Reward: -200.0\n",
      "Step 199 (56317) @ Episode 283/10000, loss: 1.0998958110809326\n",
      "Episode Reward: -200.0\n",
      "Step 199 (56516) @ Episode 284/10000, loss: 1.098959302902222\n",
      "Episode Reward: -200.0\n",
      "Step 199 (56715) @ Episode 285/10000, loss: 1.09760985374451\n",
      "Episode Reward: -200.0\n",
      "Step 199 (56914) @ Episode 286/10000, loss: 1.0999953508377075\n",
      "Episode Reward: -200.0\n",
      "Step 199 (57113) @ Episode 287/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (57312) @ Episode 288/10000, loss: 1.099903678894043\n",
      "Episode Reward: -200.0\n",
      "Step 199 (57511) @ Episode 289/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (57710) @ Episode 290/10000, loss: 1.0999972581863403\n",
      "Episode Reward: -200.0\n",
      "Step 199 (57909) @ Episode 291/10000, loss: 1.0999528527259827\n",
      "Episode Reward: -200.0\n",
      "Step 199 (58108) @ Episode 292/10000, loss: 1.0999012947082529\n",
      "Episode Reward: -200.0\n",
      "Step 199 (58307) @ Episode 293/10000, loss: 1.0999087452888489\n",
      "Episode Reward: -200.0\n",
      "Step 199 (58506) @ Episode 294/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (58705) @ Episode 295/10000, loss: 1.0999102950096137\n",
      "Episode Reward: -200.0\n",
      "Step 199 (58904) @ Episode 296/10000, loss: 1.0999012947082525\n",
      "Episode Reward: -200.0\n",
      "Step 199 (59103) @ Episode 297/10000, loss: 1.0999953508377075\n",
      "Episode Reward: -200.0\n",
      "Step 199 (59302) @ Episode 298/10000, loss: 1.0999102950096137\n",
      "Episode Reward: -200.0\n",
      "Step 199 (59501) @ Episode 299/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (59700) @ Episode 300/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (59899) @ Episode 301/10000, loss: 1.098958110809326\n",
      "Episode Reward: -200.0\n",
      "Step 100 (59999) @ Episode 302/10000, loss: 1.09910295009613\n",
      "Copied model parameters to target network.\n",
      "Step 199 (60098) @ Episode 302/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (60297) @ Episode 303/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (60496) @ Episode 304/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (60695) @ Episode 305/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (60894) @ Episode 306/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (61093) @ Episode 307/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (61292) @ Episode 308/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (61491) @ Episode 309/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (61690) @ Episode 310/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (61889) @ Episode 311/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (62088) @ Episode 312/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (62287) @ Episode 313/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (62486) @ Episode 314/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (62685) @ Episode 315/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (62884) @ Episode 316/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (63083) @ Episode 317/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (63282) @ Episode 318/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (63481) @ Episode 319/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (63680) @ Episode 320/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (63879) @ Episode 321/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 199 (64078) @ Episode 322/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (64277) @ Episode 323/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (64476) @ Episode 324/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (64675) @ Episode 325/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (64874) @ Episode 326/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (65073) @ Episode 327/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (65272) @ Episode 328/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (65471) @ Episode 329/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (65670) @ Episode 330/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (65869) @ Episode 331/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (66068) @ Episode 332/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (66267) @ Episode 333/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (66466) @ Episode 334/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (66665) @ Episode 335/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (66864) @ Episode 336/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (67063) @ Episode 337/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (67262) @ Episode 338/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (67461) @ Episode 339/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (67660) @ Episode 340/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (67859) @ Episode 341/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (68058) @ Episode 342/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (68257) @ Episode 343/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (68456) @ Episode 344/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (68655) @ Episode 345/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (68854) @ Episode 346/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (69053) @ Episode 347/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (69252) @ Episode 348/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (69451) @ Episode 349/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (69650) @ Episode 350/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (69849) @ Episode 351/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 150 (69999) @ Episode 352/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 199 (70048) @ Episode 352/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (70247) @ Episode 353/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (70446) @ Episode 354/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (70645) @ Episode 355/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (70844) @ Episode 356/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (71043) @ Episode 357/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (71242) @ Episode 358/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (71441) @ Episode 359/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (71640) @ Episode 360/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (71839) @ Episode 361/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (72038) @ Episode 362/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (72237) @ Episode 363/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (72436) @ Episode 364/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (72635) @ Episode 365/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (72834) @ Episode 366/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (73033) @ Episode 367/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (73232) @ Episode 368/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (73431) @ Episode 369/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (73630) @ Episode 370/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (73829) @ Episode 371/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (74028) @ Episode 372/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (74227) @ Episode 373/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (74426) @ Episode 374/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (74625) @ Episode 375/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (74824) @ Episode 376/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (75023) @ Episode 377/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (75222) @ Episode 378/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (75421) @ Episode 379/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (75620) @ Episode 380/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (75819) @ Episode 381/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (76018) @ Episode 382/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (76217) @ Episode 383/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (76416) @ Episode 384/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (76615) @ Episode 385/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (76814) @ Episode 386/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (77013) @ Episode 387/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (77212) @ Episode 388/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (77411) @ Episode 389/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (77610) @ Episode 390/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (77809) @ Episode 391/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (78008) @ Episode 392/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (78207) @ Episode 393/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (78406) @ Episode 394/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (78605) @ Episode 395/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (78804) @ Episode 396/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (79003) @ Episode 397/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (79202) @ Episode 398/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (79401) @ Episode 399/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (79600) @ Episode 400/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (79799) @ Episode 401/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (79998) @ Episode 402/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 1 (79999) @ Episode 403/10000, loss: 1.0e\n",
      "Copied model parameters to target network.\n",
      "Step 199 (80197) @ Episode 403/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (80396) @ Episode 404/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (80595) @ Episode 405/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (80794) @ Episode 406/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (80993) @ Episode 407/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (81192) @ Episode 408/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (81391) @ Episode 409/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (81590) @ Episode 410/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (81789) @ Episode 411/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (81988) @ Episode 412/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (82187) @ Episode 413/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (82386) @ Episode 414/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (82585) @ Episode 415/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (82784) @ Episode 416/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (82983) @ Episode 417/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (83182) @ Episode 418/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (83381) @ Episode 419/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (83580) @ Episode 420/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (83779) @ Episode 421/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (83978) @ Episode 422/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (84177) @ Episode 423/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n",
      "Step 199 (84376) @ Episode 424/10000, loss: 1.0\n",
      "Episode Reward: -200.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments_mountain/{}\".format(env.spec.id))\n",
    "\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope=\"q_estimator\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "# State processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                    env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    state_processor=state_processor,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    num_episodes=10000,\n",
    "                                    replay_memory_size=500000,\n",
    "                                    replay_memory_init_size=50000,\n",
    "                                    update_target_estimator_every=10000,\n",
    "                                    epsilon_start=1.0,\n",
    "                                    epsilon_end=0.1,\n",
    "                                    epsilon_decay_steps=500000,\n",
    "                                    discount_factor=0.99,\n",
    "                                    batch_size=64):\n",
    "        \n",
    "        if(stats.episode_rewards[-1]>-200):\n",
    "            print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = {'Reward':  stats.episode_rewards}\n",
    "df = pd.DataFrame (data)\n",
    "\n",
    "rolling_mean = df.Reward.rolling(window=50).mean()\n",
    "\n",
    "plt.plot(df.index, df.Reward, label='DQN Reward')\n",
    "plt.plot(df.index, rolling_mean, label='DQN MA Reward', color='orange')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "df.to_csv('output_DQN_mountain.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
