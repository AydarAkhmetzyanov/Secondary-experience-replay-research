{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/aydarakhmetzyanov/Documents/secondary_experience_replay/SERI.py:367: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/aydarakhmetzyanov/Documents/secondary_experience_replay/SERI.py:383: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/aydarakhmetzyanov/Documents/secondary_experience_replay/SERI.py:389: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/aydarakhmetzyanov/opt/anaconda3/envs/exp37/lib/python3.7/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /Users/aydarakhmetzyanov/opt/anaconda3/envs/exp37/lib/python3.7/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /Users/aydarakhmetzyanov/Documents/secondary_experience_replay/SERI.py:410: The name tf.squared_difference is deprecated. Please use tf.math.squared_difference instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/aydarakhmetzyanov/Documents/secondary_experience_replay/SERI.py:414: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/aydarakhmetzyanov/Documents/secondary_experience_replay/SERI.py:415: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_global_step\n",
      "WARNING:tensorflow:From /Users/aydarakhmetzyanov/opt/anaconda3/envs/exp37/lib/python3.7/site-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/aydarakhmetzyanov/Documents/secondary_experience_replay/SERI.py:418: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/aydarakhmetzyanov/Documents/secondary_experience_replay/SERI.py:419: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/aydarakhmetzyanov/Documents/secondary_experience_replay/SERI.py:420: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/aydarakhmetzyanov/Documents/secondary_experience_replay/SERI.py:374: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/aydarakhmetzyanov/Documents/secondary_experience_replay/SERI.py:341: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/aydarakhmetzyanov/Documents/secondary_experience_replay/SERI.py:473: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/aydarakhmetzyanov/Documents/secondary_experience_replay/SERI.py:116: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "Populating replay memory...\n",
      "Mean reward in replay buffer: 1.3582089552238805 STD: 1.2771236338426295\n",
      "Target mean reward: 3.9134562229091396\n",
      "Secondary experience collection...\n",
      "Last reward: 5.0\n",
      "Mean reward in replay buffer: 1.3819095477386936 STD: 1.2968004028015876\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.3869346733668342 STD: 1.293351355214085\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 1.3869346733668342 STD: 1.293351355214085\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 1.404040404040404 STD: 1.2897030069263864\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.4090909090909092 STD: 1.2861299055016042\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.4141414141414141 STD: 1.2825268603086715\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.4191919191919191 STD: 1.2788936182646007\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.4314720812182742 STD: 1.274404368868074\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 1.4314720812182742 STD: 1.274404368868074\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 1.4416243654822336 STD: 1.270904145864165\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.4540816326530612 STD: 1.2660998759442474\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.4591836734693877 STD: 1.262219188141095\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 1.4591836734693877 STD: 1.262219188141095\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.4642857142857142 STD: 1.2583057392117916\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.469387755102041 STD: 1.254359222524132\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 1.4923076923076923 STD: 1.2533223097248072\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 1.4923076923076923 STD: 1.2533223097248072\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 1.4923076923076923 STD: 1.2533223097248072\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 1.4923076923076923 STD: 1.2533223097248072\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 1.4923076923076923 STD: 1.2533223097248072\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.5051546391752577 STD: 1.24778528228131\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 1.5051546391752577 STD: 1.24778528228131\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 1.5233160621761659 STD: 1.241942945465993\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 1.538860103626943 STD: 1.241551761789283\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 1.538860103626943 STD: 1.241551761789283\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 1.538860103626943 STD: 1.241551761789283\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 1.538860103626943 STD: 1.241551761789283\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 1.538860103626943 STD: 1.241551761789283\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.5520833333333333 STD: 1.2353416270115625\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 1.5759162303664922 STD: 1.2325609084121218\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.581151832460733 STD: 1.227946824231962\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 1.6 STD: 1.220633888578836\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 1.6243386243386244 STD: 1.2167449651654798\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 1.6243386243386244 STD: 1.2167449651654798\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 1.6349206349206349 STD: 1.2112274498033149\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.648936170212766 STD: 1.2034482258359132\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 1.6595744680851063 STD: 1.1976196667066734\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.6737967914438503 STD: 1.1893379478445973\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 1.6737967914438503 STD: 1.1893379478445973\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 1.6935483870967742 STD: 1.1798585761896192\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.6989247311827957 STD: 1.1743653488937587\n",
      "Last reward: 5.0\n",
      "Mean reward in replay buffer: 1.7351351351351352 STD: 1.188764479227921\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 1.7608695652173914 STD: 1.1815481667395082\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 1.7608695652173914 STD: 1.1815481667395082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 1.7868852459016393 STD: 1.17363089417846\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 1.7868852459016393 STD: 1.17363089417846\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.7923497267759563 STD: 1.1675777725878194\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.8076923076923077 STD: 1.1569373524089648\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 1.8076923076923077 STD: 1.1569373524089648\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.8131868131868132 STD: 1.1506624362757683\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 1.8131868131868132 STD: 1.1506624362757683\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.8186813186813187 STD: 1.1443265855275813\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 1.8555555555555556 STD: 1.129207079406277\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.8611111111111112 STD: 1.1224668010364165\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.8666666666666667 STD: 1.1156579840749439\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 1.8666666666666667 STD: 1.1156579840749439\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 1.8777777777777778 STD: 1.1068883132687326\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 1.899441340782123 STD: 1.091823400478255\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 1.899441340782123 STD: 1.091823400478255\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 1.9325842696629214 STD: 1.0871385008255146\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.9435028248587571 STD: 1.0803908916152323\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.9435028248587571 STD: 1.0803908916152323\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 1.96045197740113 STD: 1.0890027857094764\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.96045197740113 STD: 1.0890027857094764\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 1.96045197740113 STD: 1.0890027857094764\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 1.96045197740113 STD: 1.0890027857094764\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 1.9829545454545454 STD: 1.097917154398308\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.0 STD: 1.0985884360051028\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.0 STD: 1.0985884360051028\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.0 STD: 1.0985884360051028\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.005714285714286 STD: 1.0959546453046587\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.0114285714285716 STD: 1.0932844716543775\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.0114285714285716 STD: 1.0932844716543775\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 2.0344827586206895 STD: 1.1012160998391494\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 2.0578034682080926 STD: 1.1086917843921273\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.0578034682080926 STD: 1.1086917843921273\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.0578034682080926 STD: 1.1086917843921273\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.069364161849711 STD: 1.108024719672731\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.069364161849711 STD: 1.108024719672731\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.0813953488372094 STD: 1.1051724882343796\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.069364161849711 STD: 1.1132594874433266\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.0872093023255816 STD: 1.1020763509946725\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.0751445086705202 STD: 1.110266889628406\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.0751445086705202 STD: 1.110266889628406\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.0930232558139537 STD: 1.0989405528222584\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.0930232558139537 STD: 1.0989405528222584\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.0930232558139537 STD: 1.0989405528222584\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.0988372093023258 STD: 1.0957647532188854\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.0988372093023258 STD: 1.0957647532188854\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.111111111111111 STD: 1.0924578034357124\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.0988372093023258 STD: 1.1010886889027576\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.1228070175438596 STD: 1.091197541375257\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.1228070175438596 STD: 1.091197541375257\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.1228070175438596 STD: 1.091197541375257\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.1228070175438596 STD: 1.091197541375257\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.1228070175438596 STD: 1.091197541375257\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.1228070175438596 STD: 1.091197541375257\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 2.1470588235294117 STD: 1.0968581527117376\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.1470588235294117 STD: 1.0968581527117376\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.1470588235294117 STD: 1.0968581527117376\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.1470588235294117 STD: 1.0968581527117376\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.1470588235294117 STD: 1.0968581527117376\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.1470588235294117 STD: 1.0968581527117376\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.152941176470588 STD: 1.0933460108665956\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.152941176470588 STD: 1.0933460108665956\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.165680473372781 STD: 1.0893448958235943\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.152941176470588 STD: 1.0987446559617593\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.152941176470588 STD: 1.0987446559617593\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.1588235294117646 STD: 1.0897906120633063\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.1588235294117646 STD: 1.0897906120633063\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.1775147928994083 STD: 1.0874679968280012\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.1893491124260356 STD: 1.0854580676750047\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.1893491124260356 STD: 1.0854580676750047\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.2083333333333335 STD: 1.0825461595815444\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.2083333333333335 STD: 1.0825461595815444\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.2083333333333335 STD: 1.0825461595815444\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.2083333333333335 STD: 1.0825461595815444\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.2083333333333335 STD: 1.0825461595815444\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.2142857142857144 STD: 1.0786044273471622\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.2142857142857144 STD: 1.0786044273471622\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.2202380952380953 STD: 1.0746150695768322\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.2261904761904763 STD: 1.0705775538609568\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.2261904761904763 STD: 1.0705775538609568\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.2395209580838324 STD: 1.065389689654998\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.2395209580838324 STD: 1.065389689654998\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.2261904761904763 STD: 1.0761562836015508\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.2261904761904763 STD: 1.0761562836015508\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.232142857142857 STD: 1.0664913332515975\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.232142857142857 STD: 1.0664913332515975\n",
      "Last reward: 5.0\n",
      "Mean reward in replay buffer: 2.2634730538922154 STD: 1.082186501611221\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.2634730538922154 STD: 1.082186501611221\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.2771084337349397 STD: 1.0766193217328217\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.283132530120482 STD: 1.0722188014052416\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.269461077844311 STD: 1.0834857055256195\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.289156626506024 STD: 1.0677659541749855\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.2951807228915664 STD: 1.0632601226197724\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.2951807228915664 STD: 1.0632601226197724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.2951807228915664 STD: 1.0632601226197724\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.3072289156626504 STD: 1.0598208119020098\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.3072289156626504 STD: 1.0598208119020098\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.3072289156626504 STD: 1.0598208119020098\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.3072289156626504 STD: 1.0598208119020098\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.3072289156626504 STD: 1.0598208119020098\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.3072289156626504 STD: 1.0598208119020098\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.327272727272727 STD: 1.0543963500369182\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.327272727272727 STD: 1.0543963500369182\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.3393939393939394 STD: 1.0505339507461726\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.3393939393939394 STD: 1.0505339507461726\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.3393939393939394 STD: 1.0505339507461726\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.3393939393939394 STD: 1.0505339507461726\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.3454545454545452 STD: 1.0456328778779551\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.3454545454545452 STD: 1.0456328778779551\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.3658536585365852 STD: 1.0392564014829753\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.3780487804878048 STD: 1.0348556569564182\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.392638036809816 STD: 1.0270152295629051\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.392638036809816 STD: 1.0270152295629051\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.3780487804878048 STD: 1.040767106217816\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.392638036809816 STD: 1.0270152295629051\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.3987730061349692 STD: 1.0216174376431046\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.3987730061349692 STD: 1.0216174376431046\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.4049079754601226 STD: 1.0161537067180044\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.4049079754601226 STD: 1.0161537067180044\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.419753086419753 STD: 1.0076008961807268\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.419753086419753 STD: 1.0076008961807268\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.425925925925926 STD: 1.0018960630659377\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.425925925925926 STD: 1.0018960630659377\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 2.4444444444444446 STD: 1.003100782635635\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.459627329192547 STD: 0.993691280151158\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.4444444444444446 STD: 1.0092737687925268\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.4658385093167703 STD: 0.9876174354261694\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.4782608695652173 STD: 0.9816246515454566\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.4782608695652173 STD: 0.9816246515454566\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.5 STD: 0.9712858623572642\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.5 STD: 0.9712858623572642\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.5 STD: 0.9712858623572642\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.5 STD: 0.9712858623572642\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.5 STD: 0.9712858623572642\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.5 STD: 0.9712858623572642\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.50625 STD: 0.9647685224377879\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.50625 STD: 0.9647685224377879\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.50625 STD: 0.9647685224377879\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.5283018867924527 STD: 0.953417044207985\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.5283018867924527 STD: 0.953417044207985\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.5283018867924527 STD: 0.953417044207985\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.5283018867924527 STD: 0.953417044207985\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.5125 STD: 0.9712049184959329\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.5125 STD: 0.9712049184959329\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.5283018867924527 STD: 0.953417044207985\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.5283018867924527 STD: 0.953417044207985\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.5283018867924527 STD: 0.953417044207985\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.5283018867924527 STD: 0.953417044207985\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.5283018867924527 STD: 0.953417044207985\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.5283018867924527 STD: 0.953417044207985\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.5283018867924527 STD: 0.953417044207985\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.5283018867924527 STD: 0.953417044207985\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 2.547169811320755 STD: 0.9526652367227907\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.547169811320755 STD: 0.9526652367227907\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.569620253164557 STD: 0.9400304142497652\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.5534591194968552 STD: 0.958953799273454\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.5534591194968552 STD: 0.958953799273454\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.569620253164557 STD: 0.9400304142497652\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.569620253164557 STD: 0.9400304142497652\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.569620253164557 STD: 0.9400304142497652\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.559748427672956 STD: 0.9452827248932931\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.559748427672956 STD: 0.9452827248932931\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.569620253164557 STD: 0.9400304142497652\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.5759493670886076 STD: 0.9327331511091159\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.559748427672956 STD: 0.9519546511414242\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.5759493670886076 STD: 0.9327331511091159\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.5822784810126582 STD: 0.9253347810312833\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.5949367088607596 STD: 0.9171957725266137\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.6178343949044587 STD: 0.9025251743568534\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.6178343949044587 STD: 0.9025251743568534\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.6178343949044587 STD: 0.9025251743568534\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.6178343949044587 STD: 0.9025251743568534\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.6178343949044587 STD: 0.9025251743568534\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.624203821656051 STD: 0.8945276130439584\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.641025641025641 STD: 0.8795065915043425\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.6474358974358974 STD: 0.8710726585707486\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.6474358974358974 STD: 0.8710726585707486\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.6305732484076434 STD: 0.8936142703379125\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.6538461538461537 STD: 0.8625083162112113\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.6538461538461537 STD: 0.8625083162112113\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.6538461538461537 STD: 0.8625083162112113\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.6538461538461537 STD: 0.8625083162112113\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.66025641025641 STD: 0.8613327622806405\n",
      "Last reward: 6.0\n",
      "Mean reward in replay buffer: 2.6903225806451614 STD: 0.9014255804469974\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.6903225806451614 STD: 0.9014255804469974\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.6903225806451614 STD: 0.9014255804469974\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.6903225806451614 STD: 0.9014255804469974\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.6903225806451614 STD: 0.9014255804469974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.6903225806451614 STD: 0.9014255804469974\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.6903225806451614 STD: 0.9014255804469974\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.6903225806451614 STD: 0.9014255804469974\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.6903225806451614 STD: 0.9014255804469974\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.6903225806451614 STD: 0.9014255804469974\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.6903225806451614 STD: 0.9014255804469974\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.6903225806451614 STD: 0.9014255804469974\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.6903225806451614 STD: 0.9014255804469974\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.6903225806451614 STD: 0.9014255804469974\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.6903225806451614 STD: 0.9014255804469974\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.6903225806451614 STD: 0.9014255804469974\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.6903225806451614 STD: 0.9014255804469974\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.6903225806451614 STD: 0.9014255804469974\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.6903225806451614 STD: 0.9014255804469974\n",
      "Last reward: 5.0\n",
      "Mean reward in replay buffer: 2.7142857142857144 STD: 0.9197490828210377\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.7142857142857144 STD: 0.9197490828210377\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.7142857142857144 STD: 0.9197490828210377\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.720779220779221 STD: 0.9182019460244293\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.720779220779221 STD: 0.9182019460244293\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.720779220779221 STD: 0.9182019460244293\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.720779220779221 STD: 0.9182019460244293\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.720779220779221 STD: 0.9182019460244293\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.720779220779221 STD: 0.9182019460244293\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.720779220779221 STD: 0.9182019460244293\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.720779220779221 STD: 0.9182019460244293\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.720779220779221 STD: 0.9182019460244293\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.720779220779221 STD: 0.9182019460244293\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 2.738562091503268 STD: 0.9231755965007655\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.7450980392156863 STD: 0.921450599560465\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.7450980392156863 STD: 0.921450599560465\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.7450980392156863 STD: 0.921450599560465\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.7516339869281046 STD: 0.9196756132322589\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.7516339869281046 STD: 0.9196756132322589\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.7516339869281046 STD: 0.9196756132322589\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.7516339869281046 STD: 0.9196756132322589\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.7516339869281046 STD: 0.9196756132322589\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 2.7697368421052633 STD: 0.9240738677566617\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.758169934640523 STD: 0.9320756674646115\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.7697368421052633 STD: 0.9240738677566617\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.7697368421052633 STD: 0.9240738677566617\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.7516339869281046 STD: 0.9478580128487071\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.7697368421052633 STD: 0.9240738677566617\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.758169934640523 STD: 0.9320756674646115\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.758169934640523 STD: 0.9320756674646115\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.758169934640523 STD: 0.9320756674646115\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.7697368421052633 STD: 0.9240738677566617\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.758169934640523 STD: 0.9320756674646115\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.7697368421052633 STD: 0.9240738677566617\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.7516339869281046 STD: 0.9478580128487071\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.7697368421052633 STD: 0.9240738677566617\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.7697368421052633 STD: 0.9240738677566617\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.764705882352941 STD: 0.9231290173440215\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.764705882352941 STD: 0.9231290173440215\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.764705882352941 STD: 0.9231290173440215\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.764705882352941 STD: 0.9231290173440215\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.7712418300653594 STD: 0.9212172441234006\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.7712418300653594 STD: 0.9212172441234006\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.7712418300653594 STD: 0.9212172441234006\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.7712418300653594 STD: 0.9212172441234006\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.776315789473684 STD: 0.9221151069565655\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.764705882352941 STD: 0.9302285081631184\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.764705882352941 STD: 0.9302285081631184\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.7712418300653594 STD: 0.9212172441234006\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.7712418300653594 STD: 0.9212172441234006\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.7712418300653594 STD: 0.9212172441234006\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.7712418300653594 STD: 0.9212172441234006\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.7712418300653594 STD: 0.9212172441234006\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.7712418300653594 STD: 0.9212172441234006\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.7712418300653594 STD: 0.9212172441234006\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.7712418300653594 STD: 0.9212172441234006\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 2.789473684210526 STD: 0.9252282979224117\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.789473684210526 STD: 0.9252282979224117\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.789473684210526 STD: 0.9252282979224117\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.789473684210526 STD: 0.9252282979224117\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.789473684210526 STD: 0.9252282979224117\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.7960526315789473 STD: 0.9231304054985638\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.7777777777777777 STD: 0.9474496391436593\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.784313725490196 STD: 0.9314757424772421\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.8026315789473686 STD: 0.9209804283803983\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.8026315789473686 STD: 0.9209804283803983\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 2.8157894736842106 STD: 0.9237201823348353\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.8157894736842106 STD: 0.9237201823348353\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.8157894736842106 STD: 0.9237201823348353\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.8157894736842106 STD: 0.9237201823348353\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.8157894736842106 STD: 0.9237201823348353\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.8157894736842106 STD: 0.9237201823348353\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8157894736842106 STD: 0.9237201823348353\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.8157894736842106 STD: 0.9237201823348353\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8157894736842106 STD: 0.9237201823348353\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.8157894736842106 STD: 0.9237201823348353\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8157894736842106 STD: 0.9237201823348353\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.827814569536424 STD: 0.9220382450778891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.827814569536424 STD: 0.9220382450778891\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8092105263157894 STD: 0.9471711289086739\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.827814569536424 STD: 0.9220382450778891\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.827814569536424 STD: 0.9220382450778891\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.8157894736842106 STD: 0.9308619706300629\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.827814569536424 STD: 0.9220382450778891\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.827814569536424 STD: 0.9220382450778891\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8092105263157894 STD: 0.9471711289086739\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.8223684210526314 STD: 0.9214297390627668\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8223684210526314 STD: 0.9214297390627668\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8223684210526314 STD: 0.9214297390627668\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.8223684210526314 STD: 0.9214297390627668\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.8223684210526314 STD: 0.9214297390627668\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.834437086092715 STD: 0.9196409703513573\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.834437086092715 STD: 0.9196409703513573\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.834437086092715 STD: 0.9196409703513573\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.834437086092715 STD: 0.9196409703513573\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.834437086092715 STD: 0.9196409703513573\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8157894736842106 STD: 0.9449836372077823\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.8223684210526314 STD: 0.9285891433469708\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.834437086092715 STD: 0.9196409703513573\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.834437086092715 STD: 0.9196409703513573\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.834437086092715 STD: 0.9196409703513573\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.834437086092715 STD: 0.9196409703513573\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.834437086092715 STD: 0.9196409703513573\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.834437086092715 STD: 0.9196409703513573\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.834437086092715 STD: 0.9196409703513573\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.8223684210526314 STD: 0.9285891433469708\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.834437086092715 STD: 0.9196409703513573\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.834437086092715 STD: 0.9196409703513573\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8157894736842106 STD: 0.9449836372077823\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.834437086092715 STD: 0.9196409703513573\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.834437086092715 STD: 0.9196409703513573\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.8410596026490067 STD: 0.9171892951242441\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.8410596026490067 STD: 0.9171892951242441\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8223684210526314 STD: 0.942744855921806\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.8410596026490067 STD: 0.9171892951242441\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8223684210526314 STD: 0.942744855921806\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8223684210526314 STD: 0.942744855921806\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.8410596026490067 STD: 0.9171892951242441\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.8410596026490067 STD: 0.9171892951242441\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8223684210526314 STD: 0.942744855921806\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8223684210526314 STD: 0.942744855921806\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.8410596026490067 STD: 0.9171892951242441\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8223684210526314 STD: 0.942744855921806\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8223684210526314 STD: 0.942744855921806\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.8410596026490067 STD: 0.9171892951242441\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.847682119205298 STD: 0.9146827819585756\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8289473684210527 STD: 0.940454418761411\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.847682119205298 STD: 0.9146827819585756\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.847682119205298 STD: 0.9146827819585756\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.847682119205298 STD: 0.9146827819585756\n",
      "Last reward: 5.0\n",
      "Mean reward in replay buffer: 2.8733333333333335 STD: 0.9290008103294858\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8543046357615895 STD: 0.9549684182797354\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8543046357615895 STD: 0.9549684182797354\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8543046357615895 STD: 0.9549684182797354\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.8609271523178808 STD: 0.938365494529985\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.8733333333333335 STD: 0.9290008103294858\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.8733333333333335 STD: 0.9290008103294858\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8543046357615895 STD: 0.9549684182797354\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.8733333333333335 STD: 0.9290008103294858\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.88 STD: 0.9262756485956046\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.88 STD: 0.9262756485956046\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.88 STD: 0.9262756485956046\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8609271523178808 STD: 0.9524686178510922\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8609271523178808 STD: 0.9524686178510922\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.88 STD: 0.9262756485956046\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.88 STD: 0.9262756485956046\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.88 STD: 0.9262756485956046\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8609271523178808 STD: 0.9524686178510922\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.88 STD: 0.9262756485956046\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8609271523178808 STD: 0.9524686178510922\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.88 STD: 0.9262756485956046\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.88 STD: 0.9262756485956046\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.88 STD: 0.9262756485956046\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.8866666666666667 STD: 0.9234939974417603\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8866666666666667 STD: 0.9234939974417603\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.8866666666666667 STD: 0.9234939974417603\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.8933333333333335 STD: 0.9206553448398065\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.9060402684563758 STD: 0.9178579784682188\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8866666666666667 STD: 0.9450447712604948\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.8933333333333335 STD: 0.9279165279129633\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.9060402684563758 STD: 0.9178579784682188\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.9060402684563758 STD: 0.9178579784682188\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.8866666666666667 STD: 0.9450447712604948\n",
      "Last reward: 5.0\n",
      "Mean reward in replay buffer: 2.9261744966442955 STD: 0.9306635218846852\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.9261744966442955 STD: 0.9306635218846852\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 2.9395973154362416 STD: 0.9316375289768816\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.9395973154362416 STD: 0.9316375289768816\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.9395973154362416 STD: 0.9316375289768816\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.9395973154362416 STD: 0.9316375289768816\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.9395973154362416 STD: 0.9316375289768816\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.9395973154362416 STD: 0.9316375289768816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.9395973154362416 STD: 0.9316375289768816\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.9395973154362416 STD: 0.9316375289768816\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.9395973154362416 STD: 0.9316375289768816\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.9395973154362416 STD: 0.9316375289768816\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.9395973154362416 STD: 0.9316375289768816\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.9527027027027026 STD: 0.9282743807619815\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.9328859060402683 STD: 0.9562343765210277\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.9527027027027026 STD: 0.9282743807619815\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.9594594594594597 STD: 0.9249260193131269\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.9594594594594597 STD: 0.9249260193131269\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.9594594594594597 STD: 0.9249260193131269\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.9594594594594597 STD: 0.9249260193131269\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 2.9662162162162162 STD: 0.9215156144973651\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.9662162162162162 STD: 0.9215156144973651\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.9662162162162162 STD: 0.9215156144973651\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.9662162162162162 STD: 0.9215156144973651\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 2.97972972972973 STD: 0.9219145606124866\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.97972972972973 STD: 0.9219145606124866\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.97972972972973 STD: 0.9219145606124866\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.97972972972973 STD: 0.9219145606124866\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.97972972972973 STD: 0.9219145606124866\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.97972972972973 STD: 0.9219145606124866\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.97972972972973 STD: 0.9219145606124866\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.97972972972973 STD: 0.9219145606124866\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 2.97972972972973 STD: 0.9219145606124866\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 2.97972972972973 STD: 0.9219145606124866\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.97972972972973 STD: 0.9219145606124866\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.0 STD: 0.9215829146057074\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.0 STD: 0.9215829146057074\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 2.97972972972973 STD: 0.9509722528953183\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.0 STD: 0.9215829146057074\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.0 STD: 0.9215829146057074\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.0 STD: 0.9215829146057074\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.0 STD: 0.9215829146057074\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.0 STD: 0.9215829146057074\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.006802721088435 STD: 0.9178339498213169\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.006802721088435 STD: 0.9178339498213169\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.006802721088435 STD: 0.9178339498213169\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.0136054421768708 STD: 0.9140186335277489\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.0136054421768708 STD: 0.9140186335277489\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.0136054421768708 STD: 0.9140186335277489\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.0136054421768708 STD: 0.9140186335277489\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.0136054421768708 STD: 0.9140186335277489\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.0136054421768708 STD: 0.9140186335277489\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.020408163265306 STD: 0.9101361312832006\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.020408163265306 STD: 0.9101361312832006\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.020408163265306 STD: 0.9101361312832006\n",
      "Last reward: 6.0\n",
      "Mean reward in replay buffer: 3.0547945205479454 STD: 0.9379421489241163\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.0547945205479454 STD: 0.9379421489241163\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.075862068965517 STD: 0.9360285387141549\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.075862068965517 STD: 0.9360285387141549\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.075862068965517 STD: 0.9360285387141549\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.0547945205479454 STD: 0.966906349405925\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.0547945205479454 STD: 0.966906349405925\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.0547945205479454 STD: 0.966906349405925\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.075862068965517 STD: 0.9360285387141549\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.0547945205479454 STD: 0.966906349405925\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.0827586206896553 STD: 0.9317206922011886\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.0827586206896553 STD: 0.9317206922011886\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.0827586206896553 STD: 0.9317206922011886\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.0827586206896553 STD: 0.9317206922011886\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.0827586206896553 STD: 0.9317206922011886\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.0827586206896553 STD: 0.9317206922011886\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.089655172413793 STD: 0.9273411917000963\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.089655172413793 STD: 0.9273411917000963\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.089655172413793 STD: 0.9273411917000963\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.089655172413793 STD: 0.9273411917000963\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.0965517241379312 STD: 0.9228890171255884\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.0965517241379312 STD: 0.9228890171255884\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.0965517241379312 STD: 0.9228890171255884\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.0965517241379312 STD: 0.9228890171255884\n",
      "Last reward: 5.0\n",
      "Mean reward in replay buffer: 3.125 STD: 0.9302605094190635\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.1468531468531467 STD: 0.9265872712486545\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.1319444444444446 STD: 0.9405141264578764\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.1319444444444446 STD: 0.9405141264578764\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.1319444444444446 STD: 0.9405141264578764\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.1468531468531467 STD: 0.9265872712486545\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.1319444444444446 STD: 0.9405141264578764\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.1319444444444446 STD: 0.9405141264578764\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.1319444444444446 STD: 0.9405141264578764\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.1538461538461537 STD: 0.9216312261472865\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.1538461538461537 STD: 0.9216312261472865\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.1319444444444446 STD: 0.9552689935504293\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.1538461538461537 STD: 0.9216312261472865\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.1538461538461537 STD: 0.9216312261472865\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.1538461538461537 STD: 0.9216312261472865\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.1538461538461537 STD: 0.9216312261472865\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.1538461538461537 STD: 0.9216312261472865\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.160839160839161 STD: 0.9165946594450758\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.160839160839161 STD: 0.9165946594450758\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.160839160839161 STD: 0.9165946594450758\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.160839160839161 STD: 0.9165946594450758\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.174825174825175 STD: 0.9140120912896814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.1888111888111887 STD: 0.9112060492003338\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.1888111888111887 STD: 0.9112060492003338\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.1888111888111887 STD: 0.9112060492003338\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.1888111888111887 STD: 0.9112060492003338\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.1888111888111887 STD: 0.9112060492003338\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.204225352112676 STD: 0.903411441450794\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.204225352112676 STD: 0.903411441450794\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.204225352112676 STD: 0.903411441450794\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.204225352112676 STD: 0.903411441450794\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.204225352112676 STD: 0.903411441450794\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.204225352112676 STD: 0.903411441450794\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.1818181818181817 STD: 0.939256250837606\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.204225352112676 STD: 0.903411441450794\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.1818181818181817 STD: 0.939256250837606\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.1818181818181817 STD: 0.939256250837606\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.1818181818181817 STD: 0.939256250837606\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.204225352112676 STD: 0.903411441450794\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.1818181818181817 STD: 0.939256250837606\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.1818181818181817 STD: 0.939256250837606\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.211267605633803 STD: 0.8978381120207937\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.211267605633803 STD: 0.8978381120207937\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.1888111888111887 STD: 0.9341038407083238\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.2183098591549295 STD: 0.8921739898299311\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.2183098591549295 STD: 0.8921739898299311\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.2183098591549295 STD: 0.8921739898299311\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.2183098591549295 STD: 0.8921739898299311\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.2183098591549295 STD: 0.8921739898299311\n",
      "Last reward: 5.0\n",
      "Mean reward in replay buffer: 3.24822695035461 STD: 0.8956723601278884\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.2624113475177303 STD: 0.8915907939578078\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.269503546099291 STD: 0.8854331677942681\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.269503546099291 STD: 0.8854331677942681\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.269503546099291 STD: 0.8854331677942681\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.269503546099291 STD: 0.8854331677942681\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.269503546099291 STD: 0.8854331677942681\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.276595744680851 STD: 0.8791747996089726\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.276595744680851 STD: 0.8791747996089726\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.276595744680851 STD: 0.8791747996089726\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.276595744680851 STD: 0.8791747996089726\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.276595744680851 STD: 0.8791747996089726\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.276595744680851 STD: 0.8791747996089726\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.276595744680851 STD: 0.8791747996089726\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.276595744680851 STD: 0.8791747996089726\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.276595744680851 STD: 0.8791747996089726\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.2836879432624113 STD: 0.8728135223414955\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.2836879432624113 STD: 0.8728135223414955\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.2836879432624113 STD: 0.8728135223414955\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.2836879432624113 STD: 0.8728135223414955\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.2836879432624113 STD: 0.8728135223414955\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.2836879432624113 STD: 0.8728135223414955\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.2836879432624113 STD: 0.8728135223414955\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.2836879432624113 STD: 0.8728135223414955\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.2836879432624113 STD: 0.8728135223414955\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.2836879432624113 STD: 0.8728135223414955\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.2836879432624113 STD: 0.8728135223414955\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.2836879432624113 STD: 0.8728135223414955\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.2836879432624113 STD: 0.8728135223414955\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.2836879432624113 STD: 0.8728135223414955\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.2836879432624113 STD: 0.8728135223414955\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.2836879432624113 STD: 0.8728135223414955\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.2836879432624113 STD: 0.8728135223414955\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.2836879432624113 STD: 0.8728135223414955\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.307142857142857 STD: 0.8641245294901645\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.307142857142857 STD: 0.8641245294901645\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.307142857142857 STD: 0.8641245294901645\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.307142857142857 STD: 0.8641245294901645\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.307142857142857 STD: 0.8641245294901645\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.307142857142857 STD: 0.8641245294901645\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.3214285714285716 STD: 0.8588754329061596\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.3214285714285716 STD: 0.8588754329061596\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.3214285714285716 STD: 0.8588754329061596\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.3214285714285716 STD: 0.8588754329061596\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.3285714285714287 STD: 0.8519369141272262\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.3453237410071943 STD: 0.8402227984017212\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.3214285714285716 STD: 0.883647258172424\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.3285714285714287 STD: 0.8603400460639118\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.3285714285714287 STD: 0.8603400460639118\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.3453237410071943 STD: 0.8402227984017212\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.3214285714285716 STD: 0.883647258172424\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.3214285714285716 STD: 0.883647258172424\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.3214285714285716 STD: 0.883647258172424\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.3285714285714287 STD: 0.8603400460639118\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.3285714285714287 STD: 0.8603400460639118\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.3597122302158273 STD: 0.8341201582071966\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.3597122302158273 STD: 0.8341201582071966\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.3597122302158273 STD: 0.8341201582071966\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.3597122302158273 STD: 0.8341201582071966\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.3597122302158273 STD: 0.8341201582071966\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.3597122302158273 STD: 0.8341201582071966\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.3597122302158273 STD: 0.8341201582071966\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.3597122302158273 STD: 0.8341201582071966\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.3597122302158273 STD: 0.8341201582071966\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.3597122302158273 STD: 0.8341201582071966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.3597122302158273 STD: 0.8341201582071966\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.3597122302158273 STD: 0.8341201582071966\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.366906474820144 STD: 0.8265861773469003\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.366906474820144 STD: 0.8265861773469003\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.366906474820144 STD: 0.8265861773469003\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.366906474820144 STD: 0.8265861773469003\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.3840579710144927 STD: 0.8134136750963862\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.3597122302158273 STD: 0.859787589580801\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.391304347826087 STD: 0.8054085657547835\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.398550724637681 STD: 0.7972567479963706\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.398550724637681 STD: 0.7972567479963706\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.398550724637681 STD: 0.7972567479963706\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.398550724637681 STD: 0.7972567479963706\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.398550724637681 STD: 0.7972567479963706\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.398550724637681 STD: 0.7972567479963706\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.3741007194244603 STD: 0.8450485046962297\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.398550724637681 STD: 0.7972567479963706\n",
      "Last reward: 5.0\n",
      "Mean reward in replay buffer: 3.4306569343065694 STD: 0.7933141618436601\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.4306569343065694 STD: 0.7933141618436601\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.4306569343065694 STD: 0.7933141618436601\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.4057971014492754 STD: 0.8426382029159578\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.4057971014492754 STD: 0.8426382029159578\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.437956204379562 STD: 0.7846066255719293\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.437956204379562 STD: 0.7846066255719293\n",
      "Last reward: 11.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.003588333174345\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.003588333174345\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.003588333174345\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.003588333174345\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.003588333174345\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.003588333174345\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.003588333174345\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.003588333174345\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.003588333174345\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.003588333174345\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.003588333174345\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.003588333174345\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.003588333174345\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.003588333174345\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.003588333174345\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.003588333174345\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.003588333174345\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.003588333174345\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.533333333333333 STD: 0.991004314944595\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.01094231155812\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.01094231155812\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.01094231155812\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.5220588235294117 STD: 0.9960433707997666\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.5407407407407407 STD: 0.9907253709485986\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.0326900801997274\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.0326900801997274\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.5407407407407407 STD: 0.9907253709485986\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.0326900801997274\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.514705882352941 STD: 1.0326900801997274\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.5407407407407407 STD: 0.9907253709485986\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.5407407407407407 STD: 0.9907253709485986\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.5407407407407407 STD: 0.9907253709485986\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.5481481481481483 STD: 0.9903905344479808\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.5481481481481483 STD: 0.9903905344479808\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.5481481481481483 STD: 0.9903905344479808\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.5481481481481483 STD: 0.9903905344479808\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.5220588235294117 STD: 1.0325582165612917\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.5481481481481483 STD: 0.9903905344479808\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.5220588235294117 STD: 1.0325582165612917\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.5481481481481483 STD: 0.9903905344479808\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.5481481481481483 STD: 0.9903905344479808\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.5220588235294117 STD: 1.0325582165612917\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.5481481481481483 STD: 0.9903905344479808\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.5481481481481483 STD: 0.9903905344479808\n",
      "Last reward: 5.0\n",
      "Mean reward in replay buffer: 3.562962962962963 STD: 0.9970659000616671\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.562962962962963 STD: 0.9970659000616671\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.562962962962963 STD: 0.9970659000616671\n",
      "Last reward: 5.0\n",
      "Mean reward in replay buffer: 3.582089552238806 STD: 1.00598590000946\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.582089552238806 STD: 1.00598590000946\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.562962962962963 STD: 1.0265680209333892\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.562962962962963 STD: 1.0265680209333892\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.562962962962963 STD: 1.0265680209333892\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.58955223880597 STD: 1.0053442650334021\n",
      "Last reward: 5.0\n",
      "Mean reward in replay buffer: 3.6044776119402986 STD: 1.011354014726806\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.6044776119402986 STD: 1.011354014726806\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.611940298507463 STD: 1.0105492272867438\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.611940298507463 STD: 1.0105492272867438\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.611940298507463 STD: 1.0105492272867438\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.6194029850746268 STD: 1.0096882279124617\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.6194029850746268 STD: 1.0096882279124617\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.6194029850746268 STD: 1.0096882279124617\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.6194029850746268 STD: 1.0096882279124617\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.6194029850746268 STD: 1.0096882279124617\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.6194029850746268 STD: 1.0096882279124617\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.6194029850746268 STD: 1.0096882279124617\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.6194029850746268 STD: 1.0096882279124617\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.6194029850746268 STD: 1.0096882279124617\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.6194029850746268 STD: 1.0096882279124617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.6194029850746268 STD: 1.0096882279124617\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.6194029850746268 STD: 1.0096882279124617\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.6194029850746268 STD: 1.0096882279124617\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.6194029850746268 STD: 1.0096882279124617\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.6194029850746268 STD: 1.0096882279124617\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.6194029850746268 STD: 1.0096882279124617\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.6194029850746268 STD: 1.0096882279124617\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.6194029850746268 STD: 1.0096882279124617\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.6194029850746268 STD: 1.0096882279124617\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.6194029850746268 STD: 1.0096882279124617\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.6194029850746268 STD: 1.0096882279124617\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.6194029850746268 STD: 1.0096882279124617\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.626865671641791 STD: 1.0087708726710667\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.626865671641791 STD: 1.0087708726710667\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.626865671641791 STD: 1.0087708726710667\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.626865671641791 STD: 1.0087708726710667\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.626865671641791 STD: 1.0087708726710667\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.626865671641791 STD: 1.0087708726710667\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.626865671641791 STD: 1.0087708726710667\n",
      "Last reward: 6.0\n",
      "Mean reward in replay buffer: 3.6492537313432836 STD: 1.0278634142210934\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.6492537313432836 STD: 1.0278634142210934\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.6492537313432836 STD: 1.0278634142210934\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.6492537313432836 STD: 1.0278634142210934\n",
      "Last reward: 6.0\n",
      "Mean reward in replay buffer: 3.6766917293233083 STD: 1.0484394765876766\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.6766917293233083 STD: 1.0484394765876766\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.6766917293233083 STD: 1.0484394765876766\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.6766917293233083 STD: 1.0484394765876766\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.6492537313432836 STD: 1.0917149555442138\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.6492537313432836 STD: 1.0917149555442138\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.6842105263157894 STD: 1.0471347707292389\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.691729323308271 STD: 1.0457739712320042\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.691729323308271 STD: 1.0457739712320042\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.691729323308271 STD: 1.0457739712320042\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.691729323308271 STD: 1.0457739712320042\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.691729323308271 STD: 1.0457739712320042\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.691729323308271 STD: 1.0457739712320042\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0443568588255576\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0443568588255576\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0443568588255576\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0443568588255576\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0443568588255576\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0443568588255576\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0443568588255576\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0443568588255576\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0443568588255576\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0443568588255576\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0443568588255576\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0443568588255576\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0443568588255576\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0443568588255576\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0443568588255576\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0443568588255576\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0443568588255576\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0443568588255576\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0443568588255576\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0443568588255576\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0443568588255576\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0443568588255576\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0443568588255576\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.706766917293233 STD: 1.042883203949062\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.706766917293233 STD: 1.042883203949062\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.706766917293233 STD: 1.042883203949062\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.706766917293233 STD: 1.042883203949062\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.706766917293233 STD: 1.042883203949062\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.706766917293233 STD: 1.042883203949062\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.706766917293233 STD: 1.042883203949062\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.706766917293233 STD: 1.042883203949062\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.706766917293233 STD: 1.042883203949062\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.7196969696969697 STD: 1.0434411210607517\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.7196969696969697 STD: 1.0434411210607517\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.7196969696969697 STD: 1.0434411210607517\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.7196969696969697 STD: 1.0434411210607517\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.699248120300752 STD: 1.0658967088937517\n",
      "Last reward: 8.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.757575757575758 STD: 1.1059948519489544\n",
      "Last reward: 9.0\n",
      "Mean reward in replay buffer: 3.8091603053435112 STD: 1.1968837988504462\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.7954545454545454 STD: 1.2026601327327173\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.7954545454545454 STD: 1.2026601327327173\n",
      "Last reward: 6.0\n",
      "Mean reward in replay buffer: 3.83206106870229 STD: 1.2099120867982052\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.83206106870229 STD: 1.2099120867982052\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.83206106870229 STD: 1.2099120867982052\n",
      "Last reward: 5.0\n",
      "Mean reward in replay buffer: 3.8473282442748094 STD: 1.2119487323680125\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8473282442748094 STD: 1.2119487323680125\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.853846153846154 STD: 1.21433006037392\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.83206106870229 STD: 1.2350812602087562\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.83206106870229 STD: 1.2350812602087562\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.83206106870229 STD: 1.2350812602087562\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.83206106870229 STD: 1.2350812602087562\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.83206106870229 STD: 1.2350812602087562\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.8396946564885495 STD: 1.2204462859205976\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.8396946564885495 STD: 1.2204462859205976\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.8396946564885495 STD: 1.2204462859205976\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.8396946564885495 STD: 1.2204462859205976\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.8396946564885495 STD: 1.2204462859205976\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.8396946564885495 STD: 1.2204462859205976\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.853846153846154 STD: 1.21433006037392\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8244274809160306 STD: 1.255639071307052\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.8473282442748094 STD: 1.2119487323680125\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.8473282442748094 STD: 1.2119487323680125\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.8473282442748094 STD: 1.2119487323680125\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.8473282442748094 STD: 1.2119487323680125\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.8473282442748094 STD: 1.2119487323680125\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.8473282442748094 STD: 1.2119487323680125\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.8473282442748094 STD: 1.2119487323680125\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8473282442748094 STD: 1.2119487323680125\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8473282442748094 STD: 1.2119487323680125\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.8473282442748094 STD: 1.2119487323680125\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.8473282442748094 STD: 1.2119487323680125\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8473282442748094 STD: 1.2119487323680125\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.8473282442748094 STD: 1.2119487323680125\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.8473282442748094 STD: 1.2119487323680125\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8473282442748094 STD: 1.2119487323680125\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.8615384615384616 STD: 1.2120445034544747\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.83206106870229 STD: 1.2536265653984424\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.8396946564885495 STD: 1.2329875780341877\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8396946564885495 STD: 1.2329875780341877\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8396946564885495 STD: 1.2329875780341877\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.8473282442748094 STD: 1.2182792558658493\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.8615384615384616 STD: 1.2120445034544747\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.8396946564885495 STD: 1.2329875780341877\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.8473282442748094 STD: 1.2182792558658493\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8473282442748094 STD: 1.2182792558658493\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8473282442748094 STD: 1.2182792558658493\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.8473282442748094 STD: 1.2182792558658493\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8473282442748094 STD: 1.2182792558658493\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.8692307692307693 STD: 1.209705336268425\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.8692307692307693 STD: 1.209705336268425\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8396946564885495 STD: 1.2515639071739237\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.876923076923077 STD: 1.2073122472061752\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.876923076923077 STD: 1.2073122472061752\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.876923076923077 STD: 1.2073122472061752\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.876923076923077 STD: 1.2073122472061752\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.876923076923077 STD: 1.2073122472061752\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.876923076923077 STD: 1.2073122472061752\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.876923076923077 STD: 1.2073122472061752\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.876923076923077 STD: 1.2073122472061752\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.8846153846153846 STD: 1.2048649149707074\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.8846153846153846 STD: 1.2048649149707074\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.8846153846153846 STD: 1.2048649149707074\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8846153846153846 STD: 1.2048649149707074\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.8846153846153846 STD: 1.2048649149707074\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8692307692307693 STD: 1.2475617244891157\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.876923076923077 STD: 1.2264234546684933\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.876923076923077 STD: 1.2264234546684933\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8692307692307693 STD: 1.2475617244891157\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8692307692307693 STD: 1.2475617244891157\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8692307692307693 STD: 1.2475617244891157\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8692307692307693 STD: 1.2475617244891157\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8692307692307693 STD: 1.2475617244891157\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8692307692307693 STD: 1.2475617244891157\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8692307692307693 STD: 1.2475617244891157\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.876923076923077 STD: 1.2264234546684933\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8692307692307693 STD: 1.2475617244891157\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.876923076923077 STD: 1.2264234546684933\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.876923076923077 STD: 1.2264234546684933\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8692307692307693 STD: 1.2475617244891157\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8692307692307693 STD: 1.2475617244891157\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8692307692307693 STD: 1.2475617244891157\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8692307692307693 STD: 1.2475617244891157\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.876923076923077 STD: 1.2264234546684933\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8692307692307693 STD: 1.2475617244891157\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8692307692307693 STD: 1.2475617244891157\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.8692307692307693 STD: 1.2475617244891157\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.876923076923077 STD: 1.2264234546684933\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.876923076923077 STD: 1.2264234546684933\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.876923076923077 STD: 1.2264234546684933\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.876923076923077 STD: 1.2264234546684933\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.876923076923077 STD: 1.2264234546684933\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.89922480620155 STD: 1.2044511278502938\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.9069767441860463 STD: 1.2018336281563442\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.9069767441860463 STD: 1.2018336281563442\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.9069767441860463 STD: 1.2018336281563442\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.9069767441860463 STD: 1.2018336281563442\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.9069767441860463 STD: 1.2018336281563442\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.9069767441860463 STD: 1.2018336281563442\n",
      "Last reward: 2.0\n",
      "Mean reward in replay buffer: 3.9069767441860463 STD: 1.2018336281563442\n",
      "Last reward: 3.0\n",
      "Mean reward in replay buffer: 3.9069767441860463 STD: 1.2018336281563442\n",
      "Last reward: 1.0\n",
      "Mean reward in replay buffer: 3.9069767441860463 STD: 1.2018336281563442\n",
      "Last reward: 0.0\n",
      "Mean reward in replay buffer: 3.9069767441860463 STD: 1.2018336281563442\n",
      "Last reward: 4.0\n",
      "Mean reward in replay buffer: 3.9147286821705425 STD: 1.1991599126572032\n",
      "Current statistics: (3.9147286821705425, 1.1991599126572032)\n",
      "WARNING:tensorflow:From /Users/aydarakhmetzyanov/Documents/secondary_experience_replay/SERI.py:232: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "\n",
      "Copied model parameters to target network.\n",
      "Step 167 (167) @ Episode 1/10000, loss: 0.0012791345361620188WARNING:tensorflow:From /Users/aydarakhmetzyanov/Documents/secondary_experience_replay/SERI.py:313: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 332 (499) @ Episode 2/10000, loss: 0.0282517727464437578\n",
      "Episode Reward: 3.0\n",
      "Step 215 (714) @ Episode 3/10000, loss: 0.0013693871442228556\n",
      "Episode Reward: 1.0\n",
      "Step 236 (950) @ Episode 4/10000, loss: 0.0012535703135654333\n",
      "Episode Reward: 1.0\n",
      "Step 229 (1179) @ Episode 5/10000, loss: 0.0011934898793697357\n",
      "Episode Reward: 1.0\n",
      "Step 314 (1493) @ Episode 6/10000, loss: 0.0014594836393371224\n",
      "Episode Reward: 3.0\n",
      "Step 273 (1766) @ Episode 7/10000, loss: 0.0552298836410045638\n",
      "Episode Reward: 2.0\n",
      "Step 284 (2050) @ Episode 8/10000, loss: 0.0008075811201706529\n",
      "Episode Reward: 2.0\n",
      "Step 219 (2269) @ Episode 9/10000, loss: 0.00053014344302937397\n",
      "Episode Reward: 1.0\n",
      "Step 227 (2496) @ Episode 10/10000, loss: 0.00052275351481512195\n",
      "Episode Reward: 1.0\n",
      "Step 280 (2776) @ Episode 11/10000, loss: 6.877677515149117e-052\n",
      "Episode Reward: 2.0\n",
      "Step 182 (2958) @ Episode 12/10000, loss: 6.829843914601952e-058\n",
      "Episode Reward: 0.0\n",
      "Step 172 (3130) @ Episode 13/10000, loss: 0.02443841286003589615\n",
      "Episode Reward: 0.0\n",
      "Step 267 (3397) @ Episode 14/10000, loss: 0.00019763610907830298\n",
      "Episode Reward: 2.0\n",
      "Step 336 (3733) @ Episode 15/10000, loss: 0.00018230068963021046\n",
      "Episode Reward: 3.0\n",
      "Step 319 (4052) @ Episode 16/10000, loss: 0.02443828247487545598\n",
      "Episode Reward: 3.0\n",
      "Step 253 (4305) @ Episode 17/10000, loss: 0.02431730739772319851\n",
      "Episode Reward: 1.0\n",
      "Step 174 (4479) @ Episode 18/10000, loss: 5.251858237897977e-057\n",
      "Episode Reward: 0.0\n",
      "Step 317 (4796) @ Episode 19/10000, loss: 0.02454458549618721405\n",
      "Episode Reward: 3.0\n",
      "Step 247 (5043) @ Episode 20/10000, loss: 0.00011063006240874529\n",
      "Episode Reward: 2.0\n",
      "Step 234 (5277) @ Episode 21/10000, loss: 3.454954276094213e-055\n",
      "Episode Reward: 1.0\n",
      "Step 312 (5589) @ Episode 22/10000, loss: 0.00025240142713300884\n",
      "Episode Reward: 2.0\n",
      "Step 174 (5763) @ Episode 23/10000, loss: 4.764568438986316e-054\n",
      "Episode Reward: 0.0\n",
      "Step 245 (6008) @ Episode 24/10000, loss: 0.00023332536511588842\n",
      "Episode Reward: 1.0\n",
      "Step 297 (6305) @ Episode 25/10000, loss: 9.665701509220526e-055\n",
      "Episode Reward: 2.0\n",
      "Step 173 (6478) @ Episode 26/10000, loss: 7.982148235896602e-054\n",
      "Episode Reward: 0.0\n",
      "Step 173 (6651) @ Episode 27/10000, loss: 0.02462482824921608923\n",
      "Episode Reward: 0.0\n",
      "Step 228 (6879) @ Episode 28/10000, loss: 6.546321674250066e-055\n",
      "Episode Reward: 1.0\n",
      "Step 251 (7130) @ Episode 29/10000, loss: 0.00011925608851015568\n",
      "Episode Reward: 1.0\n",
      "Step 209 (7339) @ Episode 30/10000, loss: 0.00012604868970811367\n",
      "Episode Reward: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 340 (7679) @ Episode 31/10000, loss: 0.00030806090217083695\n",
      "Episode Reward: 3.0\n",
      "Step 235 (7914) @ Episode 32/10000, loss: 0.00019888598762918264\n",
      "Episode Reward: 1.0\n",
      "Step 178 (8092) @ Episode 33/10000, loss: 0.00010921037755906582\n",
      "Episode Reward: 0.0\n",
      "Step 183 (8275) @ Episode 34/10000, loss: 6.603218207601458e-053\n",
      "Episode Reward: 0.0\n",
      "Step 161 (8436) @ Episode 35/10000, loss: 0.02456243336200714485\n",
      "Episode Reward: 0.0\n",
      "Step 178 (8614) @ Episode 36/10000, loss: 9.837902325671166e-054\n",
      "Episode Reward: 0.0\n",
      "Step 437 (9051) @ Episode 37/10000, loss: 6.763824785593897e-055\n",
      "Episode Reward: 4.0\n",
      "Step 274 (9325) @ Episode 38/10000, loss: 0.00026376769528724253\n",
      "Episode Reward: 2.0\n",
      "Step 182 (9507) @ Episode 39/10000, loss: 0.00013814459089189768\n",
      "Episode Reward: 0.0\n",
      "Step 178 (9685) @ Episode 40/10000, loss: 0.02443743869662285052\n",
      "Episode Reward: 0.0\n",
      "Step 287 (9972) @ Episode 41/10000, loss: 0.02492544800043106534\n",
      "Episode Reward: 2.0\n",
      "Step 27 (9999) @ Episode 42/10000, loss: 0.02480277419090271415\n",
      "Copied model parameters to target network.\n",
      "Step 212 (10184) @ Episode 42/10000, loss: 0.02434205263853073777\n",
      "Episode Reward: 1.0\n",
      "Step 182 (10366) @ Episode 43/10000, loss: 0.02450420148670673455\n",
      "Episode Reward: 0.0\n",
      "Step 336 (10702) @ Episode 44/10000, loss: 0.00014607832417823374\n",
      "Episode Reward: 3.0\n",
      "Step 168 (10870) @ Episode 45/10000, loss: 6.832145299995318e-059\n",
      "Episode Reward: 0.0\n",
      "Step 189 (11059) @ Episode 46/10000, loss: 0.00014205802290234715\n",
      "Episode Reward: 0.0\n",
      "Step 231 (11290) @ Episode 47/10000, loss: 0.04918562620878219621\n",
      "Episode Reward: 1.0\n",
      "Step 227 (11517) @ Episode 48/10000, loss: 3.883250246872194e-056\n",
      "Episode Reward: 1.0\n",
      "Step 203 (11720) @ Episode 49/10000, loss: 0.02401654981076717443\n",
      "Episode Reward: 0.0\n",
      "Step 199 (11919) @ Episode 50/10000, loss: 0.00016505413805134594\n",
      "Episode Reward: 0.0\n",
      "Step 218 (12137) @ Episode 51/10000, loss: 0.00020089330791961402\n",
      "Episode Reward: 1.0\n",
      "Step 174 (12311) @ Episode 52/10000, loss: 0.00013615556235890836\n",
      "Episode Reward: 0.0\n",
      "Step 218 (12529) @ Episode 53/10000, loss: 9.429789497517049e-056\n",
      "Episode Reward: 1.0\n",
      "Step 272 (12801) @ Episode 54/10000, loss: 0.00012367086310405284\n",
      "Episode Reward: 2.0\n",
      "Step 171 (12972) @ Episode 55/10000, loss: 0.00022690088371746242\n",
      "Episode Reward: 0.0\n",
      "Step 169 (13141) @ Episode 56/10000, loss: 0.00018903109594248235\n",
      "Episode Reward: 0.0\n",
      "Step 444 (13585) @ Episode 57/10000, loss: 0.02443655952811241717\n",
      "Episode Reward: 4.0\n",
      "Step 413 (13998) @ Episode 58/10000, loss: 0.00018569936219137162\n",
      "Episode Reward: 4.0\n",
      "Step 273 (14271) @ Episode 59/10000, loss: 0.00028196917264722298\n",
      "Episode Reward: 2.0\n",
      "Step 255 (14526) @ Episode 60/10000, loss: 0.00016171031165868044\n",
      "Episode Reward: 1.0\n",
      "Step 351 (14877) @ Episode 61/10000, loss: 0.02436278201639652352\n",
      "Episode Reward: 3.0\n",
      "Step 315 (15192) @ Episode 62/10000, loss: 0.00016890243568923324\n",
      "Episode Reward: 2.0\n",
      "Step 234 (15426) @ Episode 63/10000, loss: 0.02442282438278198205\n",
      "Episode Reward: 1.0\n",
      "Step 233 (15659) @ Episode 64/10000, loss: 8.400242950301617e-054\n",
      "Episode Reward: 1.0\n",
      "Step 235 (15894) @ Episode 65/10000, loss: 0.02478579059243202205\n",
      "Episode Reward: 1.0\n",
      "Step 273 (16167) @ Episode 66/10000, loss: 3.9278518670471385e-05\n",
      "Episode Reward: 2.0\n",
      "Step 189 (16356) @ Episode 67/10000, loss: 0.00012193307338748127\n",
      "Episode Reward: 0.0\n",
      "Step 281 (16637) @ Episode 68/10000, loss: 0.00026097608497366313\n",
      "Episode Reward: 2.0\n",
      "Step 179 (16816) @ Episode 69/10000, loss: 0.02412987314164638526\n",
      "Episode Reward: 0.0\n",
      "Step 200 (17016) @ Episode 70/10000, loss: 0.02461197599768638628\n",
      "Episode Reward: 1.0\n",
      "Step 383 (17399) @ Episode 71/10000, loss: 0.00010768460197141394\n",
      "Episode Reward: 4.0\n",
      "Step 166 (17565) @ Episode 72/10000, loss: 8.41183791635558e-0546\n",
      "Episode Reward: 0.0\n",
      "Step 180 (17745) @ Episode 73/10000, loss: 4.06177896365989e-0505\n",
      "Episode Reward: 0.0\n",
      "Step 168 (17913) @ Episode 74/10000, loss: 2.6970921680913307e-05\n",
      "Episode Reward: 0.0\n",
      "Step 250 (18163) @ Episode 75/10000, loss: 0.00022193200129549955\n",
      "Episode Reward: 1.0\n",
      "Step 273 (18436) @ Episode 76/10000, loss: 0.00010880261834245175\n",
      "Episode Reward: 2.0\n",
      "Step 341 (18777) @ Episode 77/10000, loss: 0.00036298373015597466\n",
      "Episode Reward: 6.0\n",
      "Step 250 (19027) @ Episode 78/10000, loss: 0.00058826588792726438\n",
      "Episode Reward: 1.0\n",
      "Step 273 (19300) @ Episode 79/10000, loss: 0.02434628829360008234\n",
      "Episode Reward: 2.0\n",
      "Step 246 (19546) @ Episode 80/10000, loss: 0.00049189187120646247\n",
      "Episode Reward: 1.0\n",
      "Step 338 (19884) @ Episode 81/10000, loss: 0.00011372299923095852\n",
      "Episode Reward: 3.0\n",
      "Step 115 (19999) @ Episode 82/10000, loss: 0.00016524411330465227\n",
      "Copied model parameters to target network.\n",
      "Step 300 (20184) @ Episode 82/10000, loss: 3.487940921331756e-057\n",
      "Episode Reward: 2.0\n",
      "Step 413 (20597) @ Episode 83/10000, loss: 0.00014058126544114202\n",
      "Episode Reward: 4.0\n",
      "Step 289 (20886) @ Episode 84/10000, loss: 0.00102372444234788423\n",
      "Episode Reward: 2.0\n",
      "Step 174 (21060) @ Episode 85/10000, loss: 4.169106250628829e-055\n",
      "Episode Reward: 0.0\n",
      "Step 332 (21392) @ Episode 86/10000, loss: 0.02289696037769317676\n",
      "Episode Reward: 3.0\n",
      "Step 296 (21688) @ Episode 87/10000, loss: 5.5611504649277776e-05\n",
      "Episode Reward: 2.0\n",
      "Step 303 (21991) @ Episode 88/10000, loss: 7.873872527852654e-056\n",
      "Episode Reward: 2.0\n",
      "Step 242 (22233) @ Episode 89/10000, loss: 0.00029120087856426835\n",
      "Episode Reward: 1.0\n",
      "Step 285 (22518) @ Episode 90/10000, loss: 0.00014631380327045918\n",
      "Episode Reward: 2.0\n",
      "Step 330 (22848) @ Episode 91/10000, loss: 0.02171518281102180525\n",
      "Episode Reward: 4.0\n",
      "Step 165 (23013) @ Episode 92/10000, loss: 0.00011154876847285777\n",
      "Episode Reward: 0.0\n",
      "Step 202 (23215) @ Episode 93/10000, loss: 0.00105859385803341874\n",
      "Episode Reward: 1.0\n",
      "Step 219 (23434) @ Episode 94/10000, loss: 0.00102980074007064185\n",
      "Episode Reward: 1.0\n",
      "Step 179 (23613) @ Episode 95/10000, loss: 0.00049354619113728456\n",
      "Episode Reward: 0.0\n",
      "Step 172 (23785) @ Episode 96/10000, loss: 0.00108014012221246966\n",
      "Episode Reward: 0.0\n",
      "Step 176 (23961) @ Episode 97/10000, loss: 0.00063840445363894194\n",
      "Episode Reward: 0.0\n",
      "Step 167 (24128) @ Episode 98/10000, loss: 0.00408208882436156323\n",
      "Episode Reward: 0.0\n",
      "Step 304 (24432) @ Episode 99/10000, loss: 0.00130213925149291751\n",
      "Episode Reward: 3.0\n",
      "Step 186 (24618) @ Episode 100/10000, loss: 0.00028883045888505876\n",
      "Episode Reward: 0.0\n",
      "Step 247 (24865) @ Episode 101/10000, loss: 0.00021067276247777045\n",
      "Episode Reward: 1.0\n",
      "Step 166 (25031) @ Episode 102/10000, loss: 0.00255021965131163613\n",
      "Episode Reward: 0.0\n",
      "Step 246 (25277) @ Episode 103/10000, loss: 0.00190716516226530085\n",
      "Episode Reward: 1.0\n",
      "Step 247 (25524) @ Episode 104/10000, loss: 0.00115138792898505931\n",
      "Episode Reward: 2.0\n",
      "Step 312 (25836) @ Episode 105/10000, loss: 0.00088406709255650645\n",
      "Episode Reward: 3.0\n",
      "Step 212 (26048) @ Episode 106/10000, loss: 0.01230798568576574373\n",
      "Episode Reward: 1.0\n",
      "Step 257 (26305) @ Episode 107/10000, loss: 0.00021896520047448575\n",
      "Episode Reward: 2.0\n",
      "Step 174 (26479) @ Episode 108/10000, loss: 0.00026521668769419193\n",
      "Episode Reward: 0.0\n",
      "Step 178 (26657) @ Episode 109/10000, loss: 0.00061718549113720664\n",
      "Episode Reward: 0.0\n",
      "Step 164 (26821) @ Episode 110/10000, loss: 0.00200297660194337377\n",
      "Episode Reward: 0.0\n",
      "Step 171 (26992) @ Episode 111/10000, loss: 0.01539248786866664984\n",
      "Episode Reward: 0.0\n",
      "Step 188 (27180) @ Episode 112/10000, loss: 0.00053788919467478993\n",
      "Episode Reward: 0.0\n",
      "Step 206 (27386) @ Episode 113/10000, loss: 0.00017133163055405023\n",
      "Episode Reward: 1.0\n",
      "Step 187 (27573) @ Episode 114/10000, loss: 0.00025467068189755082\n",
      "Episode Reward: 0.0\n",
      "Step 171 (27744) @ Episode 115/10000, loss: 0.01610071398317814052\n",
      "Episode Reward: 0.0\n",
      "Step 178 (27922) @ Episode 116/10000, loss: 0.00214094528928399145\n",
      "Episode Reward: 0.0\n",
      "Step 296 (28218) @ Episode 117/10000, loss: 0.00012087468348909174\n",
      "Episode Reward: 2.0\n",
      "Step 173 (28391) @ Episode 118/10000, loss: 0.00639625871554017173\n",
      "Episode Reward: 0.0\n",
      "Step 254 (28645) @ Episode 119/10000, loss: 0.00035680137807503344\n",
      "Episode Reward: 2.0\n",
      "Step 268 (28913) @ Episode 120/10000, loss: 0.00043776346137747175\n",
      "Episode Reward: 2.0\n",
      "Step 349 (29262) @ Episode 121/10000, loss: 0.00164277665317058564\n",
      "Episode Reward: 3.0\n",
      "Step 231 (29493) @ Episode 122/10000, loss: 0.00010726433538366109\n",
      "Episode Reward: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 272 (29765) @ Episode 123/10000, loss: 0.00013014259457122535\n",
      "Episode Reward: 2.0\n",
      "Step 209 (29974) @ Episode 124/10000, loss: 0.00060584838502109057\n",
      "Episode Reward: 1.0\n",
      "Step 25 (29999) @ Episode 125/10000, loss: 0.0004152805486228317\n",
      "Copied model parameters to target network.\n",
      "Step 189 (30163) @ Episode 125/10000, loss: 0.00124829239211976535\n",
      "Episode Reward: 0.0\n",
      "Step 180 (30343) @ Episode 126/10000, loss: 0.00062452512793242933\n",
      "Episode Reward: 0.0\n",
      "Step 186 (30529) @ Episode 127/10000, loss: 0.00226313481107354164\n",
      "Episode Reward: 0.0\n",
      "Step 178 (30707) @ Episode 128/10000, loss: 0.00010379004379501566\n",
      "Episode Reward: 0.0\n",
      "Step 209 (30916) @ Episode 129/10000, loss: 0.00108393782284110787\n",
      "Episode Reward: 1.0\n",
      "Step 208 (31124) @ Episode 130/10000, loss: 0.00024601153563708067\n",
      "Episode Reward: 1.0\n",
      "Step 252 (31376) @ Episode 131/10000, loss: 0.00023289848468266428\n",
      "Episode Reward: 1.0\n",
      "Step 185 (31561) @ Episode 132/10000, loss: 0.00022063982032705098\n",
      "Episode Reward: 0.0\n",
      "Step 406 (31967) @ Episode 133/10000, loss: 0.00165208021644502883\n",
      "Episode Reward: 4.0\n",
      "Step 216 (32183) @ Episode 134/10000, loss: 0.00019393468392081562\n",
      "Episode Reward: 1.0\n",
      "Step 237 (32420) @ Episode 135/10000, loss: 0.00021446472965180874\n",
      "Episode Reward: 1.0\n",
      "Step 265 (32685) @ Episode 136/10000, loss: 0.00015528713993262577\n",
      "Episode Reward: 2.0\n",
      "Step 180 (32865) @ Episode 137/10000, loss: 0.00011383250966900964\n",
      "Episode Reward: 0.0\n",
      "Step 336 (33201) @ Episode 138/10000, loss: 0.00028249048045836396\n",
      "Episode Reward: 3.0\n",
      "Step 174 (33375) @ Episode 139/10000, loss: 0.00718210032209754445\n",
      "Episode Reward: 0.0\n",
      "Step 330 (33705) @ Episode 140/10000, loss: 0.00027035418315790594\n",
      "Episode Reward: 3.0\n",
      "Step 175 (33880) @ Episode 141/10000, loss: 0.00022300193086266518\n",
      "Episode Reward: 0.0\n",
      "Step 230 (34110) @ Episode 142/10000, loss: 0.00041313213296234613\n",
      "Episode Reward: 1.0\n",
      "Step 249 (34359) @ Episode 143/10000, loss: 0.00072365684900432836\n",
      "Episode Reward: 2.0\n",
      "Step 179 (34538) @ Episode 144/10000, loss: 0.00133243820164352667\n",
      "Episode Reward: 0.0\n",
      "Step 217 (34755) @ Episode 145/10000, loss: 0.00035786195076070726\n",
      "Episode Reward: 1.0\n",
      "Step 224 (34979) @ Episode 146/10000, loss: 0.00036865542642772264\n",
      "Episode Reward: 1.0\n",
      "Step 234 (35213) @ Episode 147/10000, loss: 0.00128805963322520267\n",
      "Episode Reward: 1.0\n",
      "Step 242 (35455) @ Episode 148/10000, loss: 0.00282400776632130156\n",
      "Episode Reward: 1.0\n",
      "Step 338 (35793) @ Episode 149/10000, loss: 0.00082149577792733917\n",
      "Episode Reward: 3.0\n",
      "Step 284 (36077) @ Episode 150/10000, loss: 0.03506769984960556586\n",
      "Episode Reward: 2.0\n",
      "Step 238 (36315) @ Episode 151/10000, loss: 0.00042994815157726407\n",
      "Episode Reward: 1.0\n",
      "Step 308 (36623) @ Episode 152/10000, loss: 8.136891847243533e-051\n",
      "Episode Reward: 2.0\n",
      "Step 184 (36807) @ Episode 153/10000, loss: 0.00019601972599048167\n",
      "Episode Reward: 0.0\n",
      "Step 290 (37097) @ Episode 154/10000, loss: 7.26186772226356e-0503\n",
      "Episode Reward: 2.0\n",
      "Step 225 (37322) @ Episode 155/10000, loss: 0.00407754955813288735\n",
      "Episode Reward: 1.0\n",
      "Step 519 (37841) @ Episode 156/10000, loss: 0.00146199238952249294\n",
      "Episode Reward: 6.0\n",
      "Step 306 (38147) @ Episode 157/10000, loss: 0.00035511981695890427\n",
      "Episode Reward: 3.0\n",
      "Step 169 (38316) @ Episode 158/10000, loss: 0.00080365332541987376\n",
      "Episode Reward: 0.0\n",
      "Step 169 (38485) @ Episode 159/10000, loss: 0.00482688751071691565\n",
      "Episode Reward: 0.0\n",
      "Step 229 (38714) @ Episode 160/10000, loss: 0.01568487100303173646\n",
      "Episode Reward: 1.0\n",
      "Step 211 (38925) @ Episode 161/10000, loss: 0.00025873922277241945\n",
      "Episode Reward: 1.0\n",
      "Step 181 (39106) @ Episode 162/10000, loss: 0.00014359873603098094\n",
      "Episode Reward: 0.0\n",
      "Step 254 (39360) @ Episode 163/10000, loss: 0.00027245370438322425\n",
      "Episode Reward: 1.0\n",
      "Step 224 (39584) @ Episode 164/10000, loss: 5.9516856708796695e-05\n",
      "Episode Reward: 1.0\n",
      "Step 300 (39884) @ Episode 165/10000, loss: 0.01175876334309578737\n",
      "Episode Reward: 2.0\n",
      "Step 115 (39999) @ Episode 166/10000, loss: 0.00021210948762018234\n",
      "Copied model parameters to target network.\n",
      "Step 235 (40119) @ Episode 166/10000, loss: 0.00057759415358304988\n",
      "Episode Reward: 1.0\n",
      "Step 231 (40350) @ Episode 167/10000, loss: 0.00059537624474614864\n",
      "Episode Reward: 1.0\n",
      "Step 296 (40646) @ Episode 168/10000, loss: 0.00280561391264200277\n",
      "Episode Reward: 2.0\n",
      "Step 230 (40876) @ Episode 169/10000, loss: 0.00036845222348347306\n",
      "Episode Reward: 1.0\n",
      "Step 258 (41134) @ Episode 170/10000, loss: 0.00013067499094177037\n",
      "Episode Reward: 1.0\n",
      "Step 172 (41306) @ Episode 171/10000, loss: 3.783558349823579e-055\n",
      "Episode Reward: 0.0\n",
      "Step 304 (41610) @ Episode 172/10000, loss: 0.00728281820192933197\n",
      "Episode Reward: 2.0\n",
      "Step 181 (41791) @ Episode 173/10000, loss: 0.00016647735901642597\n",
      "Episode Reward: 0.0\n",
      "Step 170 (41961) @ Episode 174/10000, loss: 0.00011092523345723748\n",
      "Episode Reward: 0.0\n",
      "Step 373 (42334) @ Episode 175/10000, loss: 9.202687942888588e-055\n",
      "Episode Reward: 3.0\n",
      "Step 175 (42509) @ Episode 176/10000, loss: 0.00042922646389342844\n",
      "Episode Reward: 0.0\n",
      "Step 281 (42790) @ Episode 177/10000, loss: 0.00022773905948270112\n",
      "Episode Reward: 2.0\n",
      "Step 198 (42988) @ Episode 178/10000, loss: 0.00055480783339589833\n",
      "Episode Reward: 0.0\n",
      "Step 172 (43160) @ Episode 179/10000, loss: 0.00013436139852274214\n",
      "Episode Reward: 0.0\n",
      "Step 396 (43556) @ Episode 180/10000, loss: 0.00029039659420959659\n",
      "Episode Reward: 3.0\n",
      "Step 173 (43729) @ Episode 181/10000, loss: 9.535889694234356e-055\n",
      "Episode Reward: 0.0\n",
      "Step 174 (43903) @ Episode 182/10000, loss: 5.487368980539031e-051\n",
      "Episode Reward: 0.0\n",
      "Step 320 (44223) @ Episode 183/10000, loss: 0.00044988057925365865\n",
      "Episode Reward: 2.0\n",
      "Step 266 (44489) @ Episode 184/10000, loss: 0.00298543018288910442\n",
      "Episode Reward: 2.0\n",
      "Step 269 (44758) @ Episode 185/10000, loss: 9.646253602113575e-053\n",
      "Episode Reward: 2.0\n",
      "Step 276 (45034) @ Episode 186/10000, loss: 0.00248893140815198435\n",
      "Episode Reward: 2.0\n",
      "Step 218 (45252) @ Episode 187/10000, loss: 0.00112660578452050695\n",
      "Episode Reward: 1.0\n",
      "Step 270 (45522) @ Episode 188/10000, loss: 5.3046172979520634e-05\n",
      "Episode Reward: 2.0\n",
      "Step 407 (45929) @ Episode 189/10000, loss: 0.00026007089763879776\n",
      "Episode Reward: 4.0\n",
      "Step 302 (46231) @ Episode 190/10000, loss: 0.00162315554916858673\n",
      "Episode Reward: 2.0\n",
      "Step 166 (46397) @ Episode 191/10000, loss: 0.00142827932722866547\n",
      "Episode Reward: 0.0\n",
      "Step 369 (46766) @ Episode 192/10000, loss: 0.00015579808678012345\n",
      "Episode Reward: 4.0\n",
      "Step 170 (46936) @ Episode 193/10000, loss: 0.00514814630150795365\n",
      "Episode Reward: 0.0\n",
      "Step 168 (47104) @ Episode 194/10000, loss: 0.00117618171498179443\n",
      "Episode Reward: 0.0\n",
      "Step 245 (47349) @ Episode 195/10000, loss: 0.00638839136809110605\n",
      "Episode Reward: 1.0\n",
      "Step 181 (47530) @ Episode 196/10000, loss: 0.00013440746988635512\n",
      "Episode Reward: 0.0\n",
      "Step 165 (47695) @ Episode 197/10000, loss: 0.00059929577400907875\n",
      "Episode Reward: 0.0\n",
      "Step 343 (48038) @ Episode 198/10000, loss: 0.00023970373149495572\n",
      "Episode Reward: 3.0\n",
      "Step 199 (48237) @ Episode 199/10000, loss: 0.00014775854651816198\n",
      "Episode Reward: 1.0\n",
      "Step 175 (48412) @ Episode 200/10000, loss: 0.00095517793670296676\n",
      "Episode Reward: 0.0\n",
      "Step 162 (48574) @ Episode 201/10000, loss: 0.00012740635429508984\n",
      "Episode Reward: 0.0\n",
      "Step 326 (48900) @ Episode 202/10000, loss: 0.00019881369371432816\n",
      "Episode Reward: 3.0\n",
      "Step 177 (49077) @ Episode 203/10000, loss: 0.00032666150946170096\n",
      "Episode Reward: 0.0\n",
      "Step 239 (49316) @ Episode 204/10000, loss: 0.00114087969996035135\n",
      "Episode Reward: 1.0\n",
      "Step 179 (49495) @ Episode 205/10000, loss: 0.00202229782007634643\n",
      "Episode Reward: 0.0\n",
      "Step 239 (49734) @ Episode 206/10000, loss: 6.325398135231808e-053\n",
      "Episode Reward: 1.0\n",
      "Step 228 (49962) @ Episode 207/10000, loss: 0.00015375651128124446\n",
      "Episode Reward: 1.0\n",
      "Step 37 (49999) @ Episode 208/10000, loss: 0.00038846259121783084\n",
      "Copied model parameters to target network.\n",
      "Step 275 (50237) @ Episode 208/10000, loss: 0.00065133697353303437\n",
      "Episode Reward: 2.0\n",
      "Step 353 (50590) @ Episode 209/10000, loss: 0.00046397239202633555\n",
      "Episode Reward: 3.0\n",
      "Step 165 (50755) @ Episode 210/10000, loss: 0.00085174385458230973\n",
      "Episode Reward: 0.0\n",
      "Step 173 (50928) @ Episode 211/10000, loss: 0.00219539413228631691\n",
      "Episode Reward: 0.0\n",
      "Step 173 (51101) @ Episode 212/10000, loss: 0.00025424041086807847\n",
      "Episode Reward: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 305 (51406) @ Episode 213/10000, loss: 0.00014047960576135665\n",
      "Episode Reward: 2.0\n",
      "Step 238 (51644) @ Episode 214/10000, loss: 0.00014748239482287318\n",
      "Episode Reward: 1.0\n",
      "Step 224 (51868) @ Episode 215/10000, loss: 0.00103605329059064397\n",
      "Episode Reward: 1.0\n",
      "Step 323 (52191) @ Episode 216/10000, loss: 7.26766447769478e-0598\n",
      "Episode Reward: 2.0\n",
      "Step 348 (52539) @ Episode 217/10000, loss: 0.00011637662828434259\n",
      "Episode Reward: 3.0\n",
      "Step 174 (52713) @ Episode 218/10000, loss: 0.00010184268467128277\n",
      "Episode Reward: 0.0\n",
      "Step 373 (53086) @ Episode 219/10000, loss: 0.00425273412838578274\n",
      "Episode Reward: 3.0\n",
      "Step 256 (53342) @ Episode 220/10000, loss: 9.542237967252731e-055\n",
      "Episode Reward: 2.0\n",
      "Step 163 (53505) @ Episode 221/10000, loss: 0.00020841830701101577\n",
      "Episode Reward: 0.0\n",
      "Step 238 (53743) @ Episode 222/10000, loss: 0.00090781750623136767\n",
      "Episode Reward: 1.0\n",
      "Step 181 (53924) @ Episode 223/10000, loss: 0.00015535535931121558\n",
      "Episode Reward: 0.0\n",
      "Step 180 (54104) @ Episode 224/10000, loss: 0.00014887163706589497\n",
      "Episode Reward: 0.0\n",
      "Step 232 (54336) @ Episode 225/10000, loss: 0.00120484014041721824\n",
      "Episode Reward: 1.0\n",
      "Step 171 (54507) @ Episode 226/10000, loss: 0.00050631543854251537\n",
      "Episode Reward: 0.0\n",
      "Step 183 (54690) @ Episode 227/10000, loss: 0.00077074085129424933\n",
      "Episode Reward: 0.0\n",
      "Step 390 (55080) @ Episode 228/10000, loss: 8.26746691018343e-0553\n",
      "Episode Reward: 3.0\n",
      "Step 285 (55365) @ Episode 229/10000, loss: 0.00073543586768209939\n",
      "Episode Reward: 2.0\n",
      "Step 166 (55531) @ Episode 230/10000, loss: 5.325086021912284e-055\n",
      "Episode Reward: 0.0\n",
      "Step 181 (55712) @ Episode 231/10000, loss: 9.057259740075096e-056\n",
      "Episode Reward: 0.0\n",
      "Step 206 (55918) @ Episode 232/10000, loss: 4.859455657424405e-055\n",
      "Episode Reward: 1.0\n",
      "Step 180 (56098) @ Episode 233/10000, loss: 0.00028253858909010887\n",
      "Episode Reward: 0.0\n",
      "Step 188 (56286) @ Episode 234/10000, loss: 4.012764838989824e-054\n",
      "Episode Reward: 0.0\n",
      "Step 343 (56629) @ Episode 235/10000, loss: 0.00037327155587263405\n",
      "Episode Reward: 3.0\n",
      "Step 231 (56860) @ Episode 236/10000, loss: 0.00117375480476766823\n",
      "Episode Reward: 1.0\n",
      "Step 359 (57219) @ Episode 237/10000, loss: 0.00124657177366316327\n",
      "Episode Reward: 3.0\n",
      "Step 195 (57414) @ Episode 238/10000, loss: 0.00140499416738748555\n",
      "Episode Reward: 0.0\n",
      "Step 271 (57685) @ Episode 239/10000, loss: 5.574401075136848e-057\n",
      "Episode Reward: 2.0\n",
      "Step 204 (57889) @ Episode 240/10000, loss: 0.00022303967853076756\n",
      "Episode Reward: 1.0\n",
      "Step 254 (58143) @ Episode 241/10000, loss: 0.00064414506778120995\n",
      "Episode Reward: 1.0\n",
      "Step 202 (58345) @ Episode 242/10000, loss: 0.00132205220870673667\n",
      "Episode Reward: 1.0\n",
      "Step 233 (58578) @ Episode 243/10000, loss: 0.00040166717371903366\n",
      "Episode Reward: 1.0\n",
      "Step 177 (58755) @ Episode 244/10000, loss: 0.00077022163895890124\n",
      "Episode Reward: 0.0\n",
      "Step 400 (59155) @ Episode 245/10000, loss: 0.00050648621981963526\n",
      "Episode Reward: 4.0\n",
      "Step 375 (59530) @ Episode 246/10000, loss: 0.00011793968587880954\n",
      "Episode Reward: 4.0\n",
      "Step 302 (59832) @ Episode 247/10000, loss: 0.00218883086927235136\n",
      "Episode Reward: 2.0\n",
      "Step 167 (59999) @ Episode 248/10000, loss: 3.488737638690509e-054\n",
      "Copied model parameters to target network.\n",
      "Step 467 (60299) @ Episode 248/10000, loss: 0.00066289177630096677\n",
      "Episode Reward: 6.0\n",
      "Step 267 (60566) @ Episode 249/10000, loss: 0.00047593907220289114\n",
      "Episode Reward: 2.0\n",
      "Step 232 (60798) @ Episode 250/10000, loss: 0.00336248008534312254\n",
      "Episode Reward: 1.0\n",
      "Step 356 (61154) @ Episode 251/10000, loss: 0.00034076804877258837\n",
      "Episode Reward: 3.0\n",
      "Step 332 (61486) @ Episode 252/10000, loss: 0.00031673262128606445\n",
      "Episode Reward: 3.0\n",
      "Step 251 (61737) @ Episode 253/10000, loss: 0.00035925750853493814\n",
      "Episode Reward: 1.0\n",
      "Step 165 (61902) @ Episode 254/10000, loss: 0.00463935779407620463\n",
      "Episode Reward: 0.0\n",
      "Step 163 (62065) @ Episode 255/10000, loss: 0.00059751799562945962\n",
      "Episode Reward: 0.0\n",
      "Step 172 (62237) @ Episode 256/10000, loss: 0.00186906009912490845\n",
      "Episode Reward: 0.0\n",
      "Step 282 (62519) @ Episode 257/10000, loss: 0.00025704377912916243\n",
      "Episode Reward: 2.0\n",
      "Step 223 (62742) @ Episode 258/10000, loss: 0.00053268804913386702\n",
      "Episode Reward: 1.0\n",
      "Step 327 (63069) @ Episode 259/10000, loss: 0.00094086612807586797\n",
      "Episode Reward: 2.0\n",
      "Step 233 (63302) @ Episode 260/10000, loss: 0.00019790371879935265\n",
      "Episode Reward: 1.0\n",
      "Step 205 (63507) @ Episode 261/10000, loss: 0.00034364435123279694\n",
      "Episode Reward: 1.0\n",
      "Step 312 (63819) @ Episode 262/10000, loss: 8.870796591509134e-053\n",
      "Episode Reward: 3.0\n",
      "Step 222 (64041) @ Episode 263/10000, loss: 0.00084879889618605385\n",
      "Episode Reward: 1.0\n",
      "Step 203 (64244) @ Episode 264/10000, loss: 0.00367129407823085866\n",
      "Episode Reward: 1.0\n",
      "Step 308 (64552) @ Episode 265/10000, loss: 0.00088715477613732225\n",
      "Episode Reward: 3.0\n",
      "Step 233 (64785) @ Episode 266/10000, loss: 0.00096754066180437855\n",
      "Episode Reward: 1.0\n",
      "Step 297 (65082) @ Episode 267/10000, loss: 0.00040567517862655223\n",
      "Episode Reward: 2.0\n",
      "Step 211 (65293) @ Episode 268/10000, loss: 0.00095095520373433832\n",
      "Episode Reward: 1.0\n",
      "Step 363 (65656) @ Episode 269/10000, loss: 0.00081116415094584233\n",
      "Episode Reward: 3.0\n",
      "Step 270 (65926) @ Episode 270/10000, loss: 2.500277514627669e-054\n",
      "Episode Reward: 2.0\n",
      "Step 243 (66169) @ Episode 271/10000, loss: 0.00083935714792460257\n",
      "Episode Reward: 1.0\n",
      "Step 309 (66478) @ Episode 272/10000, loss: 0.00103000574745237834\n",
      "Episode Reward: 2.0\n",
      "Step 248 (66726) @ Episode 273/10000, loss: 0.00079608208034187563\n",
      "Episode Reward: 1.0\n",
      "Step 221 (66947) @ Episode 274/10000, loss: 0.00073612865526229141\n",
      "Episode Reward: 1.0\n",
      "Step 241 (67188) @ Episode 275/10000, loss: 0.00016022818454075605\n",
      "Episode Reward: 1.0\n",
      "Step 177 (67365) @ Episode 276/10000, loss: 0.00011282953346380964\n",
      "Episode Reward: 0.0\n",
      "Step 211 (67576) @ Episode 277/10000, loss: 0.00161651498638093472\n",
      "Episode Reward: 1.0\n",
      "Step 176 (67752) @ Episode 278/10000, loss: 0.00028073770226910718\n",
      "Episode Reward: 0.0\n",
      "Step 168 (67920) @ Episode 279/10000, loss: 9.405397577211261e-056\n",
      "Episode Reward: 0.0\n",
      "Step 176 (68096) @ Episode 280/10000, loss: 0.00018515232659410685\n",
      "Episode Reward: 0.0\n",
      "Step 359 (68455) @ Episode 281/10000, loss: 0.00025880633620545276\n",
      "Episode Reward: 3.0\n",
      "Step 391 (68846) @ Episode 282/10000, loss: 0.00157270545605570085\n",
      "Episode Reward: 4.0\n",
      "Step 189 (69035) @ Episode 283/10000, loss: 0.00155012472532689577\n",
      "Episode Reward: 0.0\n",
      "Step 260 (69295) @ Episode 284/10000, loss: 0.00102783902548253545\n",
      "Episode Reward: 2.0\n",
      "Step 341 (69636) @ Episode 285/10000, loss: 0.00023339138715527952\n",
      "Episode Reward: 3.0\n",
      "Step 236 (69872) @ Episode 286/10000, loss: 0.00127044762484729349\n",
      "Episode Reward: 1.0\n",
      "Step 127 (69999) @ Episode 287/10000, loss: 2.7730193323804997e-05\n",
      "Copied model parameters to target network.\n",
      "Step 223 (70095) @ Episode 287/10000, loss: 0.00480916537344455737\n",
      "Episode Reward: 1.0\n",
      "Step 338 (70433) @ Episode 288/10000, loss: 0.00162737967912107715\n",
      "Episode Reward: 3.0\n",
      "Step 210 (70643) @ Episode 289/10000, loss: 0.00038548378506675364\n",
      "Episode Reward: 1.0\n",
      "Step 270 (70913) @ Episode 290/10000, loss: 0.00023129726469051093\n",
      "Episode Reward: 2.0\n",
      "Step 176 (71089) @ Episode 291/10000, loss: 0.00015299508231692016\n",
      "Episode Reward: 0.0\n",
      "Step 185 (71274) @ Episode 292/10000, loss: 0.00158950313925743135\n",
      "Episode Reward: 0.0\n",
      "Step 230 (71504) @ Episode 293/10000, loss: 0.00807039253413677263\n",
      "Episode Reward: 1.0\n",
      "Step 316 (71820) @ Episode 294/10000, loss: 0.00084264937322586778\n",
      "Episode Reward: 3.0\n",
      "Step 172 (71992) @ Episode 295/10000, loss: 0.00120015954598784458\n",
      "Episode Reward: 0.0\n",
      "Step 176 (72168) @ Episode 296/10000, loss: 0.00044484823592938483\n",
      "Episode Reward: 0.0\n",
      "Step 182 (72350) @ Episode 297/10000, loss: 4.956236807629466e-055\n",
      "Episode Reward: 0.0\n",
      "Step 308 (72658) @ Episode 298/10000, loss: 0.00077764841262251145\n",
      "Episode Reward: 2.0\n",
      "Step 165 (72823) @ Episode 299/10000, loss: 0.00113070209044963123\n",
      "Episode Reward: 0.0\n",
      "Step 318 (73141) @ Episode 300/10000, loss: 0.00030976248672232036\n",
      "Episode Reward: 3.0\n",
      "Step 243 (73384) @ Episode 301/10000, loss: 0.00035753537667915225\n",
      "Episode Reward: 1.0\n",
      "Step 298 (73682) @ Episode 302/10000, loss: 0.00035627861507236961\n",
      "Episode Reward: 2.0\n",
      "Step 182 (73864) @ Episode 303/10000, loss: 0.02534242533147335892\n",
      "Episode Reward: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 168 (74032) @ Episode 304/10000, loss: 0.00081986130680888895\n",
      "Episode Reward: 0.0\n",
      "Step 171 (74203) @ Episode 305/10000, loss: 0.00022945013188291346\n",
      "Episode Reward: 0.0\n",
      "Step 230 (74433) @ Episode 306/10000, loss: 0.00051745079690590553\n",
      "Episode Reward: 1.0\n",
      "Step 382 (74815) @ Episode 307/10000, loss: 0.00108312419615685944\n",
      "Episode Reward: 4.0\n",
      "Step 239 (75054) @ Episode 308/10000, loss: 0.00043996368185617035\n",
      "Episode Reward: 1.0\n",
      "Step 175 (75229) @ Episode 309/10000, loss: 0.00037669340963475406\n",
      "Episode Reward: 0.0\n",
      "Step 184 (75413) @ Episode 310/10000, loss: 0.00072112085763365034\n",
      "Episode Reward: 0.0\n",
      "Step 212 (75625) @ Episode 311/10000, loss: 0.00256662745960056898\n",
      "Episode Reward: 1.0\n",
      "Step 248 (75873) @ Episode 312/10000, loss: 0.00063244503689929846\n",
      "Episode Reward: 1.0\n",
      "Step 288 (76161) @ Episode 313/10000, loss: 0.00011359558266121894\n",
      "Episode Reward: 2.0\n",
      "Step 228 (76389) @ Episode 314/10000, loss: 0.00067317794309929013\n",
      "Episode Reward: 1.0\n",
      "Step 247 (76636) @ Episode 315/10000, loss: 0.00320335547439754922\n",
      "Episode Reward: 2.0\n",
      "Step 238 (76874) @ Episode 316/10000, loss: 0.00525378715246915835\n",
      "Episode Reward: 1.0\n",
      "Step 352 (77226) @ Episode 317/10000, loss: 0.00109947030432522312\n",
      "Episode Reward: 4.0\n",
      "Step 287 (77513) @ Episode 318/10000, loss: 0.16723075509071359108\n",
      "Episode Reward: 2.0\n",
      "Step 429 (77942) @ Episode 319/10000, loss: 0.00221148249693214958\n",
      "Episode Reward: 4.0\n",
      "Step 233 (78175) @ Episode 320/10000, loss: 0.00044533438631333417\n",
      "Episode Reward: 1.0\n",
      "Step 341 (78516) @ Episode 321/10000, loss: 0.00030664764926768845\n",
      "Episode Reward: 3.0\n",
      "Step 258 (78774) @ Episode 322/10000, loss: 5.960808994132094e-054\n",
      "Episode Reward: 1.0\n",
      "Step 367 (79141) @ Episode 323/10000, loss: 9.964192577172071e-054\n",
      "Episode Reward: 3.0\n",
      "Step 171 (79312) @ Episode 324/10000, loss: 0.00067540921736508615\n",
      "Episode Reward: 0.0\n",
      "Step 251 (79563) @ Episode 325/10000, loss: 0.00024303924874402583\n",
      "Episode Reward: 1.0\n",
      "Step 308 (79871) @ Episode 326/10000, loss: 0.00267943483777344235\n",
      "Episode Reward: 2.0\n",
      "Step 128 (79999) @ Episode 327/10000, loss: 0.00019675641669891775\n",
      "Copied model parameters to target network.\n",
      "Step 279 (80150) @ Episode 327/10000, loss: 0.00014198644203133887\n",
      "Episode Reward: 2.0\n",
      "Step 243 (80393) @ Episode 328/10000, loss: 0.00281680678017437465\n",
      "Episode Reward: 1.0\n",
      "Step 329 (80722) @ Episode 329/10000, loss: 0.00496808486059308054\n",
      "Episode Reward: 3.0\n",
      "Step 263 (80985) @ Episode 330/10000, loss: 0.00129218061920255424\n",
      "Episode Reward: 2.0\n",
      "Step 451 (81436) @ Episode 331/10000, loss: 0.00145340582821518185\n",
      "Episode Reward: 5.0\n",
      "Step 256 (81692) @ Episode 332/10000, loss: 0.00166074524167925124\n",
      "Episode Reward: 1.0\n",
      "Step 288 (81980) @ Episode 333/10000, loss: 0.00459050154313445165\n",
      "Episode Reward: 3.0\n",
      "Step 168 (82148) @ Episode 334/10000, loss: 0.00167545024305582055\n",
      "Episode Reward: 0.0\n",
      "Step 292 (82440) @ Episode 335/10000, loss: 0.00108551513403654107\n",
      "Episode Reward: 2.0\n",
      "Step 286 (82726) @ Episode 336/10000, loss: 0.00029230993823148313\n",
      "Episode Reward: 2.0\n",
      "Step 171 (82897) @ Episode 337/10000, loss: 0.00029347962117753923\n",
      "Episode Reward: 0.0\n",
      "Step 181 (83078) @ Episode 338/10000, loss: 0.00032116699730977416\n",
      "Episode Reward: 0.0\n",
      "Step 167 (83245) @ Episode 339/10000, loss: 0.00111656566150486475\n",
      "Episode Reward: 0.0\n",
      "Step 309 (83554) @ Episode 340/10000, loss: 0.00027885375311598185\n",
      "Episode Reward: 3.0\n",
      "Step 350 (83904) @ Episode 341/10000, loss: 0.00033038572291843593\n",
      "Episode Reward: 3.0\n",
      "Step 223 (84127) @ Episode 342/10000, loss: 0.00172121892683207995\n",
      "Episode Reward: 1.0\n",
      "Step 188 (84315) @ Episode 343/10000, loss: 0.00658484455198049556\n",
      "Episode Reward: 0.0\n",
      "Step 253 (84568) @ Episode 344/10000, loss: 0.00012572361447382718\n",
      "Episode Reward: 1.0\n",
      "Step 298 (84866) @ Episode 345/10000, loss: 0.00126988184638321434\n",
      "Episode Reward: 2.0\n",
      "Step 171 (85037) @ Episode 346/10000, loss: 0.00061658624326810245\n",
      "Episode Reward: 0.0\n",
      "Step 275 (85312) @ Episode 347/10000, loss: 0.00049322459381073713\n",
      "Episode Reward: 2.0\n",
      "Step 173 (85485) @ Episode 348/10000, loss: 6.280167872318998e-054\n",
      "Episode Reward: 0.0\n",
      "Step 178 (85663) @ Episode 349/10000, loss: 0.00046154981828294694\n",
      "Episode Reward: 0.0\n",
      "Step 340 (86003) @ Episode 350/10000, loss: 0.00063218799186870464\n",
      "Episode Reward: 3.0\n",
      "Step 197 (86200) @ Episode 351/10000, loss: 0.00137975090183317665\n",
      "Episode Reward: 0.0\n",
      "Step 244 (86444) @ Episode 352/10000, loss: 0.00018588552484288812\n",
      "Episode Reward: 1.0\n",
      "Step 223 (86667) @ Episode 353/10000, loss: 0.00160978233907371765\n",
      "Episode Reward: 1.0\n",
      "Step 167 (86834) @ Episode 354/10000, loss: 0.00040320228436030454\n",
      "Episode Reward: 0.0\n",
      "Step 258 (87092) @ Episode 355/10000, loss: 8.097907993942499e-053\n",
      "Episode Reward: 1.0\n",
      "Step 256 (87348) @ Episode 356/10000, loss: 5.570597204496153e-054\n",
      "Episode Reward: 2.0\n",
      "Step 411 (87759) @ Episode 357/10000, loss: 0.00045592398964799947\n",
      "Episode Reward: 5.0\n",
      "Step 167 (87926) @ Episode 358/10000, loss: 0.00022994398023001854\n",
      "Episode Reward: 0.0\n",
      "Step 177 (88103) @ Episode 359/10000, loss: 0.00046855333494022495\n",
      "Episode Reward: 0.0\n",
      "Step 229 (88332) @ Episode 360/10000, loss: 0.00051975186215713625\n",
      "Episode Reward: 1.0\n",
      "Step 380 (88712) @ Episode 361/10000, loss: 7.876374002080411e-056\n",
      "Episode Reward: 3.0\n",
      "Step 214 (88926) @ Episode 362/10000, loss: 6.022174420650117e-053\n",
      "Episode Reward: 1.0\n",
      "Step 236 (89162) @ Episode 363/10000, loss: 0.00011112315405625854\n",
      "Episode Reward: 1.0\n",
      "Step 234 (89396) @ Episode 364/10000, loss: 0.00040284375427290797\n",
      "Episode Reward: 1.0\n",
      "Step 186 (89582) @ Episode 365/10000, loss: 0.00011886692664120346\n",
      "Episode Reward: 0.0\n",
      "Step 313 (89895) @ Episode 366/10000, loss: 9.376402158522978e-056\n",
      "Episode Reward: 2.0\n",
      "Step 104 (89999) @ Episode 367/10000, loss: 0.00097348133567720656\n",
      "Copied model parameters to target network.\n",
      "Step 233 (90128) @ Episode 367/10000, loss: 0.00058069918304681784\n",
      "Episode Reward: 1.0\n",
      "Step 170 (90298) @ Episode 368/10000, loss: 0.00027469039196148515\n",
      "Episode Reward: 0.0\n",
      "Step 366 (90664) @ Episode 369/10000, loss: 0.00129058759193867444\n",
      "Episode Reward: 3.0\n",
      "Step 218 (90882) @ Episode 370/10000, loss: 0.00037583379889838474\n",
      "Episode Reward: 1.0\n",
      "Step 250 (91132) @ Episode 371/10000, loss: 0.00232294551096856625\n",
      "Episode Reward: 1.0\n",
      "Step 240 (91372) @ Episode 372/10000, loss: 0.00029987766174599533\n",
      "Episode Reward: 1.0\n",
      "Step 179 (91551) @ Episode 373/10000, loss: 8.417964272666723e-056\n",
      "Episode Reward: 0.0\n",
      "Step 271 (91822) @ Episode 374/10000, loss: 0.00078749761451035744\n",
      "Episode Reward: 2.0\n",
      "Step 176 (91998) @ Episode 375/10000, loss: 0.00017593163647688925\n",
      "Episode Reward: 0.0\n",
      "Step 380 (92378) @ Episode 376/10000, loss: 0.00157075165770947932\n",
      "Episode Reward: 4.0\n",
      "Step 240 (92618) @ Episode 377/10000, loss: 0.00090170511975884444\n",
      "Episode Reward: 1.0\n",
      "Step 225 (92843) @ Episode 378/10000, loss: 0.00069444987457245593\n",
      "Episode Reward: 1.0\n",
      "Step 474 (93317) @ Episode 379/10000, loss: 0.00149147969204932457\n",
      "Episode Reward: 5.0\n",
      "Step 195 (93512) @ Episode 380/10000, loss: 0.00077154091559350492\n",
      "Episode Reward: 0.0\n",
      "Step 175 (93687) @ Episode 381/10000, loss: 0.00016261418932117522\n",
      "Episode Reward: 0.0\n",
      "Step 226 (93913) @ Episode 382/10000, loss: 0.00070818344829604035\n",
      "Episode Reward: 1.0\n",
      "Step 174 (94087) @ Episode 383/10000, loss: 0.00045740432688035079\n",
      "Episode Reward: 0.0\n",
      "Step 250 (94337) @ Episode 384/10000, loss: 0.00062162440735846766\n",
      "Episode Reward: 1.0\n",
      "Step 236 (94573) @ Episode 385/10000, loss: 0.00064186111558228735\n",
      "Episode Reward: 1.0\n",
      "Step 315 (94888) @ Episode 386/10000, loss: 0.00061749556334689264\n",
      "Episode Reward: 3.0\n",
      "Step 229 (95117) @ Episode 387/10000, loss: 0.00084988708840683125\n",
      "Episode Reward: 1.0\n",
      "Step 234 (95351) @ Episode 388/10000, loss: 0.00039458638639189303\n",
      "Episode Reward: 1.0\n",
      "Step 190 (95541) @ Episode 389/10000, loss: 0.00012213742593303323\n",
      "Episode Reward: 0.0\n",
      "Step 171 (95712) @ Episode 390/10000, loss: 0.00068877957528457055\n",
      "Episode Reward: 0.0\n",
      "Step 330 (96042) @ Episode 391/10000, loss: 0.00043965652002952993\n",
      "Episode Reward: 3.0\n",
      "Step 165 (96207) @ Episode 392/10000, loss: 9.152269922196865e-053\n",
      "Episode Reward: 0.0\n",
      "Step 421 (96628) @ Episode 393/10000, loss: 0.00146153883542865512\n",
      "Episode Reward: 4.0\n",
      "Step 254 (96882) @ Episode 394/10000, loss: 0.00080663739936426286\n",
      "Episode Reward: 2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 196 (97078) @ Episode 395/10000, loss: 7.337999704759568e-057\n",
      "Episode Reward: 0.0\n",
      "Step 235 (97313) @ Episode 396/10000, loss: 0.00122930645011365415\n",
      "Episode Reward: 1.0\n",
      "Step 275 (97588) @ Episode 397/10000, loss: 0.00039706128882244232\n",
      "Episode Reward: 2.0\n",
      "Step 413 (98001) @ Episode 398/10000, loss: 0.00073903630254790195\n",
      "Episode Reward: 4.0\n",
      "Step 176 (98177) @ Episode 399/10000, loss: 4.924974200548604e-054\n",
      "Episode Reward: 0.0\n",
      "Step 252 (98429) @ Episode 400/10000, loss: 0.00140343536622822284\n",
      "Episode Reward: 1.0\n",
      "Step 236 (98665) @ Episode 401/10000, loss: 0.00197549955919384964\n",
      "Episode Reward: 1.0\n",
      "Step 187 (98852) @ Episode 402/10000, loss: 0.00098910299129784145\n",
      "Episode Reward: 0.0\n",
      "Step 188 (99040) @ Episode 403/10000, loss: 0.00063138420227915055\n",
      "Episode Reward: 0.0\n",
      "Step 258 (99298) @ Episode 404/10000, loss: 7.130058656912297e-056\n",
      "Episode Reward: 1.0\n",
      "Step 172 (99470) @ Episode 405/10000, loss: 0.00048397382488474253\n",
      "Episode Reward: 0.0\n",
      "Step 298 (99768) @ Episode 406/10000, loss: 0.00044431965216062963\n",
      "Episode Reward: 2.0\n",
      "Step 231 (99999) @ Episode 407/10000, loss: 0.00122282211668789394\n",
      "Copied model parameters to target network.\n",
      "Step 252 (100020) @ Episode 407/10000, loss: 0.00102240056730806833\n",
      "Episode Reward: 1.0\n",
      "Step 302 (100322) @ Episode 408/10000, loss: 0.00114212185144424446\n",
      "Episode Reward: 3.0\n",
      "Step 296 (100618) @ Episode 409/10000, loss: 0.00191281200386583865\n",
      "Episode Reward: 2.0\n",
      "Step 271 (100889) @ Episode 410/10000, loss: 0.00053135247435420757\n",
      "Episode Reward: 2.0\n",
      "Step 228 (101117) @ Episode 411/10000, loss: 0.00278845173306763176\n",
      "Episode Reward: 1.0\n",
      "Step 231 (101348) @ Episode 412/10000, loss: 0.00212591025047004229\n",
      "Episode Reward: 1.0\n",
      "Step 166 (101514) @ Episode 413/10000, loss: 0.00172619149088859563\n",
      "Episode Reward: 0.0\n",
      "Step 178 (101692) @ Episode 414/10000, loss: 0.00050925376126542692\n",
      "Episode Reward: 0.0\n",
      "Step 351 (102043) @ Episode 415/10000, loss: 0.00067765096900984645\n",
      "Episode Reward: 4.0\n",
      "Step 172 (102215) @ Episode 416/10000, loss: 0.00125062710139900455\n",
      "Episode Reward: 0.0\n",
      "Step 228 (102443) @ Episode 417/10000, loss: 0.00134901318233460198\n",
      "Episode Reward: 1.0\n",
      "Step 180 (102623) @ Episode 418/10000, loss: 0.00057794555323198446\n",
      "Episode Reward: 0.0\n",
      "Step 236 (102859) @ Episode 419/10000, loss: 0.00027853716164827347\n",
      "Episode Reward: 1.0\n",
      "Step 367 (103226) @ Episode 420/10000, loss: 0.00099255982786417538\n",
      "Episode Reward: 4.0\n",
      "Step 310 (103536) @ Episode 421/10000, loss: 0.00058329070452600727\n",
      "Episode Reward: 2.0\n",
      "Step 268 (103804) @ Episode 422/10000, loss: 0.00060679402668029074\n",
      "Episode Reward: 2.0\n",
      "Step 429 (104233) @ Episode 423/10000, loss: 0.00016229359607677907\n",
      "Episode Reward: 4.0\n",
      "Step 163 (104396) @ Episode 424/10000, loss: 0.00029819222982041546\n",
      "Episode Reward: 0.0\n",
      "Step 175 (104571) @ Episode 425/10000, loss: 0.00063973467331379654\n",
      "Episode Reward: 0.0\n",
      "Step 241 (104812) @ Episode 426/10000, loss: 0.00043258568621240556\n",
      "Episode Reward: 1.0\n",
      "Step 240 (105052) @ Episode 427/10000, loss: 0.00073326844722032557\n",
      "Episode Reward: 1.0\n",
      "Step 307 (105359) @ Episode 428/10000, loss: 0.00109133427031338215\n",
      "Episode Reward: 2.0\n",
      "Step 182 (105541) @ Episode 429/10000, loss: 0.00014384466339834034\n",
      "Episode Reward: 0.0\n",
      "Step 198 (105739) @ Episode 430/10000, loss: 0.00599469058215618174\n",
      "Episode Reward: 0.0\n",
      "Step 171 (105910) @ Episode 431/10000, loss: 5.0679816922638565e-05\n",
      "Episode Reward: 0.0\n",
      "Step 175 (106085) @ Episode 432/10000, loss: 0.00099288276396691852\n",
      "Episode Reward: 0.0\n",
      "Step 215 (106300) @ Episode 433/10000, loss: 0.00081957492511719472\n",
      "Episode Reward: 1.0\n",
      "Step 410 (106710) @ Episode 434/10000, loss: 0.00037371134385466576\n",
      "Episode Reward: 4.0\n",
      "Step 172 (106882) @ Episode 435/10000, loss: 0.00105437892489135273\n",
      "Episode Reward: 0.0\n",
      "Step 231 (107113) @ Episode 436/10000, loss: 0.00073869375046342616\n",
      "Episode Reward: 1.0\n",
      "Step 222 (107335) @ Episode 437/10000, loss: 0.00021459147683344781\n",
      "Episode Reward: 1.0\n",
      "Step 165 (107500) @ Episode 438/10000, loss: 0.00037734350189566614\n",
      "Episode Reward: 0.0\n",
      "Step 281 (107781) @ Episode 439/10000, loss: 0.00051265966612845662\n",
      "Episode Reward: 2.0\n",
      "Step 273 (108054) @ Episode 440/10000, loss: 0.00067397486418485643\n",
      "Episode Reward: 2.0\n",
      "Step 234 (108288) @ Episode 441/10000, loss: 8.178464486263692e-053\n",
      "Episode Reward: 1.0\n",
      "Step 228 (108516) @ Episode 442/10000, loss: 0.00021343868866097182\n",
      "Episode Reward: 1.0\n",
      "Step 419 (108935) @ Episode 443/10000, loss: 0.00049066415522247553\n",
      "Episode Reward: 4.0\n",
      "Step 292 (109227) @ Episode 444/10000, loss: 0.00128540932200849064\n",
      "Episode Reward: 2.0\n",
      "Step 214 (109441) @ Episode 445/10000, loss: 0.00105053768493235112\n",
      "Episode Reward: 1.0\n",
      "Step 312 (109753) @ Episode 446/10000, loss: 0.00025757431285455823\n",
      "Episode Reward: 3.0\n",
      "Step 246 (109999) @ Episode 447/10000, loss: 0.00131669384427368645\n",
      "Copied model parameters to target network.\n",
      "Step 388 (110141) @ Episode 447/10000, loss: 0.00051207374781370169\n",
      "Episode Reward: 4.0\n",
      "Step 174 (110315) @ Episode 448/10000, loss: 0.00022099907801020899\n",
      "Episode Reward: 0.0\n",
      "Step 401 (110716) @ Episode 449/10000, loss: 0.00013322118320502347\n",
      "Episode Reward: 4.0\n",
      "Step 300 (111016) @ Episode 450/10000, loss: 0.00059792783576995134\n",
      "Episode Reward: 2.0\n",
      "Step 224 (111240) @ Episode 451/10000, loss: 0.00031107402173802257\n",
      "Episode Reward: 1.0\n",
      "Step 181 (111421) @ Episode 452/10000, loss: 0.00071165990084409714\n",
      "Episode Reward: 0.0\n",
      "Step 274 (111695) @ Episode 453/10000, loss: 0.00103665259666740976\n",
      "Episode Reward: 2.0\n",
      "Step 230 (111925) @ Episode 454/10000, loss: 0.00051179633010178856\n",
      "Episode Reward: 1.0\n",
      "Step 170 (112095) @ Episode 455/10000, loss: 0.00023078176309354603\n",
      "Episode Reward: 0.0\n",
      "Step 358 (112453) @ Episode 456/10000, loss: 0.00032752391416579485\n",
      "Episode Reward: 3.0\n",
      "Step 337 (112790) @ Episode 457/10000, loss: 0.00155467144213616855\n",
      "Episode Reward: 3.0\n",
      "Step 309 (113099) @ Episode 458/10000, loss: 0.00043850493966601795\n",
      "Episode Reward: 3.0\n",
      "Step 283 (113382) @ Episode 459/10000, loss: 0.00137216690927743916\n",
      "Episode Reward: 2.0\n",
      "Step 171 (113553) @ Episode 460/10000, loss: 0.00147619214840233335\n",
      "Episode Reward: 0.0\n",
      "Step 234 (113787) @ Episode 461/10000, loss: 0.00144415278919041162\n",
      "Episode Reward: 1.0\n",
      "Step 207 (113994) @ Episode 462/10000, loss: 0.00158398190978914555\n",
      "Episode Reward: 1.0\n",
      "Step 184 (114178) @ Episode 463/10000, loss: 0.00147907622158527374\n",
      "Episode Reward: 0.0\n",
      "Step 181 (114359) @ Episode 464/10000, loss: 0.00026426214026287253\n",
      "Episode Reward: 0.0\n",
      "Step 302 (114661) @ Episode 465/10000, loss: 0.00148965488187968736\n",
      "Episode Reward: 2.0\n",
      "Step 174 (114835) @ Episode 466/10000, loss: 0.00185531354509294035\n",
      "Episode Reward: 0.0\n",
      "Step 271 (115106) @ Episode 467/10000, loss: 0.00084712088573724031\n",
      "Episode Reward: 2.0\n",
      "Step 242 (115348) @ Episode 468/10000, loss: 0.00115508574526757553\n",
      "Episode Reward: 1.0\n",
      "Step 169 (115517) @ Episode 469/10000, loss: 0.01885748282074928376\n",
      "Episode Reward: 0.0\n",
      "Step 227 (115744) @ Episode 470/10000, loss: 0.00051836739294230944\n",
      "Episode Reward: 1.0\n",
      "Step 204 (115948) @ Episode 471/10000, loss: 0.00202512950636446483\n",
      "Episode Reward: 1.0\n",
      "Step 216 (116164) @ Episode 472/10000, loss: 0.00049057812429964546\n",
      "Episode Reward: 1.0\n",
      "Step 168 (116332) @ Episode 473/10000, loss: 0.00020751846022903922\n",
      "Episode Reward: 0.0\n",
      "Step 178 (116510) @ Episode 474/10000, loss: 0.00153220386710017925\n",
      "Episode Reward: 0.0\n",
      "Step 161 (116671) @ Episode 475/10000, loss: 0.00010261384159093723\n",
      "Episode Reward: 0.0\n",
      "Step 175 (116846) @ Episode 476/10000, loss: 0.00033965107286348942\n",
      "Episode Reward: 0.0\n",
      "Step 209 (117055) @ Episode 477/10000, loss: 0.00076119846198707827\n",
      "Episode Reward: 1.0\n",
      "Step 210 (117265) @ Episode 478/10000, loss: 0.00065635319333523513\n",
      "Episode Reward: 1.0\n",
      "Step 264 (117529) @ Episode 479/10000, loss: 0.00149269099347293387\n",
      "Episode Reward: 2.0\n",
      "Step 178 (117707) @ Episode 480/10000, loss: 0.00057792803272604945\n",
      "Episode Reward: 0.0\n",
      "Step 293 (118000) @ Episode 481/10000, loss: 0.00142613914795219955\n",
      "Episode Reward: 2.0\n",
      "Step 481 (118481) @ Episode 482/10000, loss: 9.7935764642898e-05474\n",
      "Episode Reward: 8.0\n",
      "Step 167 (118648) @ Episode 483/10000, loss: 0.00140567810740321875\n",
      "Episode Reward: 0.0\n",
      "Step 268 (118916) @ Episode 484/10000, loss: 0.00081130862236022954\n",
      "Episode Reward: 2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 232 (119148) @ Episode 485/10000, loss: 0.00037836114643141636\n",
      "Episode Reward: 1.0\n",
      "Step 222 (119370) @ Episode 486/10000, loss: 0.00081664684694260366\n",
      "Episode Reward: 1.0\n",
      "Step 174 (119544) @ Episode 487/10000, loss: 0.00044027762487530713\n",
      "Episode Reward: 0.0\n",
      "Step 207 (119751) @ Episode 488/10000, loss: 0.00020118869724683464\n",
      "Episode Reward: 1.0\n",
      "Step 248 (119999) @ Episode 489/10000, loss: 9.326009603682905e-056\n",
      "Copied model parameters to target network.\n",
      "Step 275 (120026) @ Episode 489/10000, loss: 0.00047079319483600557\n",
      "Episode Reward: 2.0\n",
      "Step 204 (120230) @ Episode 490/10000, loss: 0.00126390089280903347\n",
      "Episode Reward: 1.0\n",
      "Step 254 (120484) @ Episode 491/10000, loss: 0.01131012942641973534\n",
      "Episode Reward: 2.0\n",
      "Step 316 (120800) @ Episode 492/10000, loss: 0.00090377096785232426\n",
      "Episode Reward: 2.0\n",
      "Step 270 (121070) @ Episode 493/10000, loss: 0.00131489988416433333\n",
      "Episode Reward: 2.0\n",
      "Step 398 (121468) @ Episode 494/10000, loss: 0.00168211443815380335\n",
      "Episode Reward: 4.0\n",
      "Step 180 (121648) @ Episode 495/10000, loss: 0.00040100756450556226\n",
      "Episode Reward: 0.0\n",
      "Step 297 (121945) @ Episode 496/10000, loss: 0.00207029236480593704\n",
      "Episode Reward: 2.0\n",
      "Step 258 (122203) @ Episode 497/10000, loss: 0.00142712518572807314\n",
      "Episode Reward: 2.0\n",
      "Step 248 (122451) @ Episode 498/10000, loss: 0.00026809642440639436\n",
      "Episode Reward: 1.0\n",
      "Step 251 (122702) @ Episode 499/10000, loss: 0.00221240334212788168\n",
      "Episode Reward: 1.0\n",
      "Step 246 (122948) @ Episode 500/10000, loss: 0.00053128239233046775\n",
      "Episode Reward: 1.0\n",
      "Step 186 (123134) @ Episode 501/10000, loss: 0.00157205981668084862\n",
      "Episode Reward: 0.0\n",
      "Step 296 (123430) @ Episode 502/10000, loss: 0.00061227643163874755\n",
      "Episode Reward: 2.0\n",
      "Step 251 (123681) @ Episode 503/10000, loss: 0.00057430966990068556\n",
      "Episode Reward: 2.0\n",
      "Step 365 (124046) @ Episode 504/10000, loss: 0.00024313526228070267\n",
      "Episode Reward: 3.0\n",
      "Step 248 (124294) @ Episode 505/10000, loss: 0.00039466246380470693\n",
      "Episode Reward: 2.0\n",
      "Step 174 (124468) @ Episode 506/10000, loss: 8.147663902491331e-053\n",
      "Episode Reward: 0.0\n",
      "Step 208 (124676) @ Episode 507/10000, loss: 0.00020073878113180487\n",
      "Episode Reward: 1.0\n",
      "Step 214 (124890) @ Episode 508/10000, loss: 0.00029636459657922387\n",
      "Episode Reward: 1.0\n",
      "Step 184 (125074) @ Episode 509/10000, loss: 0.00023535657965112478\n",
      "Episode Reward: 0.0\n",
      "Step 227 (125301) @ Episode 510/10000, loss: 0.00051664962666109265\n",
      "Episode Reward: 1.0\n",
      "Step 288 (125589) @ Episode 511/10000, loss: 0.00098901824094355141\n",
      "Episode Reward: 2.0\n",
      "Step 239 (125828) @ Episode 512/10000, loss: 0.00144625420216470967\n",
      "Episode Reward: 1.0\n",
      "Step 220 (126048) @ Episode 513/10000, loss: 0.00086992688011378053\n",
      "Episode Reward: 1.0\n",
      "Step 298 (126346) @ Episode 514/10000, loss: 0.00098767189774662264\n",
      "Episode Reward: 2.0\n",
      "Step 187 (126533) @ Episode 515/10000, loss: 0.00032891519367694855\n",
      "Episode Reward: 0.0\n",
      "Step 227 (126760) @ Episode 516/10000, loss: 0.00099154817871749463\n",
      "Episode Reward: 1.0\n",
      "Step 227 (126987) @ Episode 517/10000, loss: 0.00068105664104223255\n",
      "Episode Reward: 1.0\n",
      "Step 163 (127150) @ Episode 518/10000, loss: 0.00177872052881866784\n",
      "Episode Reward: 0.0\n",
      "Step 429 (127579) @ Episode 519/10000, loss: 0.00078413507435470823\n",
      "Episode Reward: 4.0\n",
      "Step 182 (127761) @ Episode 520/10000, loss: 0.00036782980896532536\n",
      "Episode Reward: 0.0\n",
      "Step 272 (128033) @ Episode 521/10000, loss: 0.00078883097739890224\n",
      "Episode Reward: 2.0\n",
      "Step 254 (128287) @ Episode 522/10000, loss: 0.00039290502900257707\n",
      "Episode Reward: 1.0\n",
      "Step 166 (128453) @ Episode 523/10000, loss: 0.00058352225460112105\n",
      "Episode Reward: 0.0\n",
      "Step 297 (128750) @ Episode 524/10000, loss: 0.00070288858842104672\n",
      "Episode Reward: 3.0\n",
      "Step 178 (128928) @ Episode 525/10000, loss: 0.00060144084272906183\n",
      "Episode Reward: 0.0\n",
      "Step 168 (129096) @ Episode 526/10000, loss: 0.00132069364190101624\n",
      "Episode Reward: 0.0\n",
      "Step 177 (129273) @ Episode 527/10000, loss: 0.00048748246626928454\n",
      "Episode Reward: 0.0\n",
      "Step 168 (129441) @ Episode 528/10000, loss: 0.00243445415981113955\n",
      "Episode Reward: 0.0\n",
      "Step 196 (129637) @ Episode 529/10000, loss: 0.00239710928872227676\n",
      "Episode Reward: 0.0\n",
      "Step 362 (129999) @ Episode 530/10000, loss: 0.00060682091861963278\n",
      "Copied model parameters to target network.\n",
      "Step 375 (130012) @ Episode 530/10000, loss: 7.900383207015693e-056\n",
      "Episode Reward: 4.0\n",
      "Step 209 (130221) @ Episode 531/10000, loss: 0.00023262412287294865\n",
      "Episode Reward: 1.0\n",
      "Step 171 (130392) @ Episode 532/10000, loss: 0.00015078530122991656\n",
      "Episode Reward: 0.0\n",
      "Step 307 (130699) @ Episode 533/10000, loss: 0.00043513596756383777\n",
      "Episode Reward: 3.0\n",
      "Step 344 (131043) @ Episode 534/10000, loss: 0.00172380416188389065\n",
      "Episode Reward: 4.0\n",
      "Step 515 (131558) @ Episode 535/10000, loss: 0.00033358050859533257\n",
      "Episode Reward: 6.0\n",
      "Step 265 (131823) @ Episode 536/10000, loss: 0.00049654010217636823\n",
      "Episode Reward: 2.0\n",
      "Step 242 (132065) @ Episode 537/10000, loss: 0.00116323272231966265\n",
      "Episode Reward: 1.0\n",
      "Step 318 (132383) @ Episode 538/10000, loss: 0.00019073039584327493\n",
      "Episode Reward: 2.0\n",
      "Step 188 (132571) @ Episode 539/10000, loss: 0.00055160594638437033\n",
      "Episode Reward: 0.0\n",
      "Step 233 (132804) @ Episode 540/10000, loss: 0.00042094226228073244\n",
      "Episode Reward: 1.0\n",
      "Step 167 (132971) @ Episode 541/10000, loss: 0.00047710360377095647\n",
      "Episode Reward: 0.0\n",
      "Step 308 (133279) @ Episode 542/10000, loss: 0.00478798802942037655\n",
      "Episode Reward: 2.0\n",
      "Step 268 (133547) @ Episode 543/10000, loss: 0.00060450780438259248\n",
      "Episode Reward: 2.0\n",
      "Step 274 (133821) @ Episode 544/10000, loss: 0.00105335284024477158\n",
      "Episode Reward: 2.0\n",
      "Step 278 (134099) @ Episode 545/10000, loss: 0.00286075589247047963\n",
      "Episode Reward: 2.0\n",
      "Step 188 (134287) @ Episode 546/10000, loss: 0.00057295744773000485\n",
      "Episode Reward: 0.0\n",
      "Step 183 (134470) @ Episode 547/10000, loss: 0.00052193563897162686\n",
      "Episode Reward: 0.0\n",
      "Step 299 (134769) @ Episode 548/10000, loss: 0.00024806437431834644\n",
      "Episode Reward: 2.0\n",
      "Step 191 (134960) @ Episode 549/10000, loss: 0.00075665488839149484\n",
      "Episode Reward: 0.0\n",
      "Step 409 (135369) @ Episode 550/10000, loss: 0.00038188506732694805\n",
      "Episode Reward: 4.0\n",
      "Step 475 (135844) @ Episode 551/10000, loss: 8.705264917807654e-054\n",
      "Episode Reward: 5.0\n",
      "Step 281 (136125) @ Episode 552/10000, loss: 0.00228239456191658973\n",
      "Episode Reward: 2.0\n",
      "Step 159 (136284) @ Episode 553/10000, loss: 0.00076503254240378743\n",
      "Episode Reward: 0.0\n",
      "Step 237 (136521) @ Episode 554/10000, loss: 0.00071044953074306254\n",
      "Episode Reward: 1.0\n",
      "Step 218 (136739) @ Episode 555/10000, loss: 0.00040704119601286954\n",
      "Episode Reward: 1.0\n",
      "Step 234 (136973) @ Episode 556/10000, loss: 0.00033204903593286875\n",
      "Episode Reward: 1.0\n",
      "Step 390 (137363) @ Episode 557/10000, loss: 0.00118244090117514133\n",
      "Episode Reward: 4.0\n",
      "Step 167 (137530) @ Episode 558/10000, loss: 7.524935790570453e-052\n",
      "Episode Reward: 0.0\n",
      "Step 190 (137720) @ Episode 559/10000, loss: 0.00145686941687017684\n",
      "Episode Reward: 0.0\n",
      "Step 166 (137886) @ Episode 560/10000, loss: 0.00029753433773294095\n",
      "Episode Reward: 0.0\n",
      "Step 316 (138202) @ Episode 561/10000, loss: 0.00302622141316533187\n",
      "Episode Reward: 3.0\n",
      "Step 183 (138385) @ Episode 562/10000, loss: 0.00061545486096292736\n",
      "Episode Reward: 0.0\n",
      "Step 168 (138553) @ Episode 563/10000, loss: 0.00024965900229290133\n",
      "Episode Reward: 0.0\n",
      "Step 273 (138826) @ Episode 564/10000, loss: 0.00089572276920080184\n",
      "Episode Reward: 2.0\n",
      "Step 287 (139113) @ Episode 565/10000, loss: 0.00011227951472392306\n",
      "Episode Reward: 2.0\n",
      "Step 236 (139349) @ Episode 566/10000, loss: 0.00040929188253358006\n",
      "Episode Reward: 1.0\n",
      "Step 169 (139518) @ Episode 567/10000, loss: 0.00014808947162237018\n",
      "Episode Reward: 0.0\n",
      "Step 249 (139767) @ Episode 568/10000, loss: 0.00018294190522283316\n",
      "Episode Reward: 1.0\n",
      "Step 232 (139999) @ Episode 569/10000, loss: 0.00081540038809180267\n",
      "Copied model parameters to target network.\n",
      "Step 250 (140017) @ Episode 569/10000, loss: 0.00017555283557157964\n",
      "Episode Reward: 1.0\n",
      "Step 227 (140244) @ Episode 570/10000, loss: 0.00016923704242799434\n",
      "Episode Reward: 1.0\n",
      "Step 174 (140418) @ Episode 571/10000, loss: 0.00099869549740105877\n",
      "Episode Reward: 0.0\n",
      "Step 172 (140590) @ Episode 572/10000, loss: 0.06183435767889023095\n",
      "Episode Reward: 0.0\n",
      "Step 288 (140878) @ Episode 573/10000, loss: 0.00051073357462883065\n",
      "Episode Reward: 2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 170 (141048) @ Episode 574/10000, loss: 0.00063303543720394374\n",
      "Episode Reward: 0.0\n",
      "Step 353 (141401) @ Episode 575/10000, loss: 0.00048232660628855237\n",
      "Episode Reward: 4.0\n",
      "Step 178 (141579) @ Episode 576/10000, loss: 0.00022671534679830074\n",
      "Episode Reward: 0.0\n",
      "Step 360 (141939) @ Episode 577/10000, loss: 0.00030851966585032641\n",
      "Episode Reward: 3.0\n",
      "Step 344 (142283) @ Episode 578/10000, loss: 0.00121668574865907435\n",
      "Episode Reward: 3.0\n",
      "Step 237 (142520) @ Episode 579/10000, loss: 0.00051368970889598136\n",
      "Episode Reward: 1.0\n",
      "Step 235 (142755) @ Episode 580/10000, loss: 0.00068724004086107025\n",
      "Episode Reward: 1.0\n",
      "Step 297 (143052) @ Episode 581/10000, loss: 0.00114062509965151553\n",
      "Episode Reward: 2.0\n",
      "Step 224 (143276) @ Episode 582/10000, loss: 0.00104595790617167954\n",
      "Episode Reward: 1.0\n",
      "Step 180 (143456) @ Episode 583/10000, loss: 0.00038363115163519985\n",
      "Episode Reward: 0.0\n",
      "Step 254 (143710) @ Episode 584/10000, loss: 0.00018464091408532113\n",
      "Episode Reward: 2.0\n",
      "Step 174 (143884) @ Episode 585/10000, loss: 0.00107240118086338042\n",
      "Episode Reward: 0.0\n",
      "Step 193 (144077) @ Episode 586/10000, loss: 0.00052014482207596343\n",
      "Episode Reward: 0.0\n",
      "Step 175 (144252) @ Episode 587/10000, loss: 0.00041903677629306913\n",
      "Episode Reward: 0.0\n",
      "Step 249 (144501) @ Episode 588/10000, loss: 0.00038023290107958025\n",
      "Episode Reward: 1.0\n",
      "Step 218 (144719) @ Episode 589/10000, loss: 0.00081153737846761946\n",
      "Episode Reward: 1.0\n",
      "Step 179 (144898) @ Episode 590/10000, loss: 0.00060059560928493743\n",
      "Episode Reward: 0.0\n",
      "Step 172 (145070) @ Episode 591/10000, loss: 0.00031339266570284965\n",
      "Episode Reward: 0.0\n",
      "Step 207 (145277) @ Episode 592/10000, loss: 0.00066800613421946765\n",
      "Episode Reward: 1.0\n",
      "Step 379 (145656) @ Episode 593/10000, loss: 0.01226579211652278965\n",
      "Episode Reward: 4.0\n",
      "Step 179 (145835) @ Episode 594/10000, loss: 0.00115727260708808988\n",
      "Episode Reward: 0.0\n",
      "Step 375 (146210) @ Episode 595/10000, loss: 0.00063946523005142817\n",
      "Episode Reward: 4.0\n",
      "Step 180 (146390) @ Episode 596/10000, loss: 0.00014002935495227575\n",
      "Episode Reward: 0.0\n",
      "Step 278 (146668) @ Episode 597/10000, loss: 0.00105314957909286027\n",
      "Episode Reward: 2.0\n",
      "Step 183 (146851) @ Episode 598/10000, loss: 0.00418109679594636107\n",
      "Episode Reward: 0.0\n",
      "Step 167 (147018) @ Episode 599/10000, loss: 0.00076069001806899984\n",
      "Episode Reward: 0.0\n",
      "Step 175 (147193) @ Episode 600/10000, loss: 0.00034277653321623897\n",
      "Episode Reward: 0.0\n",
      "Step 309 (147502) @ Episode 601/10000, loss: 9.939334995578974e-058\n",
      "Episode Reward: 2.0\n",
      "Step 290 (147792) @ Episode 602/10000, loss: 0.00238958676345646416\n",
      "Episode Reward: 2.0\n",
      "Step 195 (147987) @ Episode 603/10000, loss: 0.00170408783014863737\n",
      "Episode Reward: 0.0\n",
      "Step 319 (148306) @ Episode 604/10000, loss: 0.00016807073552627116\n",
      "Episode Reward: 2.0\n",
      "Step 203 (148509) @ Episode 605/10000, loss: 0.00056308420607820154\n",
      "Episode Reward: 0.0\n",
      "Step 182 (148691) @ Episode 606/10000, loss: 0.00037679963861592114\n",
      "Episode Reward: 0.0\n",
      "Step 273 (148964) @ Episode 607/10000, loss: 0.00022534499294124544\n",
      "Episode Reward: 2.0\n",
      "Step 181 (149145) @ Episode 608/10000, loss: 0.00151039008051157786\n",
      "Episode Reward: 0.0\n",
      "Step 186 (149331) @ Episode 609/10000, loss: 0.00086594204185530543\n",
      "Episode Reward: 0.0\n",
      "Step 238 (149569) @ Episode 610/10000, loss: 0.00053061748621985327\n",
      "Episode Reward: 1.0\n",
      "Step 300 (149869) @ Episode 611/10000, loss: 0.00014018474030308425\n",
      "Episode Reward: 2.0\n",
      "Step 130 (149999) @ Episode 612/10000, loss: 0.00061825284501537687\n",
      "Copied model parameters to target network.\n",
      "Step 187 (150056) @ Episode 612/10000, loss: 0.00043182988883927464\n",
      "Episode Reward: 0.0\n",
      "Step 307 (150363) @ Episode 613/10000, loss: 0.00029059877851977944\n",
      "Episode Reward: 2.0\n",
      "Step 181 (150544) @ Episode 614/10000, loss: 0.00039603831828571856\n",
      "Episode Reward: 0.0\n",
      "Step 170 (150714) @ Episode 615/10000, loss: 0.00011635804548859596\n",
      "Episode Reward: 0.0\n",
      "Step 194 (150908) @ Episode 616/10000, loss: 0.00034859153674915433\n",
      "Episode Reward: 0.0\n",
      "Step 302 (151210) @ Episode 617/10000, loss: 0.00033809727756306533\n",
      "Episode Reward: 2.0\n",
      "Step 244 (151454) @ Episode 618/10000, loss: 0.00085601414320990443\n",
      "Episode Reward: 2.0\n",
      "Step 264 (151718) @ Episode 619/10000, loss: 0.00083055498544126753\n",
      "Episode Reward: 2.0\n",
      "Step 376 (152094) @ Episode 620/10000, loss: 0.00030475738458335416\n",
      "Episode Reward: 3.0\n",
      "Step 206 (152300) @ Episode 621/10000, loss: 0.00142922229133546353\n",
      "Episode Reward: 1.0\n",
      "Step 231 (152531) @ Episode 622/10000, loss: 0.00017135626694653183\n",
      "Episode Reward: 1.0\n",
      "Step 301 (152832) @ Episode 623/10000, loss: 0.00060796469915658247\n",
      "Episode Reward: 2.0\n",
      "Step 266 (153098) @ Episode 624/10000, loss: 0.00018738277140073478\n",
      "Episode Reward: 2.0\n",
      "Step 169 (153267) @ Episode 625/10000, loss: 0.00025973466108553117\n",
      "Episode Reward: 0.0\n",
      "Step 283 (153550) @ Episode 626/10000, loss: 0.00054484000429511076\n",
      "Episode Reward: 2.0\n",
      "Step 230 (153780) @ Episode 627/10000, loss: 0.00055290851742029196\n",
      "Episode Reward: 1.0\n",
      "Step 168 (153948) @ Episode 628/10000, loss: 0.00027883154689334333\n",
      "Episode Reward: 0.0\n",
      "Step 312 (154260) @ Episode 629/10000, loss: 0.00196912907995283672\n",
      "Episode Reward: 3.0\n",
      "Step 328 (154588) @ Episode 630/10000, loss: 0.00057826458942145114\n",
      "Episode Reward: 3.0\n",
      "Step 335 (154923) @ Episode 631/10000, loss: 0.00068766297772526745\n",
      "Episode Reward: 3.0\n",
      "Step 203 (155126) @ Episode 632/10000, loss: 0.00112373195588588715\n",
      "Episode Reward: 1.0\n",
      "Step 283 (155409) @ Episode 633/10000, loss: 0.00069473340408876544\n",
      "Episode Reward: 2.0\n",
      "Step 233 (155642) @ Episode 634/10000, loss: 0.00042816670611500742\n",
      "Episode Reward: 1.0\n",
      "Step 244 (155886) @ Episode 635/10000, loss: 6.83411126374267e-0535\n",
      "Episode Reward: 1.0\n",
      "Step 237 (156123) @ Episode 636/10000, loss: 0.00029786396771669395\n",
      "Episode Reward: 2.0\n",
      "Step 186 (156309) @ Episode 637/10000, loss: 0.00019740444258786738\n",
      "Episode Reward: 0.0\n",
      "Step 322 (156631) @ Episode 638/10000, loss: 0.00248445034958422235\n",
      "Episode Reward: 3.0\n",
      "Step 173 (156804) @ Episode 639/10000, loss: 0.00020943833806086332\n",
      "Episode Reward: 0.0\n",
      "Step 240 (157044) @ Episode 640/10000, loss: 0.00650243740528821954\n",
      "Episode Reward: 1.0\n",
      "Step 226 (157270) @ Episode 641/10000, loss: 0.00141120562329888343\n",
      "Episode Reward: 1.0\n",
      "Step 362 (157632) @ Episode 642/10000, loss: 0.00110336742363870143\n",
      "Episode Reward: 3.0\n",
      "Step 185 (157817) @ Episode 643/10000, loss: 0.00058050069492310293\n",
      "Episode Reward: 0.0\n",
      "Step 244 (158061) @ Episode 644/10000, loss: 0.00028519949410110713\n",
      "Episode Reward: 1.0\n",
      "Step 209 (158270) @ Episode 645/10000, loss: 0.00288102542981505428\n",
      "Episode Reward: 1.0\n",
      "Step 219 (158489) @ Episode 646/10000, loss: 0.00056255923118442351\n",
      "Episode Reward: 1.0\n",
      "Step 247 (158736) @ Episode 647/10000, loss: 0.00038393493741750717\n",
      "Episode Reward: 1.0\n",
      "Step 181 (158917) @ Episode 648/10000, loss: 0.00078055489575490362\n",
      "Episode Reward: 0.0\n",
      "Step 171 (159088) @ Episode 649/10000, loss: 0.00030908593907952312\n",
      "Episode Reward: 0.0\n",
      "Step 233 (159321) @ Episode 650/10000, loss: 0.00014992051001172513\n",
      "Episode Reward: 1.0\n",
      "Step 181 (159502) @ Episode 651/10000, loss: 0.00040315967635251587\n",
      "Episode Reward: 0.0\n",
      "Step 194 (159696) @ Episode 652/10000, loss: 0.00027895916718989614\n",
      "Episode Reward: 0.0\n",
      "Step 256 (159952) @ Episode 653/10000, loss: 0.00163189135491848993\n",
      "Episode Reward: 1.0\n",
      "Step 47 (159999) @ Episode 654/10000, loss: 0.00061649753479287035\n",
      "Copied model parameters to target network.\n",
      "Step 170 (160122) @ Episode 654/10000, loss: 0.00391784030944109553\n",
      "Episode Reward: 0.0\n",
      "Step 172 (160294) @ Episode 655/10000, loss: 0.00102985394187271634\n",
      "Episode Reward: 0.0\n",
      "Step 280 (160574) @ Episode 656/10000, loss: 0.00035052793100476265\n",
      "Episode Reward: 2.0\n",
      "Step 324 (160898) @ Episode 657/10000, loss: 0.00082208384992554786\n",
      "Episode Reward: 3.0\n",
      "Step 261 (161159) @ Episode 658/10000, loss: 0.00020164409943390638\n",
      "Episode Reward: 2.0\n",
      "Step 173 (161332) @ Episode 659/10000, loss: 0.00203469023108482364\n",
      "Episode Reward: 0.0\n",
      "Step 350 (161682) @ Episode 660/10000, loss: 0.00199995376169681555\n",
      "Episode Reward: 3.0\n",
      "Step 243 (161925) @ Episode 661/10000, loss: 0.00118634069804102187\n",
      "Episode Reward: 2.0\n",
      "Step 313 (162238) @ Episode 662/10000, loss: 0.00050814135465770967\n",
      "Episode Reward: 2.0\n",
      "Step 187 (162425) @ Episode 663/10000, loss: 0.00024524744367226965\n",
      "Episode Reward: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 170 (162595) @ Episode 664/10000, loss: 0.00023601253633387387\n",
      "Episode Reward: 0.0\n",
      "Step 255 (162850) @ Episode 665/10000, loss: 0.00034899020101875067\n",
      "Episode Reward: 1.0\n",
      "Step 332 (163182) @ Episode 666/10000, loss: 0.00028961669886484744\n",
      "Episode Reward: 3.0\n",
      "Step 182 (163364) @ Episode 667/10000, loss: 0.00033192444243468344\n",
      "Episode Reward: 0.0\n",
      "Step 396 (163760) @ Episode 668/10000, loss: 0.00059754896210506565\n",
      "Episode Reward: 4.0\n",
      "Step 267 (164027) @ Episode 669/10000, loss: 0.00145754369441419842\n",
      "Episode Reward: 2.0\n",
      "Step 178 (164205) @ Episode 670/10000, loss: 0.00023800740018486977\n",
      "Episode Reward: 0.0\n",
      "Step 183 (164388) @ Episode 671/10000, loss: 0.00057026243302971127\n",
      "Episode Reward: 0.0\n",
      "Step 230 (164618) @ Episode 672/10000, loss: 0.00016343992319889367\n",
      "Episode Reward: 1.0\n",
      "Step 229 (164847) @ Episode 673/10000, loss: 0.00026413559680804613\n",
      "Episode Reward: 1.0\n",
      "Step 246 (165093) @ Episode 674/10000, loss: 0.00057544664014130837\n",
      "Episode Reward: 1.0\n",
      "Step 266 (165359) @ Episode 675/10000, loss: 0.00069691601675003777\n",
      "Episode Reward: 2.0\n",
      "Step 170 (165529) @ Episode 676/10000, loss: 0.00015446980250999331\n",
      "Episode Reward: 0.0\n",
      "Step 169 (165698) @ Episode 677/10000, loss: 0.00032285088673233986\n",
      "Episode Reward: 0.0\n",
      "Step 223 (165921) @ Episode 678/10000, loss: 0.00036805804120376706\n",
      "Episode Reward: 1.0\n",
      "Step 187 (166108) @ Episode 679/10000, loss: 0.00010801481403177604\n",
      "Episode Reward: 0.0\n",
      "Step 188 (166296) @ Episode 680/10000, loss: 0.00054159737192094337\n",
      "Episode Reward: 0.0\n",
      "Step 266 (166562) @ Episode 681/10000, loss: 0.00319776684045791637\n",
      "Episode Reward: 2.0\n",
      "Step 271 (166833) @ Episode 682/10000, loss: 0.00045860666432417932\n",
      "Episode Reward: 2.0\n",
      "Step 172 (167005) @ Episode 683/10000, loss: 9.38718076213263e-0524\n",
      "Episode Reward: 0.0\n",
      "Step 176 (167181) @ Episode 684/10000, loss: 0.00181481672916561376\n",
      "Episode Reward: 0.0\n",
      "Step 236 (167417) @ Episode 685/10000, loss: 0.00043486291542649273\n",
      "Episode Reward: 1.0\n",
      "Step 179 (167596) @ Episode 686/10000, loss: 0.00015164191427174956\n",
      "Episode Reward: 0.0\n",
      "Step 182 (167778) @ Episode 687/10000, loss: 0.00030435383087024095\n",
      "Episode Reward: 0.0\n",
      "Step 181 (167959) @ Episode 688/10000, loss: 0.00046066247159615168\n",
      "Episode Reward: 0.0\n",
      "Step 165 (168124) @ Episode 689/10000, loss: 0.00053807348012924193\n",
      "Episode Reward: 0.0\n",
      "Step 216 (168340) @ Episode 690/10000, loss: 0.00114257691893726592\n",
      "Episode Reward: 1.0\n",
      "Step 367 (168707) @ Episode 691/10000, loss: 0.00053957634372636685\n",
      "Episode Reward: 3.0\n",
      "Step 350 (169057) @ Episode 692/10000, loss: 0.00138800940476357945\n",
      "Episode Reward: 3.0\n",
      "Step 172 (169229) @ Episode 693/10000, loss: 0.00058529514353722333\n",
      "Episode Reward: 0.0\n",
      "Step 168 (169397) @ Episode 694/10000, loss: 8.719081233721226e-057\n",
      "Episode Reward: 0.0\n",
      "Step 240 (169637) @ Episode 695/10000, loss: 0.00032982515404000884\n",
      "Episode Reward: 1.0\n",
      "Step 229 (169866) @ Episode 696/10000, loss: 0.00184852944221347575\n",
      "Episode Reward: 1.0\n",
      "Step 133 (169999) @ Episode 697/10000, loss: 0.00045963059528730816\n",
      "Copied model parameters to target network.\n",
      "Step 234 (170100) @ Episode 697/10000, loss: 0.00195590243674814772\n",
      "Episode Reward: 1.0\n",
      "Step 172 (170272) @ Episode 698/10000, loss: 0.00055167410755536887\n",
      "Episode Reward: 0.0\n",
      "Step 166 (170438) @ Episode 699/10000, loss: 0.00031123359804041684\n",
      "Episode Reward: 0.0\n",
      "Step 349 (170787) @ Episode 700/10000, loss: 0.00058669666759669787\n",
      "Episode Reward: 4.0\n",
      "Step 172 (170959) @ Episode 701/10000, loss: 0.00024234833836089823\n",
      "Episode Reward: 0.0\n",
      "Step 239 (171198) @ Episode 702/10000, loss: 0.00029683657339774074\n",
      "Episode Reward: 1.0\n",
      "Step 235 (171433) @ Episode 703/10000, loss: 0.00218181451782584268\n",
      "Episode Reward: 1.0\n",
      "Step 231 (171664) @ Episode 704/10000, loss: 0.00121954153291881086\n",
      "Episode Reward: 1.0\n",
      "Step 226 (171890) @ Episode 705/10000, loss: 0.00073612621054053315\n",
      "Episode Reward: 1.0\n",
      "Step 266 (172156) @ Episode 706/10000, loss: 0.00627624988555908295\n",
      "Episode Reward: 2.0\n",
      "Step 169 (172325) @ Episode 707/10000, loss: 0.00116284959949553019\n",
      "Episode Reward: 0.0\n",
      "Step 161 (172486) @ Episode 708/10000, loss: 8.976345270639285e-056\n",
      "Episode Reward: 0.0\n",
      "Step 280 (172766) @ Episode 709/10000, loss: 0.00141586968675255786\n",
      "Episode Reward: 2.0\n",
      "Step 566 (173332) @ Episode 710/10000, loss: 0.00338463741354644337\n",
      "Episode Reward: 9.0\n",
      "Step 260 (173592) @ Episode 711/10000, loss: 0.00178874540142714983\n",
      "Episode Reward: 2.0\n",
      "Step 199 (173791) @ Episode 712/10000, loss: 0.00034586086985655131\n",
      "Episode Reward: 1.0\n",
      "Step 178 (173969) @ Episode 713/10000, loss: 0.00065726961474865677\n",
      "Episode Reward: 0.0\n",
      "Step 272 (174241) @ Episode 714/10000, loss: 0.00023895295453257862\n",
      "Episode Reward: 2.0\n",
      "Step 288 (174529) @ Episode 715/10000, loss: 0.00028942729113623522\n",
      "Episode Reward: 2.0\n",
      "Step 181 (174710) @ Episode 716/10000, loss: 0.00016705424059182405\n",
      "Episode Reward: 0.0\n",
      "Step 217 (174927) @ Episode 717/10000, loss: 0.00020749229588545864\n",
      "Episode Reward: 1.0\n",
      "Step 170 (175097) @ Episode 718/10000, loss: 0.00043601176002994187\n",
      "Episode Reward: 0.0\n",
      "Step 274 (175371) @ Episode 719/10000, loss: 0.00143796089105308061\n",
      "Episode Reward: 2.0\n",
      "Step 291 (175662) @ Episode 720/10000, loss: 0.00012013611558359116\n",
      "Episode Reward: 3.0\n",
      "Step 271 (175933) @ Episode 721/10000, loss: 0.00092971947742626073\n",
      "Episode Reward: 2.0\n",
      "Step 172 (176105) @ Episode 722/10000, loss: 0.00039424770511686855\n",
      "Episode Reward: 0.0\n",
      "Step 281 (176386) @ Episode 723/10000, loss: 0.00118279387243092067\n",
      "Episode Reward: 2.0\n",
      "Step 263 (176649) @ Episode 724/10000, loss: 0.00084415543824434286\n",
      "Episode Reward: 2.0\n",
      "Step 262 (176911) @ Episode 725/10000, loss: 0.00064393656793981794\n",
      "Episode Reward: 2.0\n",
      "Step 186 (177097) @ Episode 726/10000, loss: 0.00035308868973515937\n",
      "Episode Reward: 0.0\n",
      "Step 351 (177448) @ Episode 727/10000, loss: 0.00046987150562927127\n",
      "Episode Reward: 4.0\n",
      "Step 168 (177616) @ Episode 728/10000, loss: 0.00117498554755002264\n",
      "Episode Reward: 0.0\n",
      "Step 235 (177851) @ Episode 729/10000, loss: 0.00065324519528076053\n",
      "Episode Reward: 1.0\n",
      "Step 174 (178025) @ Episode 730/10000, loss: 0.00130281853489577777\n",
      "Episode Reward: 0.0\n",
      "Step 294 (178319) @ Episode 731/10000, loss: 0.00010096661571878945\n",
      "Episode Reward: 2.0\n",
      "Step 175 (178494) @ Episode 732/10000, loss: 0.00069647532654926185\n",
      "Episode Reward: 0.0\n",
      "Step 434 (178928) @ Episode 733/10000, loss: 0.00047884118976071477\n",
      "Episode Reward: 5.0\n",
      "Step 167 (179095) @ Episode 734/10000, loss: 0.00174141605384647856\n",
      "Episode Reward: 0.0\n",
      "Step 271 (179366) @ Episode 735/10000, loss: 0.00045757833868265156\n",
      "Episode Reward: 2.0\n",
      "Step 234 (179600) @ Episode 736/10000, loss: 0.00073264667298644783\n",
      "Episode Reward: 1.0\n",
      "Step 178 (179778) @ Episode 737/10000, loss: 0.00150709203444421374\n",
      "Episode Reward: 0.0\n",
      "Step 221 (179999) @ Episode 738/10000, loss: 0.00059176492504775523\n",
      "Copied model parameters to target network.\n",
      "Step 242 (180020) @ Episode 738/10000, loss: 0.00031389447394758463\n",
      "Episode Reward: 1.0\n",
      "Step 215 (180235) @ Episode 739/10000, loss: 0.00041106581920757895\n",
      "Episode Reward: 1.0\n",
      "Step 366 (180601) @ Episode 740/10000, loss: 0.00040080939652398237\n",
      "Episode Reward: 3.0\n",
      "Step 270 (180871) @ Episode 741/10000, loss: 0.00023749234969727695\n",
      "Episode Reward: 2.0\n",
      "Step 265 (181136) @ Episode 742/10000, loss: 0.00274489773437380885\n",
      "Episode Reward: 2.0\n",
      "Step 481 (181617) @ Episode 743/10000, loss: 0.00053993461187928924\n",
      "Episode Reward: 5.0\n",
      "Step 355 (181972) @ Episode 744/10000, loss: 0.00056643469724804164\n",
      "Episode Reward: 3.0\n",
      "Step 227 (182199) @ Episode 745/10000, loss: 0.00013979373034089804\n",
      "Episode Reward: 1.0\n",
      "Step 162 (182361) @ Episode 746/10000, loss: 0.00021387015294749293\n",
      "Episode Reward: 0.0\n",
      "Step 227 (182588) @ Episode 747/10000, loss: 0.00214280001819133765\n",
      "Episode Reward: 1.0\n",
      "Step 178 (182766) @ Episode 748/10000, loss: 0.00013828816008754075\n",
      "Episode Reward: 0.0\n",
      "Step 339 (183105) @ Episode 749/10000, loss: 0.00011109791375929499\n",
      "Episode Reward: 3.0\n",
      "Step 170 (183275) @ Episode 750/10000, loss: 0.00146419089287519453\n",
      "Episode Reward: 0.0\n",
      "Step 210 (183485) @ Episode 751/10000, loss: 0.00050290900981053715\n",
      "Episode Reward: 1.0\n",
      "Step 202 (183687) @ Episode 752/10000, loss: 0.00044363475171849131\n",
      "Episode Reward: 1.0\n",
      "Step 238 (183925) @ Episode 753/10000, loss: 0.00055575807346031075\n",
      "Episode Reward: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 298 (184223) @ Episode 754/10000, loss: 0.00037268392043188214\n",
      "Episode Reward: 2.0\n",
      "Step 234 (184457) @ Episode 755/10000, loss: 0.00097172270761802793\n",
      "Episode Reward: 1.0\n",
      "Step 284 (184741) @ Episode 756/10000, loss: 0.00032926994026638578\n",
      "Episode Reward: 2.0\n",
      "Step 284 (185025) @ Episode 757/10000, loss: 0.00027658924227580439\n",
      "Episode Reward: 2.0\n",
      "Step 398 (185423) @ Episode 758/10000, loss: 0.00045703258365392685\n",
      "Episode Reward: 4.0\n",
      "Step 167 (185590) @ Episode 759/10000, loss: 0.00172848557122051723\n",
      "Episode Reward: 0.0\n",
      "Step 165 (185755) @ Episode 760/10000, loss: 0.00064222060609608894\n",
      "Episode Reward: 0.0\n",
      "Step 311 (186066) @ Episode 761/10000, loss: 0.00019617014913819733\n",
      "Episode Reward: 2.0\n",
      "Step 189 (186255) @ Episode 762/10000, loss: 0.00043032644316554075\n",
      "Episode Reward: 0.0\n",
      "Step 176 (186431) @ Episode 763/10000, loss: 0.00015093554975464947\n",
      "Episode Reward: 0.0\n",
      "Step 329 (186760) @ Episode 764/10000, loss: 0.00081899372162297373\n",
      "Episode Reward: 3.0\n",
      "Step 399 (187159) @ Episode 765/10000, loss: 0.00013910999405197836\n",
      "Episode Reward: 3.0\n",
      "Step 165 (187324) @ Episode 766/10000, loss: 0.00253572827205061944\n",
      "Episode Reward: 0.0\n",
      "Step 177 (187501) @ Episode 767/10000, loss: 0.00140591547824442395\n",
      "Episode Reward: 0.0\n",
      "Step 185 (187686) @ Episode 768/10000, loss: 0.00024049034982454032\n",
      "Episode Reward: 0.0\n",
      "Step 239 (187925) @ Episode 769/10000, loss: 0.00059616647195070984\n",
      "Episode Reward: 1.0\n",
      "Step 170 (188095) @ Episode 770/10000, loss: 0.00034972946741618216\n",
      "Episode Reward: 0.0\n",
      "Step 292 (188387) @ Episode 771/10000, loss: 0.00092197454068809757\n",
      "Episode Reward: 2.0\n",
      "Step 230 (188617) @ Episode 772/10000, loss: 0.00405547115951776527\n",
      "Episode Reward: 1.0\n",
      "Step 267 (188884) @ Episode 773/10000, loss: 0.00035304078483022758\n",
      "Episode Reward: 2.0\n",
      "Step 272 (189156) @ Episode 774/10000, loss: 0.00010349774674978107\n",
      "Episode Reward: 2.0\n",
      "Step 225 (189381) @ Episode 775/10000, loss: 0.00057562039000913517\n",
      "Episode Reward: 1.0\n",
      "Step 293 (189674) @ Episode 776/10000, loss: 0.00131136912386864427\n",
      "Episode Reward: 2.0\n",
      "Step 325 (189999) @ Episode 777/10000, loss: 0.00017536953964736313\n",
      "Copied model parameters to target network.\n",
      "Step 458 (190132) @ Episode 777/10000, loss: 0.00074681953992694627\n",
      "Episode Reward: 4.0\n",
      "Step 223 (190355) @ Episode 778/10000, loss: 0.00045944898738525818\n",
      "Episode Reward: 1.0\n",
      "Step 475 (190830) @ Episode 779/10000, loss: 0.00036235779407434165\n",
      "Episode Reward: 8.0\n",
      "Step 163 (190993) @ Episode 780/10000, loss: 0.00010428349196445197\n",
      "Episode Reward: 0.0\n",
      "Step 213 (191206) @ Episode 781/10000, loss: 0.00192283059004694226\n",
      "Episode Reward: 1.0\n",
      "Step 168 (191374) @ Episode 782/10000, loss: 0.00030345306731760595\n",
      "Episode Reward: 0.0\n",
      "Step 269 (191643) @ Episode 783/10000, loss: 0.00035532331094145775\n",
      "Episode Reward: 2.0\n",
      "Step 350 (191993) @ Episode 784/10000, loss: 0.00073781504761427649\n",
      "Episode Reward: 3.0\n",
      "Step 483 (192476) @ Episode 785/10000, loss: 0.00040427208296023315\n",
      "Episode Reward: 6.0\n",
      "Step 165 (192641) @ Episode 786/10000, loss: 0.00033704270026646554\n",
      "Episode Reward: 0.0\n",
      "Step 225 (192866) @ Episode 787/10000, loss: 0.00042446330189704895\n",
      "Episode Reward: 1.0\n",
      "Step 298 (193164) @ Episode 788/10000, loss: 0.00042480131378397346\n",
      "Episode Reward: 2.0\n",
      "Step 220 (193384) @ Episode 789/10000, loss: 0.00258580036461353308\n",
      "Episode Reward: 1.0\n",
      "Step 205 (193589) @ Episode 790/10000, loss: 0.00774770835414528855\n",
      "Episode Reward: 1.0\n",
      "Step 166 (193755) @ Episode 791/10000, loss: 0.00193993491120636465\n",
      "Episode Reward: 0.0\n",
      "Step 162 (193917) @ Episode 792/10000, loss: 0.00040807836921885617\n",
      "Episode Reward: 0.0\n",
      "Step 160 (194077) @ Episode 793/10000, loss: 0.00055984937353059655\n",
      "Episode Reward: 0.0\n",
      "Step 180 (194257) @ Episode 794/10000, loss: 0.00303660565987229355\n",
      "Episode Reward: 0.0\n",
      "Step 296 (194553) @ Episode 795/10000, loss: 0.00012604049697984015\n",
      "Episode Reward: 2.0\n",
      "Step 171 (194724) @ Episode 796/10000, loss: 0.00073124840855598455\n",
      "Episode Reward: 0.0\n",
      "Step 174 (194898) @ Episode 797/10000, loss: 0.00012615122250281274\n",
      "Episode Reward: 0.0\n",
      "Step 211 (195109) @ Episode 798/10000, loss: 0.00019146567501593385\n",
      "Episode Reward: 1.0\n",
      "Step 283 (195392) @ Episode 799/10000, loss: 0.00022504385560750963\n",
      "Episode Reward: 2.0\n",
      "Step 200 (195592) @ Episode 800/10000, loss: 0.00029876566259190447\n",
      "Episode Reward: 1.0\n",
      "Step 368 (195960) @ Episode 801/10000, loss: 0.00016013783169910312\n",
      "Episode Reward: 3.0\n",
      "Step 247 (196207) @ Episode 802/10000, loss: 0.00041250637150369585\n",
      "Episode Reward: 1.0\n",
      "Step 210 (196417) @ Episode 803/10000, loss: 0.00013230016338638967\n",
      "Episode Reward: 1.0\n",
      "Step 174 (196591) @ Episode 804/10000, loss: 0.00042153531103394926\n",
      "Episode Reward: 0.0\n",
      "Step 277 (196868) @ Episode 805/10000, loss: 0.00033456017263233665\n",
      "Episode Reward: 2.0\n",
      "Step 177 (197045) @ Episode 806/10000, loss: 0.00022373240790329874\n",
      "Episode Reward: 0.0\n",
      "Step 176 (197221) @ Episode 807/10000, loss: 0.00020737755403388292\n",
      "Episode Reward: 0.0\n",
      "Step 286 (197507) @ Episode 808/10000, loss: 0.00032457883935421705\n",
      "Episode Reward: 2.0\n",
      "Step 243 (197750) @ Episode 809/10000, loss: 0.00040354154771193863\n",
      "Episode Reward: 1.0\n",
      "Step 407 (198157) @ Episode 810/10000, loss: 0.00093074532924219976\n",
      "Episode Reward: 4.0\n",
      "Step 211 (198368) @ Episode 811/10000, loss: 0.00097498332615941767\n",
      "Episode Reward: 1.0\n",
      "Step 175 (198543) @ Episode 812/10000, loss: 0.00028081392520107335\n",
      "Episode Reward: 0.0\n",
      "Step 270 (198813) @ Episode 813/10000, loss: 0.00027916510589420795\n",
      "Episode Reward: 2.0\n",
      "Step 243 (199056) @ Episode 814/10000, loss: 0.00032684378675185144\n",
      "Episode Reward: 1.0\n",
      "Step 336 (199392) @ Episode 815/10000, loss: 0.00022528239060193364\n",
      "Episode Reward: 3.0\n",
      "Step 242 (199634) @ Episode 816/10000, loss: 0.00068698835093528038\n",
      "Episode Reward: 1.0\n",
      "Step 214 (199848) @ Episode 817/10000, loss: 0.00055368104949593544\n",
      "Episode Reward: 1.0\n",
      "Step 151 (199999) @ Episode 818/10000, loss: 0.00089399109128862626\n",
      "Copied model parameters to target network.\n",
      "Step 271 (200119) @ Episode 818/10000, loss: 0.00027236915775574744\n",
      "Episode Reward: 2.0\n",
      "Step 414 (200533) @ Episode 819/10000, loss: 0.00041676140972413124\n",
      "Episode Reward: 4.0\n",
      "Step 169 (200702) @ Episode 820/10000, loss: 0.00035142240813001999\n",
      "Episode Reward: 0.0\n",
      "Step 165 (200867) @ Episode 821/10000, loss: 0.00138847134076058866\n",
      "Episode Reward: 0.0\n",
      "Step 164 (201031) @ Episode 822/10000, loss: 6.664022657787427e-058\n",
      "Episode Reward: 0.0\n",
      "Step 252 (201283) @ Episode 823/10000, loss: 0.00082924572052434095\n",
      "Episode Reward: 1.0\n",
      "Step 211 (201494) @ Episode 824/10000, loss: 0.00216305232606828202\n",
      "Episode Reward: 1.0\n",
      "Step 258 (201752) @ Episode 825/10000, loss: 0.00055448396597057582\n",
      "Episode Reward: 2.0\n",
      "Step 405 (202157) @ Episode 826/10000, loss: 0.00029605632880702615\n",
      "Episode Reward: 4.0\n",
      "Step 302 (202459) @ Episode 827/10000, loss: 0.00029806714155711234\n",
      "Episode Reward: 2.0\n",
      "Step 262 (202721) @ Episode 828/10000, loss: 0.00072745478246361028\n",
      "Episode Reward: 2.0\n",
      "Step 208 (202929) @ Episode 829/10000, loss: 0.00023287822841666645\n",
      "Episode Reward: 1.0\n",
      "Step 214 (203143) @ Episode 830/10000, loss: 0.00062640273245051564\n",
      "Episode Reward: 1.0\n",
      "Step 428 (203571) @ Episode 831/10000, loss: 0.00027036093524657196\n",
      "Episode Reward: 4.0\n",
      "Step 332 (203903) @ Episode 832/10000, loss: 0.00066533539211377515\n",
      "Episode Reward: 3.0\n",
      "Step 176 (204079) @ Episode 833/10000, loss: 0.00020398836932145067\n",
      "Episode Reward: 0.0\n",
      "Step 308 (204387) @ Episode 834/10000, loss: 0.00068321410799399029\n",
      "Episode Reward: 2.0\n",
      "Step 297 (204684) @ Episode 835/10000, loss: 0.00096444331575185066\n",
      "Episode Reward: 2.0\n",
      "Step 177 (204861) @ Episode 836/10000, loss: 0.00061729468870908026\n",
      "Episode Reward: 0.0\n",
      "Step 272 (205133) @ Episode 837/10000, loss: 0.00025313097285106782\n",
      "Episode Reward: 2.0\n",
      "Step 312 (205445) @ Episode 838/10000, loss: 0.00030143602634780113\n",
      "Episode Reward: 2.0\n",
      "Step 303 (205748) @ Episode 839/10000, loss: 9.77513482212089e-0531\n",
      "Episode Reward: 2.0\n",
      "Step 176 (205924) @ Episode 840/10000, loss: 0.00030796125065535307\n",
      "Episode Reward: 0.0\n",
      "Step 182 (206106) @ Episode 841/10000, loss: 0.00074559723725542437\n",
      "Episode Reward: 0.0\n",
      "Step 181 (206287) @ Episode 842/10000, loss: 0.00031400081934407353\n",
      "Episode Reward: 0.0\n",
      "Step 189 (206476) @ Episode 843/10000, loss: 0.00052965356735512617\n",
      "Episode Reward: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 169 (206645) @ Episode 844/10000, loss: 0.00024216082238126546\n",
      "Episode Reward: 0.0\n",
      "Step 231 (206876) @ Episode 845/10000, loss: 0.00014124714653007686\n",
      "Episode Reward: 1.0\n",
      "Step 328 (207204) @ Episode 846/10000, loss: 0.00046710437163710594\n",
      "Episode Reward: 3.0\n",
      "Step 482 (207686) @ Episode 847/10000, loss: 0.00030499394051730633\n",
      "Episode Reward: 5.0\n",
      "Step 248 (207934) @ Episode 848/10000, loss: 0.00032998659298755233\n",
      "Episode Reward: 1.0\n",
      "Step 229 (208163) @ Episode 849/10000, loss: 0.00028212886536493977\n",
      "Episode Reward: 1.0\n",
      "Step 272 (208435) @ Episode 850/10000, loss: 0.00061343575362116142\n",
      "Episode Reward: 2.0\n",
      "Step 356 (208791) @ Episode 851/10000, loss: 0.00055393169168382888\n",
      "Episode Reward: 3.0\n",
      "Step 443 (209234) @ Episode 852/10000, loss: 0.00019766800687648356\n",
      "Episode Reward: 5.0\n",
      "Step 247 (209481) @ Episode 853/10000, loss: 0.00233527179807424555\n",
      "Episode Reward: 1.0\n",
      "Step 187 (209668) @ Episode 854/10000, loss: 0.00019216521468479186\n",
      "Episode Reward: 0.0\n",
      "Step 264 (209932) @ Episode 855/10000, loss: 0.00061033805832266811\n",
      "Episode Reward: 2.0\n",
      "Step 67 (209999) @ Episode 856/10000, loss: 0.00077694829087704422\n",
      "Copied model parameters to target network.\n",
      "Step 189 (210121) @ Episode 856/10000, loss: 0.00235757301561534426\n",
      "Episode Reward: 0.0\n",
      "Step 163 (210284) @ Episode 857/10000, loss: 0.00067749881418421868\n",
      "Episode Reward: 0.0\n",
      "Step 234 (210518) @ Episode 858/10000, loss: 0.00035721156746149063\n",
      "Episode Reward: 1.0\n",
      "Step 175 (210693) @ Episode 859/10000, loss: 0.00099862378556281334\n",
      "Episode Reward: 0.0\n",
      "Step 239 (210932) @ Episode 860/10000, loss: 0.00505679287016391753\n",
      "Episode Reward: 1.0\n",
      "Step 390 (211322) @ Episode 861/10000, loss: 0.00122578104492276994\n",
      "Episode Reward: 4.0\n",
      "Step 233 (211555) @ Episode 862/10000, loss: 0.00011900222307303921\n",
      "Episode Reward: 1.0\n",
      "Step 215 (211770) @ Episode 863/10000, loss: 0.00013299167039804163\n",
      "Episode Reward: 1.0\n",
      "Step 161 (211931) @ Episode 864/10000, loss: 0.00018486622138880193\n",
      "Episode Reward: 0.0\n",
      "Step 270 (212201) @ Episode 865/10000, loss: 0.00096830772235989574\n",
      "Episode Reward: 2.0\n",
      "Step 292 (212493) @ Episode 866/10000, loss: 0.00102642492856830367\n",
      "Episode Reward: 2.0\n",
      "Step 285 (212778) @ Episode 867/10000, loss: 0.00146649277303367856\n",
      "Episode Reward: 2.0\n",
      "Step 231 (213009) @ Episode 868/10000, loss: 0.00078370439587160943\n",
      "Episode Reward: 1.0\n",
      "Step 279 (213288) @ Episode 869/10000, loss: 0.00027489007334224883\n",
      "Episode Reward: 2.0\n",
      "Step 227 (213515) @ Episode 870/10000, loss: 0.00048160273581743247\n",
      "Episode Reward: 1.0\n",
      "Step 240 (213755) @ Episode 871/10000, loss: 0.00016517002950422466\n",
      "Episode Reward: 1.0\n",
      "Step 168 (213923) @ Episode 872/10000, loss: 0.00027045613387599597\n",
      "Episode Reward: 0.0\n",
      "Step 174 (214097) @ Episode 873/10000, loss: 0.00028900383040308951\n",
      "Episode Reward: 0.0\n",
      "Step 177 (214274) @ Episode 874/10000, loss: 0.00031138659687712793\n",
      "Episode Reward: 0.0\n",
      "Step 292 (214566) @ Episode 875/10000, loss: 0.00032041221857070923\n",
      "Episode Reward: 2.0\n",
      "Step 168 (214734) @ Episode 876/10000, loss: 0.00084547727601602676\n",
      "Episode Reward: 0.0\n",
      "Step 355 (215089) @ Episode 877/10000, loss: 0.00048402993706986314\n",
      "Episode Reward: 2.0\n",
      "Step 236 (215325) @ Episode 878/10000, loss: 0.00015135193825699394\n",
      "Episode Reward: 1.0\n",
      "Step 165 (215490) @ Episode 879/10000, loss: 0.00013457334716804326\n",
      "Episode Reward: 0.0\n",
      "Step 289 (215779) @ Episode 880/10000, loss: 0.00035731907701119784\n",
      "Episode Reward: 2.0\n",
      "Step 194 (215973) @ Episode 881/10000, loss: 0.00014112102508079262\n",
      "Episode Reward: 0.0\n",
      "Step 193 (216166) @ Episode 882/10000, loss: 0.00019445954239927232\n",
      "Episode Reward: 1.0\n",
      "Step 470 (216636) @ Episode 883/10000, loss: 9.549559035804123e-056\n",
      "Episode Reward: 5.0\n",
      "Step 168 (216804) @ Episode 884/10000, loss: 0.00012185967352706939\n",
      "Episode Reward: 0.0\n",
      "Step 180 (216984) @ Episode 885/10000, loss: 0.00066343246726319191\n",
      "Episode Reward: 0.0\n",
      "Step 373 (217357) @ Episode 886/10000, loss: 0.00050398294115439065\n",
      "Episode Reward: 3.0\n",
      "Step 283 (217640) @ Episode 887/10000, loss: 0.00140379462391138083\n",
      "Episode Reward: 2.0\n",
      "Step 223 (217863) @ Episode 888/10000, loss: 0.00026000445359386504\n",
      "Episode Reward: 1.0\n",
      "Step 195 (218058) @ Episode 889/10000, loss: 0.00059791456442326312\n",
      "Episode Reward: 1.0\n",
      "Step 176 (218234) @ Episode 890/10000, loss: 0.00130724371410906317\n",
      "Episode Reward: 0.0\n",
      "Step 251 (218485) @ Episode 891/10000, loss: 0.00040625786641612653\n",
      "Episode Reward: 1.0\n",
      "Step 166 (218651) @ Episode 892/10000, loss: 4.556227213470265e-057\n",
      "Episode Reward: 0.0\n",
      "Step 164 (218815) @ Episode 893/10000, loss: 0.00089207367273047574\n",
      "Episode Reward: 0.0\n",
      "Step 269 (219084) @ Episode 894/10000, loss: 0.00228646094910800467\n",
      "Episode Reward: 2.0\n",
      "Step 259 (219343) @ Episode 895/10000, loss: 0.00144477176945656546\n",
      "Episode Reward: 2.0\n",
      "Step 284 (219627) @ Episode 896/10000, loss: 0.00086574873421341187\n",
      "Episode Reward: 2.0\n",
      "Step 365 (219992) @ Episode 897/10000, loss: 0.00062233634525910024\n",
      "Episode Reward: 3.0\n",
      "Step 7 (219999) @ Episode 898/10000, loss: 0.00044300119043327874\n",
      "Copied model parameters to target network.\n",
      "Step 203 (220195) @ Episode 898/10000, loss: 0.00107433390803635123\n",
      "Episode Reward: 0.0\n",
      "Step 369 (220564) @ Episode 899/10000, loss: 0.00027428398607298733\n",
      "Episode Reward: 3.0\n",
      "Step 286 (220850) @ Episode 900/10000, loss: 0.00011416162305977196\n",
      "Episode Reward: 2.0\n",
      "Step 170 (221020) @ Episode 901/10000, loss: 0.00578521285206079525\n",
      "Episode Reward: 0.0\n",
      "Step 315 (221335) @ Episode 902/10000, loss: 0.00071483151987195017\n",
      "Episode Reward: 2.0\n",
      "Step 284 (221619) @ Episode 903/10000, loss: 7.88202160038054e-0579\n",
      "Episode Reward: 2.0\n",
      "Step 242 (221861) @ Episode 904/10000, loss: 0.00066872296156361745\n",
      "Episode Reward: 1.0\n",
      "Step 168 (222029) @ Episode 905/10000, loss: 0.00028915930306538948\n",
      "Episode Reward: 0.0\n",
      "Step 167 (222196) @ Episode 906/10000, loss: 0.00016634617350064218\n",
      "Episode Reward: 0.0\n",
      "Step 324 (222520) @ Episode 907/10000, loss: 0.00010908655531238765\n",
      "Episode Reward: 3.0\n",
      "Step 303 (222823) @ Episode 908/10000, loss: 0.00048358869389630854\n",
      "Episode Reward: 2.0\n",
      "Step 165 (222988) @ Episode 909/10000, loss: 4.072083902428858e-056\n",
      "Episode Reward: 0.0\n",
      "Step 181 (223169) @ Episode 910/10000, loss: 0.00016902484640013427\n",
      "Episode Reward: 0.0\n",
      "Step 168 (223337) @ Episode 911/10000, loss: 0.00069816346513107422\n",
      "Episode Reward: 0.0\n",
      "Step 174 (223511) @ Episode 912/10000, loss: 0.00038572179619222884\n",
      "Episode Reward: 0.0\n",
      "Step 173 (223684) @ Episode 913/10000, loss: 0.00036299967905506492\n",
      "Episode Reward: 0.0\n",
      "Step 244 (223928) @ Episode 914/10000, loss: 0.00013688979379367083\n",
      "Episode Reward: 1.0\n",
      "Step 285 (224213) @ Episode 915/10000, loss: 0.00060718564782291653\n",
      "Episode Reward: 2.0\n",
      "Step 246 (224459) @ Episode 916/10000, loss: 0.00132021505851298573\n",
      "Episode Reward: 1.0\n",
      "Step 172 (224631) @ Episode 917/10000, loss: 0.00023982343554962426\n",
      "Episode Reward: 0.0\n",
      "Step 216 (224847) @ Episode 918/10000, loss: 0.00028973360895179215\n",
      "Episode Reward: 1.0\n",
      "Step 240 (225087) @ Episode 919/10000, loss: 0.00012122831685701385\n",
      "Episode Reward: 1.0\n",
      "Step 249 (225336) @ Episode 920/10000, loss: 0.00086832197848707443\n",
      "Episode Reward: 1.0\n",
      "Step 206 (225542) @ Episode 921/10000, loss: 0.00012693271855823696\n",
      "Episode Reward: 1.0\n",
      "Step 411 (225953) @ Episode 922/10000, loss: 0.00014589510101359338\n",
      "Episode Reward: 5.0\n",
      "Step 271 (226224) @ Episode 923/10000, loss: 0.00095907633658498534\n",
      "Episode Reward: 2.0\n",
      "Step 415 (226639) @ Episode 924/10000, loss: 0.00010041797213489192\n",
      "Episode Reward: 4.0\n",
      "Step 348 (226987) @ Episode 925/10000, loss: 0.00023037525534164169\n",
      "Episode Reward: 3.0\n",
      "Step 177 (227164) @ Episode 926/10000, loss: 0.00168006052263081072\n",
      "Episode Reward: 0.0\n",
      "Step 293 (227457) @ Episode 927/10000, loss: 0.00021852509235031903\n",
      "Episode Reward: 2.0\n",
      "Step 316 (227773) @ Episode 928/10000, loss: 0.00064774788916110995\n",
      "Episode Reward: 2.0\n",
      "Step 183 (227956) @ Episode 929/10000, loss: 0.00029097840888425715\n",
      "Episode Reward: 0.0\n",
      "Step 210 (228166) @ Episode 930/10000, loss: 0.00010989126894855872\n",
      "Episode Reward: 1.0\n",
      "Step 174 (228340) @ Episode 931/10000, loss: 0.00027149735251441698\n",
      "Episode Reward: 0.0\n",
      "Step 338 (228678) @ Episode 932/10000, loss: 0.00142069603316485884\n",
      "Episode Reward: 4.0\n",
      "Step 346 (229024) @ Episode 933/10000, loss: 0.00037016483838669956\n",
      "Episode Reward: 3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 350 (229374) @ Episode 934/10000, loss: 0.00011161863949382678\n",
      "Episode Reward: 3.0\n",
      "Step 237 (229611) @ Episode 935/10000, loss: 0.00128847314044833186\n",
      "Episode Reward: 1.0\n",
      "Step 340 (229951) @ Episode 936/10000, loss: 0.00073386938311159615\n",
      "Episode Reward: 4.0\n",
      "Step 48 (229999) @ Episode 937/10000, loss: 0.00032567110611125827\n",
      "Copied model parameters to target network.\n",
      "Step 269 (230220) @ Episode 937/10000, loss: 0.00068666914012283093\n",
      "Episode Reward: 1.0\n",
      "Step 267 (230487) @ Episode 938/10000, loss: 0.00423044338822364886\n",
      "Episode Reward: 2.0\n",
      "Step 234 (230721) @ Episode 939/10000, loss: 0.00019054964650422335\n",
      "Episode Reward: 1.0\n",
      "Step 208 (230929) @ Episode 940/10000, loss: 0.00024308494175784297\n",
      "Episode Reward: 0.0\n",
      "Step 173 (231102) @ Episode 941/10000, loss: 8.78048813319765e-0513\n",
      "Episode Reward: 0.0\n",
      "Step 168 (231270) @ Episode 942/10000, loss: 0.00060639483854174614\n",
      "Episode Reward: 0.0\n",
      "Step 238 (231508) @ Episode 943/10000, loss: 0.00048206435167230666\n",
      "Episode Reward: 1.0\n",
      "Step 241 (231749) @ Episode 944/10000, loss: 0.01220600679516792317\n",
      "Episode Reward: 1.0\n",
      "Step 168 (231917) @ Episode 945/10000, loss: 0.00030157304718159143\n",
      "Episode Reward: 0.0\n",
      "Step 254 (232171) @ Episode 946/10000, loss: 0.00261115096509456633\n",
      "Episode Reward: 1.0\n",
      "Step 282 (232453) @ Episode 947/10000, loss: 0.00080996687756851328\n",
      "Episode Reward: 2.0\n",
      "Step 190 (232643) @ Episode 948/10000, loss: 0.00032203464070335036\n",
      "Episode Reward: 0.0\n",
      "Step 214 (232857) @ Episode 949/10000, loss: 0.00061356317019090067\n",
      "Episode Reward: 1.0\n",
      "Step 182 (233039) @ Episode 950/10000, loss: 0.00011456418724264955\n",
      "Episode Reward: 0.0\n",
      "Step 389 (233428) @ Episode 951/10000, loss: 0.00046167679829522975\n",
      "Episode Reward: 4.0\n",
      "Step 278 (233706) @ Episode 952/10000, loss: 0.00161638646386563785\n",
      "Episode Reward: 2.0\n",
      "Step 254 (233960) @ Episode 953/10000, loss: 0.00048616863205097616\n",
      "Episode Reward: 1.0\n",
      "Step 246 (234206) @ Episode 954/10000, loss: 0.00096912635490298277\n",
      "Episode Reward: 1.0\n",
      "Step 285 (234491) @ Episode 955/10000, loss: 0.00073361553950235256\n",
      "Episode Reward: 2.0\n",
      "Step 346 (234837) @ Episode 956/10000, loss: 0.00017143404693342745\n",
      "Episode Reward: 3.0\n",
      "Step 176 (235013) @ Episode 957/10000, loss: 0.00027972881798632443\n",
      "Episode Reward: 0.0\n",
      "Step 239 (235252) @ Episode 958/10000, loss: 0.00064682809170335536\n",
      "Episode Reward: 1.0\n",
      "Step 182 (235434) @ Episode 959/10000, loss: 0.00207045348361134534\n",
      "Episode Reward: 0.0\n",
      "Step 277 (235711) @ Episode 960/10000, loss: 0.00269433157518506056\n",
      "Episode Reward: 2.0\n",
      "Step 198 (235909) @ Episode 961/10000, loss: 9.867871267488226e-052\n",
      "Episode Reward: 1.0\n",
      "Step 270 (236179) @ Episode 962/10000, loss: 0.00097411760361865165\n",
      "Episode Reward: 2.0\n",
      "Step 304 (236483) @ Episode 963/10000, loss: 0.00034491793485358363\n",
      "Episode Reward: 2.0\n",
      "Step 240 (236723) @ Episode 964/10000, loss: 0.00020866226986981928\n",
      "Episode Reward: 1.0\n",
      "Step 254 (236977) @ Episode 965/10000, loss: 0.00051185523625463256\n",
      "Episode Reward: 1.0\n",
      "Step 274 (237251) @ Episode 966/10000, loss: 0.00279007316567003733\n",
      "Episode Reward: 2.0\n",
      "Step 188 (237439) @ Episode 967/10000, loss: 0.00012187066022306681\n",
      "Episode Reward: 0.0\n",
      "Step 285 (237724) @ Episode 968/10000, loss: 9.73834321484901e-0553\n",
      "Episode Reward: 2.0\n",
      "Step 243 (237967) @ Episode 969/10000, loss: 0.00056189729366451553\n",
      "Episode Reward: 1.0\n",
      "Step 172 (238139) @ Episode 970/10000, loss: 0.00017003281391225755\n",
      "Episode Reward: 0.0\n",
      "Step 428 (238567) @ Episode 971/10000, loss: 0.00011947079474339262\n",
      "Episode Reward: 4.0\n",
      "Step 229 (238796) @ Episode 972/10000, loss: 0.00019923057698179036\n",
      "Episode Reward: 1.0\n",
      "Step 215 (239011) @ Episode 973/10000, loss: 0.00112780684139579534\n",
      "Episode Reward: 1.0\n",
      "Step 309 (239320) @ Episode 974/10000, loss: 0.00052422552835196264\n",
      "Episode Reward: 3.0\n",
      "Step 271 (239591) @ Episode 975/10000, loss: 0.00063936819788068534\n",
      "Episode Reward: 2.0\n",
      "Step 334 (239925) @ Episode 976/10000, loss: 0.00273729837499558934\n",
      "Episode Reward: 3.0\n",
      "Step 74 (239999) @ Episode 977/10000, loss: 0.00037776128738187253\n",
      "Copied model parameters to target network.\n",
      "Step 333 (240258) @ Episode 977/10000, loss: 0.00025739194825291634\n",
      "Episode Reward: 3.0\n",
      "Step 173 (240431) @ Episode 978/10000, loss: 0.00170709495432674885\n",
      "Episode Reward: 0.0\n",
      "Step 179 (240610) @ Episode 979/10000, loss: 0.00043652168824337423\n",
      "Episode Reward: 0.0\n",
      "Step 181 (240791) @ Episode 980/10000, loss: 0.00504592340439558144\n",
      "Episode Reward: 0.0\n",
      "Step 220 (241011) @ Episode 981/10000, loss: 0.00073440000414848338\n",
      "Episode Reward: 1.0\n",
      "Step 237 (241248) @ Episode 982/10000, loss: 0.00133185100276023154\n",
      "Episode Reward: 1.0\n",
      "Step 285 (241533) @ Episode 983/10000, loss: 0.00016729033086448908\n",
      "Episode Reward: 2.0\n",
      "Step 294 (241827) @ Episode 984/10000, loss: 0.00022148434072732925\n",
      "Episode Reward: 2.0\n",
      "Step 187 (242014) @ Episode 985/10000, loss: 0.00045964581659063697\n",
      "Episode Reward: 0.0\n",
      "Step 317 (242331) @ Episode 986/10000, loss: 0.01043660193681717422\n",
      "Episode Reward: 2.0\n",
      "Step 212 (242543) @ Episode 987/10000, loss: 0.00075627322075888516\n",
      "Episode Reward: 1.0\n",
      "Step 211 (242754) @ Episode 988/10000, loss: 0.00038986458093859255\n",
      "Episode Reward: 1.0\n",
      "Step 278 (243032) @ Episode 989/10000, loss: 0.00030306304688565433\n",
      "Episode Reward: 2.0\n",
      "Step 241 (243273) @ Episode 990/10000, loss: 0.00737109547480940855\n",
      "Episode Reward: 1.0\n",
      "Step 227 (243500) @ Episode 991/10000, loss: 0.00105679838452488186\n",
      "Episode Reward: 1.0\n",
      "Step 246 (243746) @ Episode 992/10000, loss: 0.00106083636637777189\n",
      "Episode Reward: 1.0\n",
      "Step 270 (244016) @ Episode 993/10000, loss: 0.00051565765170380477\n",
      "Episode Reward: 2.0\n",
      "Step 233 (244249) @ Episode 994/10000, loss: 0.00028617313364520674\n",
      "Episode Reward: 1.0\n",
      "Step 238 (244487) @ Episode 995/10000, loss: 0.00150222913362085823\n",
      "Episode Reward: 1.0\n",
      "Step 200 (244687) @ Episode 996/10000, loss: 0.00024077917623799294\n",
      "Episode Reward: 0.0\n",
      "Step 174 (244861) @ Episode 997/10000, loss: 0.00022061515483073899\n",
      "Episode Reward: 0.0\n",
      "Step 315 (245176) @ Episode 998/10000, loss: 0.00071053334977477795\n",
      "Episode Reward: 2.0\n",
      "Step 235 (245411) @ Episode 999/10000, loss: 0.00062151747988536956\n",
      "Episode Reward: 1.0\n",
      "Step 324 (245735) @ Episode 1000/10000, loss: 0.00067993334960192447\n",
      "Episode Reward: 3.0\n",
      "Step 237 (245972) @ Episode 1001/10000, loss: 0.00187905912753194574\n",
      "Episode Reward: 1.0\n",
      "Step 201 (246173) @ Episode 1002/10000, loss: 0.00115967565216124062\n",
      "Episode Reward: 1.0\n",
      "Step 305 (246478) @ Episode 1003/10000, loss: 0.00072104792343452577\n",
      "Episode Reward: 2.0\n",
      "Step 195 (246673) @ Episode 1004/10000, loss: 0.00071955128805711873\n",
      "Episode Reward: 0.0\n",
      "Step 249 (246922) @ Episode 1005/10000, loss: 0.00360077247023582468\n",
      "Episode Reward: 2.0\n",
      "Step 184 (247106) @ Episode 1006/10000, loss: 0.00024804036365821965\n",
      "Episode Reward: 0.0\n",
      "Step 373 (247479) @ Episode 1007/10000, loss: 0.00026261148741468787\n",
      "Episode Reward: 3.0\n",
      "Step 279 (247758) @ Episode 1008/10000, loss: 0.00140279042534530165\n",
      "Episode Reward: 1.0\n",
      "Step 298 (248056) @ Episode 1009/10000, loss: 0.00032020625076256696\n",
      "Episode Reward: 2.0\n",
      "Step 260 (248316) @ Episode 1010/10000, loss: 6.733614281984046e-058\n",
      "Episode Reward: 2.0\n",
      "Step 207 (248523) @ Episode 1011/10000, loss: 0.00050833204295486215\n",
      "Episode Reward: 1.0\n",
      "Step 187 (248710) @ Episode 1012/10000, loss: 0.00248783756978809831\n",
      "Episode Reward: 0.0\n",
      "Step 223 (248933) @ Episode 1013/10000, loss: 0.00027766599669121206\n",
      "Episode Reward: 1.0\n",
      "Step 205 (249138) @ Episode 1014/10000, loss: 0.00038114254130050544\n",
      "Episode Reward: 0.0\n",
      "Step 246 (249384) @ Episode 1015/10000, loss: 0.00016673021309543404\n",
      "Episode Reward: 1.0\n",
      "Step 268 (249652) @ Episode 1016/10000, loss: 0.00011719464964699632\n",
      "Episode Reward: 2.0\n",
      "Step 347 (249999) @ Episode 1017/10000, loss: 0.00085382658289745457\n",
      "Copied model parameters to target network.\n",
      "Step 381 (250033) @ Episode 1017/10000, loss: 0.00420960551127791495\n",
      "Episode Reward: 4.0\n",
      "Step 177 (250210) @ Episode 1018/10000, loss: 0.00301856594160199174\n",
      "Episode Reward: 0.0\n",
      "Step 358 (250568) @ Episode 1019/10000, loss: 0.00025327329058200128\n",
      "Episode Reward: 3.0\n",
      "Step 339 (250907) @ Episode 1020/10000, loss: 0.00049245177069678966\n",
      "Episode Reward: 3.0\n",
      "Step 307 (251214) @ Episode 1021/10000, loss: 0.00120954343583434823\n",
      "Episode Reward: 2.0\n",
      "Step 280 (251494) @ Episode 1022/10000, loss: 0.00149651058018207554\n",
      "Episode Reward: 2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 268 (251762) @ Episode 1023/10000, loss: 0.00275996490381658086\n",
      "Episode Reward: 2.0\n",
      "Step 179 (251941) @ Episode 1024/10000, loss: 0.00031239696545526385\n",
      "Episode Reward: 0.0\n",
      "Step 281 (252222) @ Episode 1025/10000, loss: 0.00023202516604214907\n",
      "Episode Reward: 2.0\n",
      "Step 312 (252534) @ Episode 1026/10000, loss: 0.00036690002889372415\n",
      "Episode Reward: 3.0\n",
      "Step 283 (252817) @ Episode 1027/10000, loss: 0.00069324637297540994\n",
      "Episode Reward: 2.0\n",
      "Step 431 (253248) @ Episode 1028/10000, loss: 0.00087865849491208791\n",
      "Episode Reward: 5.0\n",
      "Step 366 (253614) @ Episode 1029/10000, loss: 0.00224391324445605286\n",
      "Episode Reward: 4.0\n",
      "Step 182 (253796) @ Episode 1030/10000, loss: 0.00197225459851324562\n",
      "Episode Reward: 0.0\n",
      "Step 172 (253968) @ Episode 1031/10000, loss: 0.00223352340981364253\n",
      "Episode Reward: 0.0\n",
      "Step 172 (254140) @ Episode 1032/10000, loss: 0.00049840268911793836\n",
      "Episode Reward: 0.0\n",
      "Step 177 (254317) @ Episode 1033/10000, loss: 0.00082030985504388816\n",
      "Episode Reward: 0.0\n",
      "Step 176 (254493) @ Episode 1034/10000, loss: 0.00285608367994427727\n",
      "Episode Reward: 0.0\n",
      "Step 213 (254706) @ Episode 1035/10000, loss: 0.08091501891613007975\n",
      "Episode Reward: 1.0\n",
      "Step 197 (254903) @ Episode 1036/10000, loss: 0.00295652775093913088\n",
      "Episode Reward: 0.0\n",
      "Step 411 (255314) @ Episode 1037/10000, loss: 0.00061013887170702227\n",
      "Episode Reward: 4.0\n",
      "Step 247 (255561) @ Episode 1038/10000, loss: 0.00092102040071040393\n",
      "Episode Reward: 1.0\n",
      "Step 173 (255734) @ Episode 1039/10000, loss: 0.00020273067639209334\n",
      "Episode Reward: 0.0\n",
      "Step 198 (255932) @ Episode 1040/10000, loss: 0.00041792370029725134\n",
      "Episode Reward: 0.0\n",
      "Step 261 (256193) @ Episode 1041/10000, loss: 0.00027438363758847124\n",
      "Episode Reward: 1.0\n",
      "Step 167 (256360) @ Episode 1042/10000, loss: 0.00025692419148981577\n",
      "Episode Reward: 0.0\n",
      "Step 295 (256655) @ Episode 1043/10000, loss: 0.00029960382380522797\n",
      "Episode Reward: 2.0\n",
      "Step 165 (256820) @ Episode 1044/10000, loss: 0.00079871556954458364\n",
      "Episode Reward: 0.0\n",
      "Step 224 (257044) @ Episode 1045/10000, loss: 0.00052639021305367353\n",
      "Episode Reward: 1.0\n",
      "Step 373 (257417) @ Episode 1046/10000, loss: 0.00090806139633059547\n",
      "Episode Reward: 4.0\n",
      "Step 186 (257603) @ Episode 1047/10000, loss: 0.00016106913972180337\n",
      "Episode Reward: 0.0\n",
      "Step 179 (257782) @ Episode 1048/10000, loss: 0.00027656304882839328\n",
      "Episode Reward: 0.0\n",
      "Step 386 (258168) @ Episode 1049/10000, loss: 0.00032548414310440421\n",
      "Episode Reward: 4.0\n",
      "Step 248 (258416) @ Episode 1050/10000, loss: 0.00138896063435822735\n",
      "Episode Reward: 1.0\n",
      "Step 217 (258633) @ Episode 1051/10000, loss: 0.00088069617049768571\n",
      "Episode Reward: 1.0\n",
      "Step 367 (259000) @ Episode 1052/10000, loss: 8.659190643811598e-057\n",
      "Episode Reward: 3.0\n",
      "Step 305 (259305) @ Episode 1053/10000, loss: 0.00099814590066671378\n",
      "Episode Reward: 2.0\n",
      "Step 228 (259533) @ Episode 1054/10000, loss: 0.00109906401485204767\n",
      "Episode Reward: 1.0\n",
      "Step 233 (259766) @ Episode 1055/10000, loss: 0.00025643751723691821\n",
      "Episode Reward: 1.0\n",
      "Step 233 (259999) @ Episode 1056/10000, loss: 0.00316506298258900648\n",
      "Copied model parameters to target network.\n",
      "Step 317 (260083) @ Episode 1056/10000, loss: 0.00027836271328851587\n",
      "Episode Reward: 2.0\n",
      "Step 178 (260261) @ Episode 1057/10000, loss: 0.00339229404926300054\n",
      "Episode Reward: 0.0\n",
      "Step 208 (260469) @ Episode 1058/10000, loss: 0.00064183131325989961\n",
      "Episode Reward: 1.0\n",
      "Step 344 (260813) @ Episode 1059/10000, loss: 0.00021643331274390228\n",
      "Episode Reward: 3.0\n",
      "Step 324 (261137) @ Episode 1060/10000, loss: 0.00035655326792038977\n",
      "Episode Reward: 3.0\n",
      "Step 183 (261320) @ Episode 1061/10000, loss: 0.00014398615167010576\n",
      "Episode Reward: 0.0\n",
      "Step 394 (261714) @ Episode 1062/10000, loss: 0.00043044443009421234\n",
      "Episode Reward: 4.0\n",
      "Step 176 (261890) @ Episode 1063/10000, loss: 0.00085191678954288363\n",
      "Episode Reward: 0.0\n",
      "Step 245 (262135) @ Episode 1064/10000, loss: 0.01106151007115840904\n",
      "Episode Reward: 1.0\n",
      "Step 304 (262439) @ Episode 1065/10000, loss: 0.00020053303160239018\n",
      "Episode Reward: 3.0\n",
      "Step 248 (262687) @ Episode 1066/10000, loss: 0.00060858577489852975\n",
      "Episode Reward: 1.0\n",
      "Step 268 (262955) @ Episode 1067/10000, loss: 0.00766440993174910555\n",
      "Episode Reward: 1.0\n",
      "Step 231 (263186) @ Episode 1068/10000, loss: 0.00051548320334404715\n",
      "Episode Reward: 1.0\n",
      "Step 321 (263507) @ Episode 1069/10000, loss: 0.00026043900288641453\n",
      "Episode Reward: 2.0\n",
      "Step 215 (263722) @ Episode 1070/10000, loss: 0.00031721178675070405\n",
      "Episode Reward: 1.0\n",
      "Step 238 (263960) @ Episode 1071/10000, loss: 0.00039448979077860713\n",
      "Episode Reward: 1.0\n",
      "Step 247 (264207) @ Episode 1072/10000, loss: 0.00027751192101277415\n",
      "Episode Reward: 1.0\n",
      "Step 250 (264457) @ Episode 1073/10000, loss: 0.02032313682138924959\n",
      "Episode Reward: 1.0\n",
      "Step 366 (264823) @ Episode 1074/10000, loss: 0.00059607892762869635\n",
      "Episode Reward: 3.0\n",
      "Step 237 (265060) @ Episode 1075/10000, loss: 0.00015882731531746686\n",
      "Episode Reward: 1.0\n",
      "Step 212 (265272) @ Episode 1076/10000, loss: 0.00034345439053140587\n",
      "Episode Reward: 1.0\n",
      "Step 268 (265540) @ Episode 1077/10000, loss: 0.00084962800610810524\n",
      "Episode Reward: 2.0\n",
      "Step 222 (265762) @ Episode 1078/10000, loss: 0.00270898430608212955\n",
      "Episode Reward: 1.0\n",
      "Step 254 (266016) @ Episode 1079/10000, loss: 0.00094077317044138916\n",
      "Episode Reward: 1.0\n",
      "Step 335 (266351) @ Episode 1080/10000, loss: 0.00028370151994749904\n",
      "Episode Reward: 3.0\n",
      "Step 305 (266656) @ Episode 1081/10000, loss: 0.00093308731447905333\n",
      "Episode Reward: 2.0\n",
      "Step 351 (267007) @ Episode 1082/10000, loss: 0.00019405686180107296\n",
      "Episode Reward: 3.0\n",
      "Step 304 (267311) @ Episode 1083/10000, loss: 0.00510977953672409133\n",
      "Episode Reward: 2.0\n",
      "Step 241 (267552) @ Episode 1084/10000, loss: 0.00757878879085183147\n",
      "Episode Reward: 1.0\n",
      "Step 186 (267738) @ Episode 1085/10000, loss: 0.00106108863838016997\n",
      "Episode Reward: 0.0\n",
      "Step 246 (267984) @ Episode 1086/10000, loss: 0.00015818999963812538\n",
      "Episode Reward: 1.0\n",
      "Step 382 (268366) @ Episode 1087/10000, loss: 0.00010855343134608129\n",
      "Episode Reward: 3.0\n",
      "Step 178 (268544) @ Episode 1088/10000, loss: 0.00044195522787049413\n",
      "Episode Reward: 0.0\n",
      "Step 284 (268828) @ Episode 1089/10000, loss: 0.00012432312360033393\n",
      "Episode Reward: 2.0\n",
      "Step 209 (269037) @ Episode 1090/10000, loss: 0.00054043123964220295\n",
      "Episode Reward: 1.0\n",
      "Step 301 (269338) @ Episode 1091/10000, loss: 0.00152004440315067774\n",
      "Episode Reward: 2.0\n",
      "Step 186 (269524) @ Episode 1092/10000, loss: 0.00033638073364272714\n",
      "Episode Reward: 0.0\n",
      "Step 283 (269807) @ Episode 1093/10000, loss: 0.00040659029036760336\n",
      "Episode Reward: 2.0\n",
      "Step 192 (269999) @ Episode 1094/10000, loss: 0.00022215775970835244\n",
      "Copied model parameters to target network.\n",
      "Step 229 (270036) @ Episode 1094/10000, loss: 0.00049505516653880483\n",
      "Episode Reward: 1.0\n",
      "Step 433 (270469) @ Episode 1095/10000, loss: 0.00205280655063688768\n",
      "Episode Reward: 5.0\n",
      "Step 272 (270741) @ Episode 1096/10000, loss: 0.01100825611501932144\n",
      "Episode Reward: 2.0\n",
      "Step 347 (271088) @ Episode 1097/10000, loss: 0.00031067448435351254\n",
      "Episode Reward: 3.0\n",
      "Step 316 (271404) @ Episode 1098/10000, loss: 0.00187131855636835138\n",
      "Episode Reward: 3.0\n",
      "Step 168 (271572) @ Episode 1099/10000, loss: 0.00170363986399024726\n",
      "Episode Reward: 0.0\n",
      "Step 299 (271871) @ Episode 1100/10000, loss: 0.00066942413104698068\n",
      "Episode Reward: 2.0\n",
      "Step 189 (272060) @ Episode 1101/10000, loss: 0.00171912671066820623\n",
      "Episode Reward: 0.0\n",
      "Step 229 (272289) @ Episode 1102/10000, loss: 0.00173971650656312767\n",
      "Episode Reward: 1.0\n",
      "Step 191 (272480) @ Episode 1103/10000, loss: 0.00590876024216413518\n",
      "Episode Reward: 0.0\n",
      "Step 220 (272700) @ Episode 1104/10000, loss: 0.00041344808414578442\n",
      "Episode Reward: 1.0\n",
      "Step 274 (272974) @ Episode 1105/10000, loss: 0.00117067026440054183\n",
      "Episode Reward: 2.0\n",
      "Step 211 (273185) @ Episode 1106/10000, loss: 0.00013262289576232433\n",
      "Episode Reward: 1.0\n",
      "Step 409 (273594) @ Episode 1107/10000, loss: 0.00054288620594888933\n",
      "Episode Reward: 4.0\n",
      "Step 208 (273802) @ Episode 1108/10000, loss: 0.00124945584684610378\n",
      "Episode Reward: 1.0\n",
      "Step 328 (274130) @ Episode 1109/10000, loss: 0.00079382798867300156\n",
      "Episode Reward: 3.0\n",
      "Step 270 (274400) @ Episode 1110/10000, loss: 0.00020336161833256483\n",
      "Episode Reward: 2.0\n",
      "Step 355 (274755) @ Episode 1111/10000, loss: 0.00227651232853531843\n",
      "Episode Reward: 4.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 275 (275030) @ Episode 1112/10000, loss: 0.00043210334843024611\n",
      "Episode Reward: 2.0\n",
      "Step 346 (275376) @ Episode 1113/10000, loss: 0.00052773486822843557\n",
      "Episode Reward: 3.0\n",
      "Step 280 (275656) @ Episode 1114/10000, loss: 0.00023820414207875729\n",
      "Episode Reward: 2.0\n",
      "Step 166 (275822) @ Episode 1115/10000, loss: 0.00106671894900500778\n",
      "Episode Reward: 0.0\n",
      "Step 258 (276080) @ Episode 1116/10000, loss: 0.00274475803598761563\n",
      "Episode Reward: 2.0\n",
      "Step 240 (276320) @ Episode 1117/10000, loss: 0.00142093934118747718\n",
      "Episode Reward: 1.0\n",
      "Step 238 (276558) @ Episode 1118/10000, loss: 0.00034728558966889987\n",
      "Episode Reward: 1.0\n",
      "Step 302 (276860) @ Episode 1119/10000, loss: 0.00036859294050373137\n",
      "Episode Reward: 2.0\n",
      "Step 171 (277031) @ Episode 1120/10000, loss: 0.00083927251398563396\n",
      "Episode Reward: 0.0\n",
      "Step 455 (277486) @ Episode 1121/10000, loss: 0.00055339047685265545\n",
      "Episode Reward: 4.0\n",
      "Step 220 (277706) @ Episode 1122/10000, loss: 0.00038788618985563517\n",
      "Episode Reward: 1.0\n",
      "Step 207 (277913) @ Episode 1123/10000, loss: 0.00036289478885009885\n",
      "Episode Reward: 0.0\n",
      "Step 192 (278105) @ Episode 1124/10000, loss: 0.00175072229467332364\n",
      "Episode Reward: 0.0\n",
      "Step 211 (278316) @ Episode 1125/10000, loss: 8.740513294469565e-054\n",
      "Episode Reward: 1.0\n",
      "Step 196 (278512) @ Episode 1126/10000, loss: 0.00015836543752811856\n",
      "Episode Reward: 0.0\n",
      "Step 240 (278752) @ Episode 1127/10000, loss: 0.00850920565426349683\n",
      "Episode Reward: 1.0\n",
      "Step 231 (278983) @ Episode 1128/10000, loss: 0.00057670020032674077\n",
      "Episode Reward: 1.0\n",
      "Step 171 (279154) @ Episode 1129/10000, loss: 0.00035462583764456217\n",
      "Episode Reward: 0.0\n",
      "Step 310 (279464) @ Episode 1130/10000, loss: 8.078212704276666e-052\n",
      "Episode Reward: 2.0\n",
      "Step 259 (279723) @ Episode 1131/10000, loss: 0.00191373715642839677\n",
      "Episode Reward: 1.0\n",
      "Step 276 (279999) @ Episode 1132/10000, loss: 0.00026296204305253923\n",
      "Copied model parameters to target network.\n",
      "Step 420 (280143) @ Episode 1132/10000, loss: 0.00517809856683015876\n",
      "Episode Reward: 7.0\n",
      "Step 244 (280387) @ Episode 1133/10000, loss: 9.696292545413598e-053\n",
      "Episode Reward: 1.0\n",
      "Step 262 (280649) @ Episode 1134/10000, loss: 0.00021707252017222345\n",
      "Episode Reward: 2.0\n",
      "Step 226 (280875) @ Episode 1135/10000, loss: 0.00125749816652387384\n",
      "Episode Reward: 1.0\n",
      "Step 235 (281110) @ Episode 1136/10000, loss: 0.00136773020494729282\n",
      "Episode Reward: 1.0\n",
      "Step 233 (281343) @ Episode 1137/10000, loss: 0.00092810776550322773\n",
      "Episode Reward: 1.0\n",
      "Step 264 (281607) @ Episode 1138/10000, loss: 0.00022869934036862105\n",
      "Episode Reward: 1.0\n",
      "Step 291 (281898) @ Episode 1139/10000, loss: 0.00293264118954539303\n",
      "Episode Reward: 2.0\n",
      "Step 249 (282147) @ Episode 1140/10000, loss: 0.00060696853324770935\n",
      "Episode Reward: 1.0\n",
      "Step 258 (282405) @ Episode 1141/10000, loss: 0.00094869325403124098\n",
      "Episode Reward: 1.0\n",
      "Step 201 (282606) @ Episode 1142/10000, loss: 0.00092033128021284942\n",
      "Episode Reward: 0.0\n",
      "Step 415 (283021) @ Episode 1143/10000, loss: 0.00021946054766885936\n",
      "Episode Reward: 4.0\n",
      "Step 292 (283313) @ Episode 1144/10000, loss: 0.00437602167949080565\n",
      "Episode Reward: 2.0\n",
      "Step 301 (283614) @ Episode 1145/10000, loss: 0.00055979611352086077\n",
      "Episode Reward: 3.0\n",
      "Step 182 (283796) @ Episode 1146/10000, loss: 0.01691088266670704396\n",
      "Episode Reward: 0.0\n",
      "Step 173 (283969) @ Episode 1147/10000, loss: 0.00076722854282706983\n",
      "Episode Reward: 0.0\n",
      "Step 223 (284192) @ Episode 1148/10000, loss: 0.00026220915606245463\n",
      "Episode Reward: 1.0\n",
      "Step 244 (284436) @ Episode 1149/10000, loss: 0.00074499117908999326\n",
      "Episode Reward: 1.0\n",
      "Step 245 (284681) @ Episode 1150/10000, loss: 0.00049801421118900187\n",
      "Episode Reward: 1.0\n",
      "Step 212 (284893) @ Episode 1151/10000, loss: 0.00515499431639909748\n",
      "Episode Reward: 1.0\n",
      "Step 231 (285124) @ Episode 1152/10000, loss: 0.00213740835897624549\n",
      "Episode Reward: 1.0\n",
      "Step 358 (285482) @ Episode 1153/10000, loss: 0.00072083791019394995\n",
      "Episode Reward: 3.0\n",
      "Step 276 (285758) @ Episode 1154/10000, loss: 0.00256199855357408528\n",
      "Episode Reward: 2.0\n",
      "Step 311 (286069) @ Episode 1155/10000, loss: 0.00042516327812336385\n",
      "Episode Reward: 2.0\n",
      "Step 260 (286329) @ Episode 1156/10000, loss: 0.00543702905997633985\n",
      "Episode Reward: 2.0\n",
      "Step 238 (286567) @ Episode 1157/10000, loss: 0.00071806635241955526\n",
      "Episode Reward: 1.0\n",
      "Step 171 (286738) @ Episode 1158/10000, loss: 0.00032160617411136627\n",
      "Episode Reward: 0.0\n",
      "Step 226 (286964) @ Episode 1159/10000, loss: 0.00067537865834310654\n",
      "Episode Reward: 1.0\n",
      "Step 211 (287175) @ Episode 1160/10000, loss: 0.00043187133269384503\n",
      "Episode Reward: 1.0\n",
      "Step 181 (287356) @ Episode 1161/10000, loss: 0.00068442313931882388\n",
      "Episode Reward: 0.0\n",
      "Step 171 (287527) @ Episode 1162/10000, loss: 0.00075545685831457387\n",
      "Episode Reward: 0.0\n",
      "Step 216 (287743) @ Episode 1163/10000, loss: 0.00024605839280411617\n",
      "Episode Reward: 1.0\n",
      "Step 182 (287925) @ Episode 1164/10000, loss: 0.00126008933875709775\n",
      "Episode Reward: 0.0\n",
      "Step 349 (288274) @ Episode 1165/10000, loss: 0.00084521091775968679\n",
      "Episode Reward: 3.0\n",
      "Step 200 (288474) @ Episode 1166/10000, loss: 0.00015713201719336212\n",
      "Episode Reward: 1.0\n",
      "Step 253 (288727) @ Episode 1167/10000, loss: 0.00045233345008455217\n",
      "Episode Reward: 1.0\n",
      "Step 212 (288939) @ Episode 1168/10000, loss: 0.00039760541403666143\n",
      "Episode Reward: 1.0\n",
      "Step 214 (289153) @ Episode 1169/10000, loss: 0.00032406928949058056\n",
      "Episode Reward: 1.0\n",
      "Step 179 (289332) @ Episode 1170/10000, loss: 0.00534966960549354552\n",
      "Episode Reward: 0.0\n",
      "Step 252 (289584) @ Episode 1171/10000, loss: 0.00015359539247583598\n",
      "Episode Reward: 1.0\n",
      "Step 245 (289829) @ Episode 1172/10000, loss: 0.00096023251535370955\n",
      "Episode Reward: 1.0\n",
      "Step 170 (289999) @ Episode 1173/10000, loss: 0.00309869949705898764\n",
      "Copied model parameters to target network.\n",
      "Step 288 (290117) @ Episode 1173/10000, loss: 0.00408020848408341485\n",
      "Episode Reward: 2.0\n",
      "Step 215 (290332) @ Episode 1174/10000, loss: 0.00430695293471217163\n",
      "Episode Reward: 1.0\n",
      "Step 349 (290681) @ Episode 1175/10000, loss: 0.00228252145461738168\n",
      "Episode Reward: 3.0\n",
      "Step 224 (290905) @ Episode 1176/10000, loss: 0.00017050297174137086\n",
      "Episode Reward: 1.0\n",
      "Step 229 (291134) @ Episode 1177/10000, loss: 0.00255663506686687473\n",
      "Episode Reward: 1.0\n",
      "Step 181 (291315) @ Episode 1178/10000, loss: 0.00026950711617246272\n",
      "Episode Reward: 0.0\n",
      "Step 225 (291540) @ Episode 1179/10000, loss: 0.00015595478180330247\n",
      "Episode Reward: 1.0\n",
      "Step 302 (291842) @ Episode 1180/10000, loss: 0.00069208571221679456\n",
      "Episode Reward: 3.0\n",
      "Step 237 (292079) @ Episode 1181/10000, loss: 0.00013814405247103423\n",
      "Episode Reward: 1.0\n",
      "Step 203 (292282) @ Episode 1182/10000, loss: 0.00492050312459468836\n",
      "Episode Reward: 1.0\n",
      "Step 409 (292691) @ Episode 1183/10000, loss: 0.00392409320920705857\n",
      "Episode Reward: 5.0\n",
      "Step 239 (292930) @ Episode 1184/10000, loss: 0.00032660076976753777\n",
      "Episode Reward: 1.0\n",
      "Step 174 (293104) @ Episode 1185/10000, loss: 0.00082922226283699272\n",
      "Episode Reward: 0.0\n",
      "Step 194 (293298) @ Episode 1186/10000, loss: 0.00308172637596726442\n",
      "Episode Reward: 0.0\n",
      "Step 261 (293559) @ Episode 1187/10000, loss: 0.00119810714386403565\n",
      "Episode Reward: 2.0\n",
      "Step 183 (293742) @ Episode 1188/10000, loss: 0.00042352048330940306\n",
      "Episode Reward: 0.0\n",
      "Step 222 (293964) @ Episode 1189/10000, loss: 0.00105502293445169937\n",
      "Episode Reward: 1.0\n",
      "Step 226 (294190) @ Episode 1190/10000, loss: 0.00102969130966812373\n",
      "Episode Reward: 1.0\n",
      "Step 244 (294434) @ Episode 1191/10000, loss: 0.00838736630976200137\n",
      "Episode Reward: 2.0\n",
      "Step 317 (294751) @ Episode 1192/10000, loss: 0.00115823152009397753\n",
      "Episode Reward: 2.0\n",
      "Step 232 (294983) @ Episode 1193/10000, loss: 0.01388463657349348873\n",
      "Episode Reward: 1.0\n",
      "Step 365 (295348) @ Episode 1194/10000, loss: 0.00082414131611585623\n",
      "Episode Reward: 4.0\n",
      "Step 177 (295525) @ Episode 1195/10000, loss: 0.00038114655762910843\n",
      "Episode Reward: 0.0\n",
      "Step 363 (295888) @ Episode 1196/10000, loss: 0.00025917688617482786\n",
      "Episode Reward: 4.0\n",
      "Step 222 (296110) @ Episode 1197/10000, loss: 0.00361587037332355983\n",
      "Episode Reward: 1.0\n",
      "Step 184 (296294) @ Episode 1198/10000, loss: 0.00160460430197417745\n",
      "Episode Reward: 0.0\n",
      "Step 348 (296642) @ Episode 1199/10000, loss: 0.00025189659208990633\n",
      "Episode Reward: 3.0\n",
      "Step 246 (296888) @ Episode 1200/10000, loss: 0.00270655448548495777\n",
      "Episode Reward: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 168 (297056) @ Episode 1201/10000, loss: 0.00050581950927153233\n",
      "Episode Reward: 0.0\n",
      "Step 292 (297348) @ Episode 1202/10000, loss: 0.00055089499801397324\n",
      "Episode Reward: 2.0\n",
      "Step 476 (297824) @ Episode 1203/10000, loss: 0.00336534460075199615\n",
      "Episode Reward: 6.0\n",
      "Step 219 (298043) @ Episode 1204/10000, loss: 0.00057703565107658515\n",
      "Episode Reward: 1.0\n",
      "Step 235 (298278) @ Episode 1205/10000, loss: 0.00153973163105547433\n",
      "Episode Reward: 1.0\n",
      "Step 210 (298488) @ Episode 1206/10000, loss: 0.00150111899711191656\n",
      "Episode Reward: 1.0\n",
      "Step 234 (298722) @ Episode 1207/10000, loss: 0.00131562445312738427\n",
      "Episode Reward: 1.0\n",
      "Step 242 (298964) @ Episode 1208/10000, loss: 0.00196451577357947834\n",
      "Episode Reward: 1.0\n",
      "Step 184 (299148) @ Episode 1209/10000, loss: 0.00020475871860980988\n",
      "Episode Reward: 0.0\n",
      "Step 205 (299353) @ Episode 1210/10000, loss: 0.00072840991197153936\n",
      "Episode Reward: 0.0\n",
      "Step 296 (299649) @ Episode 1211/10000, loss: 0.00067489605862647336\n",
      "Episode Reward: 2.0\n",
      "Step 262 (299911) @ Episode 1212/10000, loss: 0.01067124400287866694\n",
      "Episode Reward: 2.0\n",
      "Step 88 (299999) @ Episode 1213/10000, loss: 0.00037411513039842257\n",
      "Copied model parameters to target network.\n",
      "Step 210 (300121) @ Episode 1213/10000, loss: 0.00117254874203354125\n",
      "Episode Reward: 1.0\n",
      "Step 224 (300345) @ Episode 1214/10000, loss: 0.00020568729087244724\n",
      "Episode Reward: 1.0\n",
      "Step 240 (300585) @ Episode 1215/10000, loss: 0.00021304641268216074\n",
      "Episode Reward: 1.0\n",
      "Step 351 (300936) @ Episode 1216/10000, loss: 0.00010728853521868587\n",
      "Episode Reward: 4.0\n",
      "Step 398 (301334) @ Episode 1217/10000, loss: 0.00061408913461491477\n",
      "Episode Reward: 4.0\n",
      "Step 324 (301658) @ Episode 1218/10000, loss: 0.00337144127115607265\n",
      "Episode Reward: 3.0\n",
      "Step 282 (301940) @ Episode 1219/10000, loss: 0.00277728517539799276\n",
      "Episode Reward: 2.0\n",
      "Step 465 (302405) @ Episode 1220/10000, loss: 0.00064749002922326336\n",
      "Episode Reward: 6.0\n",
      "Step 258 (302663) @ Episode 1221/10000, loss: 0.00017162406584247947\n",
      "Episode Reward: 2.0\n",
      "Step 220 (302883) @ Episode 1222/10000, loss: 0.00066241127206012611\n",
      "Episode Reward: 1.0\n",
      "Step 470 (303353) @ Episode 1223/10000, loss: 0.00020774532458744943\n",
      "Episode Reward: 5.0\n",
      "Step 465 (303818) @ Episode 1224/10000, loss: 0.00077771552605554466\n",
      "Episode Reward: 5.0\n",
      "Step 229 (304047) @ Episode 1225/10000, loss: 0.00027012109057977796\n",
      "Episode Reward: 1.0\n",
      "Step 171 (304218) @ Episode 1226/10000, loss: 0.00045803026296198375\n",
      "Episode Reward: 0.0\n",
      "Step 245 (304463) @ Episode 1227/10000, loss: 0.00057022331748157744\n",
      "Episode Reward: 1.0\n",
      "Step 253 (304716) @ Episode 1228/10000, loss: 0.00071363139431923637\n",
      "Episode Reward: 1.0\n",
      "Step 163 (304879) @ Episode 1229/10000, loss: 0.00023071040050126612\n",
      "Episode Reward: 0.0\n",
      "Step 230 (305109) @ Episode 1230/10000, loss: 0.00156635383609682325\n",
      "Episode Reward: 1.0\n",
      "Step 279 (305388) @ Episode 1231/10000, loss: 0.00090557831572368744\n",
      "Episode Reward: 2.0\n",
      "Step 422 (305810) @ Episode 1232/10000, loss: 0.00037941223126836124\n",
      "Episode Reward: 4.0\n",
      "Step 285 (306095) @ Episode 1233/10000, loss: 0.00143870001193135983\n",
      "Episode Reward: 2.0\n",
      "Step 172 (306267) @ Episode 1234/10000, loss: 0.00050192367052659398\n",
      "Episode Reward: 0.0\n",
      "Step 278 (306545) @ Episode 1235/10000, loss: 0.00019068970868829638\n",
      "Episode Reward: 2.0\n",
      "Step 236 (306781) @ Episode 1236/10000, loss: 0.00017069997556973256\n",
      "Episode Reward: 1.0\n",
      "Step 286 (307067) @ Episode 1237/10000, loss: 0.00022696121595799923\n",
      "Episode Reward: 2.0\n",
      "Step 263 (307330) @ Episode 1238/10000, loss: 0.00080232258187606936\n",
      "Episode Reward: 2.0\n",
      "Step 255 (307585) @ Episode 1239/10000, loss: 0.01534213311970234133\n",
      "Episode Reward: 1.0\n",
      "Step 397 (307982) @ Episode 1240/10000, loss: 0.00047829054528847337\n",
      "Episode Reward: 4.0\n",
      "Step 270 (308252) @ Episode 1241/10000, loss: 0.00111125828698277478\n",
      "Episode Reward: 2.0\n",
      "Step 239 (308491) @ Episode 1242/10000, loss: 0.00520046800374984794\n",
      "Episode Reward: 1.0\n",
      "Step 360 (308851) @ Episode 1243/10000, loss: 0.00021576687868218876\n",
      "Episode Reward: 4.0\n",
      "Step 174 (309025) @ Episode 1244/10000, loss: 0.00050278956769034277\n",
      "Episode Reward: 0.0\n",
      "Step 338 (309363) @ Episode 1245/10000, loss: 0.00073719053762033585\n",
      "Episode Reward: 3.0\n",
      "Step 389 (309752) @ Episode 1246/10000, loss: 0.00194868200924247525\n",
      "Episode Reward: 3.0\n",
      "Step 247 (309999) @ Episode 1247/10000, loss: 0.00023113959468901157\n",
      "Copied model parameters to target network.\n",
      "Step 442 (310194) @ Episode 1247/10000, loss: 0.00553363794460892722\n",
      "Episode Reward: 5.0\n",
      "Step 534 (310728) @ Episode 1248/10000, loss: 0.00148300640285015178\n",
      "Episode Reward: 6.0\n",
      "Step 227 (310955) @ Episode 1249/10000, loss: 0.00044881139183416966\n",
      "Episode Reward: 1.0\n",
      "Step 434 (311389) @ Episode 1250/10000, loss: 0.00093601772096008064\n",
      "Episode Reward: 4.0\n",
      "Step 331 (311720) @ Episode 1251/10000, loss: 0.00063097814563661817\n",
      "Episode Reward: 3.0\n",
      "Step 258 (311978) @ Episode 1252/10000, loss: 0.00108001206535846157\n",
      "Episode Reward: 1.0\n",
      "Step 251 (312229) @ Episode 1253/10000, loss: 0.00037784498999826615\n",
      "Episode Reward: 1.0\n",
      "Step 308 (312537) @ Episode 1254/10000, loss: 0.00024048867635428905\n",
      "Episode Reward: 2.0\n",
      "Step 293 (312830) @ Episode 1255/10000, loss: 0.00532739842310547856\n",
      "Episode Reward: 2.0\n",
      "Step 192 (313022) @ Episode 1256/10000, loss: 0.00471629807725548749\n",
      "Episode Reward: 0.0\n",
      "Step 334 (313356) @ Episode 1257/10000, loss: 0.00030013173818588257\n",
      "Episode Reward: 2.0\n",
      "Step 376 (313732) @ Episode 1258/10000, loss: 0.00074838323052972565\n",
      "Episode Reward: 3.0\n",
      "Step 315 (314047) @ Episode 1259/10000, loss: 0.00120309053454548123\n",
      "Episode Reward: 3.0\n",
      "Step 359 (314406) @ Episode 1260/10000, loss: 0.00089712598128244282\n",
      "Episode Reward: 3.0\n",
      "Step 472 (314878) @ Episode 1261/10000, loss: 0.00033940537832677364\n",
      "Episode Reward: 5.0\n",
      "Step 254 (315132) @ Episode 1262/10000, loss: 0.00041759927989915013\n",
      "Episode Reward: 1.0\n",
      "Step 257 (315389) @ Episode 1263/10000, loss: 0.00084214343223720795\n",
      "Episode Reward: 1.0\n",
      "Step 354 (315743) @ Episode 1264/10000, loss: 0.00507090892642736435\n",
      "Episode Reward: 3.0\n",
      "Step 316 (316059) @ Episode 1265/10000, loss: 0.00144423544406890875\n",
      "Episode Reward: 2.0\n",
      "Step 409 (316468) @ Episode 1266/10000, loss: 0.00051933148643001916\n",
      "Episode Reward: 4.0\n",
      "Step 246 (316714) @ Episode 1267/10000, loss: 0.00016560766380280256\n",
      "Episode Reward: 2.0\n",
      "Step 461 (317175) @ Episode 1268/10000, loss: 0.00032875512260943656\n",
      "Episode Reward: 8.0\n",
      "Step 291 (317466) @ Episode 1269/10000, loss: 0.00026692455867305438\n",
      "Episode Reward: 2.0\n",
      "Step 257 (317723) @ Episode 1270/10000, loss: 0.00394183536991477277\n",
      "Episode Reward: 1.0\n",
      "Step 295 (318018) @ Episode 1271/10000, loss: 0.00042670825496315956\n",
      "Episode Reward: 2.0\n",
      "Step 304 (318322) @ Episode 1272/10000, loss: 0.00085972360102459794\n",
      "Episode Reward: 2.0\n",
      "Step 380 (318702) @ Episode 1273/10000, loss: 0.00207542954012751665\n",
      "Episode Reward: 3.0\n",
      "Step 435 (319137) @ Episode 1274/10000, loss: 0.00388346984982490543\n",
      "Episode Reward: 5.0\n",
      "Step 215 (319352) @ Episode 1275/10000, loss: 0.00847565289586782569\n",
      "Episode Reward: 1.0\n",
      "Step 360 (319712) @ Episode 1276/10000, loss: 0.00130818993784487256\n",
      "Episode Reward: 3.0\n",
      "Step 287 (319999) @ Episode 1277/10000, loss: 0.00015545119822490964\n",
      "Copied model parameters to target network.\n",
      "Step 467 (320179) @ Episode 1277/10000, loss: 0.00067999336170032627\n",
      "Episode Reward: 5.0\n",
      "Step 503 (320682) @ Episode 1278/10000, loss: 0.00018391056801192462\n",
      "Episode Reward: 6.0\n",
      "Step 333 (321015) @ Episode 1279/10000, loss: 0.00736659998074173954\n",
      "Episode Reward: 3.0\n",
      "Step 291 (321306) @ Episode 1280/10000, loss: 0.00068632548209279783\n",
      "Episode Reward: 2.0\n",
      "Step 292 (321598) @ Episode 1281/10000, loss: 0.00030342733953148127\n",
      "Episode Reward: 2.0\n",
      "Step 428 (322026) @ Episode 1282/10000, loss: 0.00068843236658722163\n",
      "Episode Reward: 4.0\n",
      "Step 440 (322466) @ Episode 1283/10000, loss: 0.00040507828816771507\n",
      "Episode Reward: 4.0\n",
      "Step 476 (322942) @ Episode 1284/10000, loss: 0.00075384683441370735\n",
      "Episode Reward: 6.0\n",
      "Step 494 (323436) @ Episode 1285/10000, loss: 0.00038197386311367154\n",
      "Episode Reward: 5.0\n",
      "Step 234 (323670) @ Episode 1286/10000, loss: 0.00016802211757749324\n",
      "Episode Reward: 1.0\n",
      "Step 409 (324079) @ Episode 1287/10000, loss: 0.00026495181373320523\n",
      "Episode Reward: 5.0\n",
      "Step 306 (324385) @ Episode 1288/10000, loss: 0.00081756821600720297\n",
      "Episode Reward: 2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 261 (324646) @ Episode 1289/10000, loss: 0.00039006996667012575\n",
      "Episode Reward: 2.0\n",
      "Step 210 (324856) @ Episode 1290/10000, loss: 0.00039337552152574065\n",
      "Episode Reward: 1.0\n",
      "Step 227 (325083) @ Episode 1291/10000, loss: 0.00133958575315773494\n",
      "Episode Reward: 1.0\n",
      "Step 492 (325575) @ Episode 1292/10000, loss: 0.00169402849860489376\n",
      "Episode Reward: 6.0\n",
      "Step 407 (325982) @ Episode 1293/10000, loss: 0.00173605850432068106\n",
      "Episode Reward: 4.0\n",
      "Step 311 (326293) @ Episode 1294/10000, loss: 0.00029857453773729503\n",
      "Episode Reward: 2.0\n",
      "Step 413 (326706) @ Episode 1295/10000, loss: 0.00030171175603754823\n",
      "Episode Reward: 4.0\n",
      "Step 329 (327035) @ Episode 1296/10000, loss: 0.00148189277388155461\n",
      "Episode Reward: 2.0\n",
      "Step 354 (327389) @ Episode 1297/10000, loss: 0.00356012629345059438\n",
      "Episode Reward: 4.0\n",
      "Step 488 (327877) @ Episode 1298/10000, loss: 0.00027644555666483945\n",
      "Episode Reward: 5.0\n",
      "Step 265 (328142) @ Episode 1299/10000, loss: 0.00179421622306108476\n",
      "Episode Reward: 2.0\n",
      "Step 480 (328622) @ Episode 1300/10000, loss: 0.00063151202630251653\n",
      "Episode Reward: 6.0\n",
      "Step 504 (329126) @ Episode 1301/10000, loss: 0.00082877266686409717\n",
      "Episode Reward: 5.0\n",
      "Step 401 (329527) @ Episode 1302/10000, loss: 0.00016625682474114765\n",
      "Episode Reward: 3.0\n",
      "Step 470 (329997) @ Episode 1303/10000, loss: 0.00239827879704535774\n",
      "Episode Reward: 5.0\n",
      "Step 2 (329999) @ Episode 1304/10000, loss: 0.0011488597374409437\n",
      "Copied model parameters to target network.\n",
      "Step 330 (330327) @ Episode 1304/10000, loss: 0.00287374621257185942\n",
      "Episode Reward: 3.0\n",
      "Step 352 (330679) @ Episode 1305/10000, loss: 0.00692001963034272262\n",
      "Episode Reward: 3.0\n",
      "Step 511 (331190) @ Episode 1306/10000, loss: 0.00071866210782900453\n",
      "Episode Reward: 8.0\n",
      "Step 407 (331597) @ Episode 1307/10000, loss: 0.00402862858027219863\n",
      "Episode Reward: 4.0\n",
      "Step 598 (332195) @ Episode 1308/10000, loss: 0.00035444812965579336\n",
      "Episode Reward: 7.0\n",
      "Step 239 (332434) @ Episode 1309/10000, loss: 0.00060475646751001485\n",
      "Episode Reward: 1.0\n",
      "Step 495 (332929) @ Episode 1310/10000, loss: 0.00450328644365072258\n",
      "Episode Reward: 5.0\n",
      "Step 435 (333364) @ Episode 1311/10000, loss: 0.00043385909521020957\n",
      "Episode Reward: 5.0\n",
      "Step 511 (333875) @ Episode 1312/10000, loss: 0.00051251775585114966\n",
      "Episode Reward: 6.0\n",
      "Step 401 (334276) @ Episode 1313/10000, loss: 0.00062302115838974718\n",
      "Episode Reward: 4.0\n",
      "Step 357 (334633) @ Episode 1314/10000, loss: 0.00066700036404654388\n",
      "Episode Reward: 3.0\n",
      "Step 725 (335358) @ Episode 1315/10000, loss: 0.00047806807560846216\n",
      "Episode Reward: 12.0\n",
      "Step 493 (335851) @ Episode 1316/10000, loss: 0.00044725887710228567\n",
      "Episode Reward: 5.0\n",
      "Step 492 (336343) @ Episode 1317/10000, loss: 0.00059660273836925635\n",
      "Episode Reward: 6.0\n",
      "Step 400 (336743) @ Episode 1318/10000, loss: 0.00616426859050989154\n",
      "Episode Reward: 4.0\n",
      "Step 427 (337170) @ Episode 1319/10000, loss: 0.00075546745210886352\n",
      "Episode Reward: 4.0\n",
      "Step 315 (337485) @ Episode 1320/10000, loss: 0.00176601554267108444\n",
      "Episode Reward: 3.0\n",
      "Step 397 (337882) @ Episode 1321/10000, loss: 0.00014034318155609076\n",
      "Episode Reward: 4.0\n",
      "Step 290 (338172) @ Episode 1322/10000, loss: 0.00036026316229254013\n",
      "Episode Reward: 3.0\n",
      "Step 494 (338666) @ Episode 1323/10000, loss: 0.00077227351721376186\n",
      "Episode Reward: 6.0\n",
      "Step 395 (339061) @ Episode 1324/10000, loss: 0.00162425753660500052\n",
      "Episode Reward: 4.0\n",
      "Step 480 (339541) @ Episode 1325/10000, loss: 0.00138002936728298667\n",
      "Episode Reward: 5.0\n",
      "Step 335 (339876) @ Episode 1326/10000, loss: 0.00279263174161314962\n",
      "Episode Reward: 3.0\n",
      "Step 123 (339999) @ Episode 1327/10000, loss: 0.00046987630776129663\n",
      "Copied model parameters to target network.\n",
      "Step 468 (340344) @ Episode 1327/10000, loss: 0.00062330934451892974\n",
      "Episode Reward: 6.0\n",
      "Step 362 (340706) @ Episode 1328/10000, loss: 0.00024157107691280544\n",
      "Episode Reward: 3.0\n",
      "Step 237 (340943) @ Episode 1329/10000, loss: 0.00064074841793626553\n",
      "Episode Reward: 1.0\n",
      "Step 451 (341394) @ Episode 1330/10000, loss: 0.00031317840330302715\n",
      "Episode Reward: 5.0\n",
      "Step 452 (341846) @ Episode 1331/10000, loss: 0.00111589010339230375\n",
      "Episode Reward: 4.0\n",
      "Step 380 (342226) @ Episode 1332/10000, loss: 0.00039503202424384654\n",
      "Episode Reward: 4.0\n",
      "Step 365 (342591) @ Episode 1333/10000, loss: 0.00454778177663683935\n",
      "Episode Reward: 3.0\n",
      "Step 487 (343078) @ Episode 1334/10000, loss: 0.00325494748540222646\n",
      "Episode Reward: 6.0\n",
      "Step 447 (343525) @ Episode 1335/10000, loss: 0.00093166495207697155\n",
      "Episode Reward: 5.0\n",
      "Step 509 (344034) @ Episode 1336/10000, loss: 0.00370598514564335355\n",
      "Episode Reward: 5.0\n",
      "Step 440 (344474) @ Episode 1337/10000, loss: 0.00063189183128997684\n",
      "Episode Reward: 5.0\n",
      "Step 390 (344864) @ Episode 1338/10000, loss: 0.00092047301586717378\n",
      "Episode Reward: 4.0\n",
      "Step 300 (345164) @ Episode 1339/10000, loss: 0.00359854055568575864\n",
      "Episode Reward: 2.0\n",
      "Step 496 (345660) @ Episode 1340/10000, loss: 0.00706730876117944755\n",
      "Episode Reward: 5.0\n",
      "Step 498 (346158) @ Episode 1341/10000, loss: 0.00046182758524082666\n",
      "Episode Reward: 5.0\n",
      "Step 295 (346453) @ Episode 1342/10000, loss: 0.00022454431746155024\n",
      "Episode Reward: 2.0\n",
      "Step 372 (346825) @ Episode 1343/10000, loss: 0.00034122506622225046\n",
      "Episode Reward: 4.0\n",
      "Step 403 (347228) @ Episode 1344/10000, loss: 0.00137893285136669873\n",
      "Episode Reward: 4.0\n",
      "Step 449 (347677) @ Episode 1345/10000, loss: 0.00312881357967853554\n",
      "Episode Reward: 5.0\n",
      "Step 450 (348127) @ Episode 1346/10000, loss: 0.00040130069828592247\n",
      "Episode Reward: 4.0\n",
      "Step 456 (348583) @ Episode 1347/10000, loss: 0.00074686564039438965\n",
      "Episode Reward: 5.0\n",
      "Step 362 (348945) @ Episode 1348/10000, loss: 0.00295166717842221262\n",
      "Episode Reward: 4.0\n",
      "Step 476 (349421) @ Episode 1349/10000, loss: 0.00130069360602647077\n",
      "Episode Reward: 5.0\n",
      "Step 430 (349851) @ Episode 1350/10000, loss: 0.00262047932483255865\n",
      "Episode Reward: 5.0\n",
      "Step 148 (349999) @ Episode 1351/10000, loss: 0.00372188957408070564\n",
      "Copied model parameters to target network.\n",
      "Step 508 (350359) @ Episode 1351/10000, loss: 0.01706584729254245887\n",
      "Episode Reward: 5.0\n",
      "Step 290 (350649) @ Episode 1352/10000, loss: 0.00065294327214360246\n",
      "Episode Reward: 3.0\n",
      "Step 586 (351235) @ Episode 1353/10000, loss: 0.00084635673556476836\n",
      "Episode Reward: 7.0\n",
      "Step 528 (351763) @ Episode 1354/10000, loss: 0.00159567478112876426\n",
      "Episode Reward: 6.0\n",
      "Step 271 (352034) @ Episode 1355/10000, loss: 0.00771591207012534127\n",
      "Episode Reward: 2.0\n",
      "Step 496 (352530) @ Episode 1356/10000, loss: 0.00050293782260268934\n",
      "Episode Reward: 6.0\n",
      "Step 492 (353022) @ Episode 1357/10000, loss: 0.00451178243383765293\n",
      "Episode Reward: 5.0\n",
      "Step 385 (353407) @ Episode 1358/10000, loss: 0.00070900318678468477\n",
      "Episode Reward: 4.0\n",
      "Step 452 (353859) @ Episode 1359/10000, loss: 0.00067906093318015346\n",
      "Episode Reward: 9.0\n",
      "Step 504 (354363) @ Episode 1360/10000, loss: 0.00271768355742096937\n",
      "Episode Reward: 5.0\n",
      "Step 263 (354626) @ Episode 1361/10000, loss: 0.00025439940509386364\n",
      "Episode Reward: 2.0\n",
      "Step 532 (355158) @ Episode 1362/10000, loss: 0.01298415381461381956\n",
      "Episode Reward: 7.0\n",
      "Step 486 (355644) @ Episode 1363/10000, loss: 0.00397735508158803535\n",
      "Episode Reward: 5.0\n",
      "Step 370 (356014) @ Episode 1364/10000, loss: 0.01193708367645740534\n",
      "Episode Reward: 3.0\n",
      "Step 268 (356282) @ Episode 1365/10000, loss: 0.00133502378594130283\n",
      "Episode Reward: 2.0\n",
      "Step 331 (356613) @ Episode 1366/10000, loss: 0.00281592202372849657\n",
      "Episode Reward: 3.0\n",
      "Step 542 (357155) @ Episode 1367/10000, loss: 0.00070754135958850383\n",
      "Episode Reward: 7.0\n",
      "Step 355 (357510) @ Episode 1368/10000, loss: 0.00146005605347454555\n",
      "Episode Reward: 3.0\n",
      "Step 439 (357949) @ Episode 1369/10000, loss: 0.00075795559678226716\n",
      "Episode Reward: 4.0\n",
      "Step 517 (358466) @ Episode 1370/10000, loss: 0.00060281099285930423\n",
      "Episode Reward: 6.0\n",
      "Step 328 (358794) @ Episode 1371/10000, loss: 0.00134539348073303797\n",
      "Episode Reward: 3.0\n",
      "Step 536 (359330) @ Episode 1372/10000, loss: 0.00054336071480065586\n",
      "Episode Reward: 6.0\n",
      "Step 342 (359672) @ Episode 1373/10000, loss: 0.00493901781737804403\n",
      "Episode Reward: 3.0\n",
      "Step 251 (359923) @ Episode 1374/10000, loss: 0.01614287123084068397\n",
      "Episode Reward: 2.0\n",
      "Step 76 (359999) @ Episode 1375/10000, loss: 0.00904776714742183768\n",
      "Copied model parameters to target network.\n",
      "Step 743 (360666) @ Episode 1375/10000, loss: 0.00108001288026571277\n",
      "Episode Reward: 13.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 411 (361077) @ Episode 1376/10000, loss: 0.00057770393323153266\n",
      "Episode Reward: 5.0\n",
      "Step 384 (361461) @ Episode 1377/10000, loss: 0.00244654016569256847\n",
      "Episode Reward: 3.0\n",
      "Step 289 (361750) @ Episode 1378/10000, loss: 0.00082078075502067857\n",
      "Episode Reward: 2.0\n",
      "Step 333 (362083) @ Episode 1379/10000, loss: 0.00252461736090481343\n",
      "Episode Reward: 3.0\n",
      "Step 258 (362341) @ Episode 1380/10000, loss: 0.00408211816102266335\n",
      "Episode Reward: 2.0\n",
      "Step 214 (362555) @ Episode 1381/10000, loss: 0.00061668548732995995\n",
      "Episode Reward: 1.0\n",
      "Step 355 (362910) @ Episode 1382/10000, loss: 0.00199116719886660586\n",
      "Episode Reward: 3.0\n",
      "Step 359 (363269) @ Episode 1383/10000, loss: 0.00350773264653980736\n",
      "Episode Reward: 3.0\n",
      "Step 304 (363573) @ Episode 1384/10000, loss: 0.00147308770101517443\n",
      "Episode Reward: 3.0\n",
      "Step 322 (363895) @ Episode 1385/10000, loss: 0.00222925515845417987\n",
      "Episode Reward: 3.0\n",
      "Step 411 (364306) @ Episode 1386/10000, loss: 0.00244421465322375374\n",
      "Episode Reward: 4.0\n",
      "Step 588 (364894) @ Episode 1387/10000, loss: 0.00577190052717924136\n",
      "Episode Reward: 7.0\n",
      "Step 390 (365284) @ Episode 1388/10000, loss: 0.00231094844639301385\n",
      "Episode Reward: 4.0\n",
      "Step 301 (365585) @ Episode 1389/10000, loss: 0.00095984863582998517\n",
      "Episode Reward: 2.0\n",
      "Step 318 (365903) @ Episode 1390/10000, loss: 0.00329333613626658923\n",
      "Episode Reward: 3.0\n",
      "Step 523 (366426) @ Episode 1391/10000, loss: 0.00471384683623909952\n",
      "Episode Reward: 6.0\n",
      "Step 283 (366709) @ Episode 1392/10000, loss: 0.00110915233381092554\n",
      "Episode Reward: 1.0\n",
      "Step 348 (367057) @ Episode 1393/10000, loss: 0.00092560704797506336\n",
      "Episode Reward: 4.0\n",
      "Step 270 (367327) @ Episode 1394/10000, loss: 0.00147878634743392477\n",
      "Episode Reward: 2.0\n",
      "Step 469 (367796) @ Episode 1395/10000, loss: 0.00225735572166740973\n",
      "Episode Reward: 6.0\n",
      "Step 232 (368028) @ Episode 1396/10000, loss: 0.00160047109238803398\n",
      "Episode Reward: 1.0\n",
      "Step 506 (368534) @ Episode 1397/10000, loss: 0.00050208938773721464\n",
      "Episode Reward: 5.0\n",
      "Step 378 (368912) @ Episode 1398/10000, loss: 0.00085812545148655774\n",
      "Episode Reward: 4.0\n",
      "Step 418 (369330) @ Episode 1399/10000, loss: 0.00129743292927742545\n",
      "Episode Reward: 4.0\n",
      "Step 258 (369588) @ Episode 1400/10000, loss: 0.00035928454599343245\n",
      "Episode Reward: 2.0\n",
      "Step 403 (369991) @ Episode 1401/10000, loss: 0.00031193616450764247\n",
      "Episode Reward: 4.0\n",
      "Step 8 (369999) @ Episode 1402/10000, loss: 0.0017913986230269074\n",
      "Copied model parameters to target network.\n",
      "Step 308 (370299) @ Episode 1402/10000, loss: 0.00160863343626260766\n",
      "Episode Reward: 2.0\n",
      "Step 301 (370600) @ Episode 1403/10000, loss: 0.00362129835411906245\n",
      "Episode Reward: 2.0\n",
      "Step 450 (371050) @ Episode 1404/10000, loss: 0.00131200416944921026\n",
      "Episode Reward: 5.0\n",
      "Step 518 (371568) @ Episode 1405/10000, loss: 0.00053812225814908747\n",
      "Episode Reward: 6.0\n",
      "Step 460 (372028) @ Episode 1406/10000, loss: 0.00028045038925483823\n",
      "Episode Reward: 5.0\n",
      "Step 322 (372350) @ Episode 1407/10000, loss: 0.00317278504371643077\n",
      "Episode Reward: 3.0\n",
      "Step 437 (372787) @ Episode 1408/10000, loss: 0.00163980084471404555\n",
      "Episode Reward: 5.0\n",
      "Step 259 (373046) @ Episode 1409/10000, loss: 0.00177198392339050777\n",
      "Episode Reward: 1.0\n",
      "Step 315 (373361) @ Episode 1410/10000, loss: 0.00072005658876150857\n",
      "Episode Reward: 3.0\n",
      "Step 341 (373702) @ Episode 1411/10000, loss: 0.00271015940234065065\n",
      "Episode Reward: 3.0\n",
      "Step 328 (374030) @ Episode 1412/10000, loss: 0.00181400217115879065\n",
      "Episode Reward: 2.0\n",
      "Step 376 (374406) @ Episode 1413/10000, loss: 0.00082621071487665187\n",
      "Episode Reward: 4.0\n",
      "Step 563 (374969) @ Episode 1414/10000, loss: 0.00186609779484570036\n",
      "Episode Reward: 7.0\n",
      "Step 369 (375338) @ Episode 1415/10000, loss: 0.00060916907386854297\n",
      "Episode Reward: 3.0\n",
      "Step 458 (375796) @ Episode 1416/10000, loss: 0.00065561727387830626\n",
      "Episode Reward: 5.0\n",
      "Step 496 (376292) @ Episode 1417/10000, loss: 0.00178392825182527393\n",
      "Episode Reward: 6.0\n",
      "Step 421 (376713) @ Episode 1418/10000, loss: 0.00152542570140212773\n",
      "Episode Reward: 4.0\n",
      "Step 438 (377151) @ Episode 1419/10000, loss: 0.00345068750903010376\n",
      "Episode Reward: 4.0\n",
      "Step 485 (377636) @ Episode 1420/10000, loss: 0.00066336954478174457\n",
      "Episode Reward: 6.0\n",
      "Step 578 (378214) @ Episode 1421/10000, loss: 0.00516810547560453412\n",
      "Episode Reward: 7.0\n",
      "Step 392 (378606) @ Episode 1422/10000, loss: 0.00147344905417412526\n",
      "Episode Reward: 4.0\n",
      "Step 595 (379201) @ Episode 1423/10000, loss: 0.00575926480814814612\n",
      "Episode Reward: 7.0\n",
      "Step 536 (379737) @ Episode 1424/10000, loss: 0.00155861338134855036\n",
      "Episode Reward: 5.0\n",
      "Step 262 (379999) @ Episode 1425/10000, loss: 0.00071451958501711494\n",
      "Copied model parameters to target network.\n",
      "Step 483 (380220) @ Episode 1425/10000, loss: 0.00257162190973758726\n",
      "Episode Reward: 5.0\n",
      "Step 515 (380735) @ Episode 1426/10000, loss: 0.00510644074529409433\n",
      "Episode Reward: 5.0\n",
      "Step 332 (381067) @ Episode 1427/10000, loss: 0.00077583634993061427\n",
      "Episode Reward: 3.0\n",
      "Step 424 (381491) @ Episode 1428/10000, loss: 0.00099336321000009787\n",
      "Episode Reward: 4.0\n",
      "Step 441 (381932) @ Episode 1429/10000, loss: 0.00039099127752706413\n",
      "Episode Reward: 5.0\n",
      "Step 603 (382535) @ Episode 1430/10000, loss: 0.00535033550113439664\n",
      "Episode Reward: 8.0\n",
      "Step 300 (382835) @ Episode 1431/10000, loss: 0.00153686921112239367\n",
      "Episode Reward: 2.0\n",
      "Step 470 (383305) @ Episode 1432/10000, loss: 0.00476856809109449444\n",
      "Episode Reward: 5.0\n",
      "Step 639 (383944) @ Episode 1433/10000, loss: 0.00248767691664397785\n",
      "Episode Reward: 8.0\n",
      "Step 453 (384397) @ Episode 1434/10000, loss: 0.00615928182378411324\n",
      "Episode Reward: 4.0\n",
      "Step 521 (384918) @ Episode 1435/10000, loss: 0.00109195429831743245\n",
      "Episode Reward: 6.0\n",
      "Step 749 (385667) @ Episode 1436/10000, loss: 0.00068230903707444676\n",
      "Episode Reward: 11.0\n",
      "Step 407 (386074) @ Episode 1437/10000, loss: 0.00227078702300786973\n",
      "Episode Reward: 4.0\n",
      "Step 424 (386498) @ Episode 1438/10000, loss: 0.00067222776124253873\n",
      "Episode Reward: 4.0\n",
      "Step 434 (386932) @ Episode 1439/10000, loss: 0.00118037709034979344\n",
      "Episode Reward: 6.0\n",
      "Step 546 (387478) @ Episode 1440/10000, loss: 0.00357495178468525433\n",
      "Episode Reward: 11.0\n",
      "Step 486 (387964) @ Episode 1441/10000, loss: 0.00029493114561773837\n",
      "Episode Reward: 6.0\n",
      "Step 521 (388485) @ Episode 1442/10000, loss: 0.00156860088463872676\n",
      "Episode Reward: 9.0\n",
      "Step 594 (389079) @ Episode 1443/10000, loss: 0.00042138472781516616\n",
      "Episode Reward: 8.0\n",
      "Step 430 (389509) @ Episode 1444/10000, loss: 0.00115142134018242365\n",
      "Episode Reward: 5.0\n",
      "Step 366 (389875) @ Episode 1445/10000, loss: 0.00443492271006107355\n",
      "Episode Reward: 4.0\n",
      "Step 124 (389999) @ Episode 1446/10000, loss: 0.0016073007136583328\n",
      "Copied model parameters to target network.\n",
      "Step 598 (390473) @ Episode 1446/10000, loss: 0.00213355338200926886\n",
      "Episode Reward: 7.0\n",
      "Step 639 (391112) @ Episode 1447/10000, loss: 0.00194101571105420647\n",
      "Episode Reward: 8.0\n",
      "Step 450 (391562) @ Episode 1448/10000, loss: 0.00274314032867550857\n",
      "Episode Reward: 5.0\n",
      "Step 524 (392086) @ Episode 1449/10000, loss: 0.00264012510888278554\n",
      "Episode Reward: 6.0\n",
      "Step 584 (392670) @ Episode 1450/10000, loss: 0.00371953705325722737\n",
      "Episode Reward: 6.0\n",
      "Step 779 (393449) @ Episode 1451/10000, loss: 0.00235932925716042547\n",
      "Episode Reward: 16.0\n",
      "Step 336 (393785) @ Episode 1452/10000, loss: 0.00085291807772591714\n",
      "Episode Reward: 3.0\n",
      "Step 264 (394049) @ Episode 1453/10000, loss: 0.00086583773372694853\n",
      "Episode Reward: 2.0\n",
      "Step 676 (394725) @ Episode 1454/10000, loss: 0.00060966017190366983\n",
      "Episode Reward: 9.0\n",
      "Step 679 (395404) @ Episode 1455/10000, loss: 0.00098273088224232296\n",
      "Episode Reward: 8.0\n",
      "Step 576 (395980) @ Episode 1456/10000, loss: 0.00266497978009283544\n",
      "Episode Reward: 7.0\n",
      "Step 488 (396468) @ Episode 1457/10000, loss: 0.00799230020493269867\n",
      "Episode Reward: 6.0\n",
      "Step 510 (396978) @ Episode 1458/10000, loss: 0.00140398426447063686\n",
      "Episode Reward: 7.0\n",
      "Step 390 (397368) @ Episode 1459/10000, loss: 0.00909270904958248166\n",
      "Episode Reward: 4.0\n",
      "Step 435 (397803) @ Episode 1460/10000, loss: 0.00491368910297751474\n",
      "Episode Reward: 5.0\n",
      "Step 464 (398267) @ Episode 1461/10000, loss: 0.00268463115207850936\n",
      "Episode Reward: 4.0\n",
      "Step 639 (398906) @ Episode 1462/10000, loss: 0.00029311521211639047\n",
      "Episode Reward: 9.0\n",
      "Step 687 (399593) @ Episode 1463/10000, loss: 0.00240760156884789474\n",
      "Episode Reward: 9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 406 (399999) @ Episode 1464/10000, loss: 0.00107395625673234464\n",
      "Copied model parameters to target network.\n",
      "Step 536 (400129) @ Episode 1464/10000, loss: 0.0032575621735304594\n",
      "Episode Reward: 7.0\n",
      "Step 415 (400544) @ Episode 1465/10000, loss: 0.00326166371814906693\n",
      "Episode Reward: 4.0\n",
      "Step 514 (401058) @ Episode 1466/10000, loss: 0.00295550352893769746\n",
      "Episode Reward: 8.0\n",
      "Step 522 (401580) @ Episode 1467/10000, loss: 0.00182626151945441966\n",
      "Episode Reward: 7.0\n",
      "Step 735 (402315) @ Episode 1468/10000, loss: 0.00089843646856024864\n",
      "Episode Reward: 9.0\n",
      "Step 400 (402715) @ Episode 1469/10000, loss: 0.00365795940160751345\n",
      "Episode Reward: 3.0\n",
      "Step 573 (403288) @ Episode 1470/10000, loss: 0.00168546196073293695\n",
      "Episode Reward: 7.0\n",
      "Step 409 (403697) @ Episode 1471/10000, loss: 0.00142172735650092363\n",
      "Episode Reward: 5.0\n",
      "Step 475 (404172) @ Episode 1472/10000, loss: 0.00249529560096561987\n",
      "Episode Reward: 6.0\n",
      "Step 712 (404884) @ Episode 1473/10000, loss: 0.00606653559952974374\n",
      "Episode Reward: 12.0\n",
      "Step 410 (405294) @ Episode 1474/10000, loss: 0.00786522217094898245\n",
      "Episode Reward: 5.0\n",
      "Step 433 (405727) @ Episode 1475/10000, loss: 0.00429146271198988406\n",
      "Episode Reward: 5.0\n",
      "Step 539 (406266) @ Episode 1476/10000, loss: 0.00105831935070455077\n",
      "Episode Reward: 6.0\n",
      "Step 667 (406933) @ Episode 1477/10000, loss: 0.00081519782543182375\n",
      "Episode Reward: 13.0\n",
      "Step 662 (407595) @ Episode 1478/10000, loss: 0.00500462530180811956\n",
      "Episode Reward: 9.0\n",
      "Step 570 (408165) @ Episode 1479/10000, loss: 0.00146333710290491586\n",
      "Episode Reward: 7.0\n",
      "Step 362 (408527) @ Episode 1480/10000, loss: 0.00201266002841293856\n",
      "Episode Reward: 3.0\n",
      "Step 614 (409141) @ Episode 1481/10000, loss: 0.00300845433957874776\n",
      "Episode Reward: 8.0\n",
      "Step 731 (409872) @ Episode 1482/10000, loss: 0.00223487662151455974\n",
      "Episode Reward: 12.0\n",
      "Step 127 (409999) @ Episode 1483/10000, loss: 0.0017500303220003843\n",
      "Copied model parameters to target network.\n",
      "Step 587 (410459) @ Episode 1483/10000, loss: 0.0068877637386322024\n",
      "Episode Reward: 7.0\n",
      "Step 1246 (411705) @ Episode 1484/10000, loss: 0.0007818794110789895\n",
      "Episode Reward: 22.0\n",
      "Step 759 (412464) @ Episode 1485/10000, loss: 0.00069809670094400646\n",
      "Episode Reward: 14.0\n",
      "Step 513 (412977) @ Episode 1486/10000, loss: 0.00299799512140452864\n",
      "Episode Reward: 7.0\n",
      "Step 757 (413734) @ Episode 1487/10000, loss: 0.00640436122193932587\n",
      "Episode Reward: 10.0\n",
      "Step 560 (414294) @ Episode 1488/10000, loss: 0.00653829053044319153\n",
      "Episode Reward: 10.0\n",
      "Step 537 (414831) @ Episode 1489/10000, loss: 0.00069006357807666067\n",
      "Episode Reward: 7.0\n",
      "Step 820 (415651) @ Episode 1490/10000, loss: 0.00470413733273744646\n",
      "Episode Reward: 14.0\n",
      "Step 566 (416217) @ Episode 1491/10000, loss: 0.00476713990792632157\n",
      "Episode Reward: 7.0\n",
      "Step 577 (416794) @ Episode 1492/10000, loss: 0.00178799440618604426\n",
      "Episode Reward: 8.0\n",
      "Step 664 (417458) @ Episode 1493/10000, loss: 0.00155843328684568475\n",
      "Episode Reward: 9.0\n",
      "Step 631 (418089) @ Episode 1494/10000, loss: 0.00130527350120246435\n",
      "Episode Reward: 11.0\n",
      "Step 560 (418649) @ Episode 1495/10000, loss: 0.00278381677344441427\n",
      "Episode Reward: 8.0\n",
      "Step 631 (419280) @ Episode 1496/10000, loss: 0.00190537178423255683\n",
      "Episode Reward: 8.0\n",
      "Step 636 (419916) @ Episode 1497/10000, loss: 0.00140203849878162157\n",
      "Episode Reward: 10.0\n",
      "Step 83 (419999) @ Episode 1498/10000, loss: 0.0015754479682072997\n",
      "Copied model parameters to target network.\n",
      "Step 495 (420411) @ Episode 1498/10000, loss: 0.0025077452883124356\n",
      "Episode Reward: 6.0\n",
      "Step 681 (421092) @ Episode 1499/10000, loss: 0.00262262159958481886\n",
      "Episode Reward: 13.0\n",
      "Step 562 (421654) @ Episode 1500/10000, loss: 0.00490658357739448555\n",
      "Episode Reward: 8.0\n",
      "Step 593 (422247) @ Episode 1501/10000, loss: 0.0031084399670362473\n",
      "Episode Reward: 8.0\n",
      "Step 467 (422714) @ Episode 1502/10000, loss: 0.00102727953344583514\n",
      "Episode Reward: 4.0\n",
      "Step 542 (423256) @ Episode 1503/10000, loss: 0.00358734233304858214\n",
      "Episode Reward: 7.0\n",
      "Step 635 (423891) @ Episode 1504/10000, loss: 0.00488494522869586946\n",
      "Episode Reward: 7.0\n",
      "Step 757 (424648) @ Episode 1505/10000, loss: 0.00110520271118730355\n",
      "Episode Reward: 11.0\n",
      "Step 801 (425449) @ Episode 1506/10000, loss: 0.00569787528365850454\n",
      "Episode Reward: 11.0\n",
      "Step 889 (426338) @ Episode 1507/10000, loss: 0.00169788824860006574\n",
      "Episode Reward: 13.0\n",
      "Step 619 (426957) @ Episode 1508/10000, loss: 0.00181845831684768233\n",
      "Episode Reward: 9.0\n",
      "Step 603 (427560) @ Episode 1509/10000, loss: 0.00491691241040825844\n",
      "Episode Reward: 8.0\n",
      "Step 841 (428401) @ Episode 1510/10000, loss: 0.00285110017284750943\n",
      "Episode Reward: 13.0\n",
      "Step 766 (429167) @ Episode 1511/10000, loss: 0.00320277293212711845\n",
      "Episode Reward: 10.0\n",
      "Step 725 (429892) @ Episode 1512/10000, loss: 0.00606086896732449574\n",
      "Episode Reward: 8.0\n",
      "Step 107 (429999) @ Episode 1513/10000, loss: 0.0010077171027660373\n",
      "Copied model parameters to target network.\n",
      "Step 816 (430708) @ Episode 1513/10000, loss: 0.00105394609272480014\n",
      "Episode Reward: 15.0\n",
      "Step 858 (431566) @ Episode 1514/10000, loss: 0.0016482131322845817\n",
      "Episode Reward: 18.0\n",
      "Step 687 (432253) @ Episode 1515/10000, loss: 0.00223601050674915343\n",
      "Episode Reward: 9.0\n",
      "Step 843 (433096) @ Episode 1516/10000, loss: 0.00070759968366473916\n",
      "Episode Reward: 13.0\n",
      "Step 969 (434065) @ Episode 1517/10000, loss: 0.00133695302065461879\n",
      "Episode Reward: 15.0\n",
      "Step 679 (434744) @ Episode 1518/10000, loss: 0.00085787085117772227\n",
      "Episode Reward: 9.0\n",
      "Step 656 (435400) @ Episode 1519/10000, loss: 0.00066957337548956276\n",
      "Episode Reward: 9.0\n",
      "Step 824 (436224) @ Episode 1520/10000, loss: 0.02561393752694135757\n",
      "Episode Reward: 14.0\n",
      "Step 869 (437093) @ Episode 1521/10000, loss: 0.00513562187552452176\n",
      "Episode Reward: 20.0\n",
      "Step 829 (437922) @ Episode 1522/10000, loss: 0.0022132480517029762\n",
      "Episode Reward: 11.0\n",
      "Step 919 (438841) @ Episode 1523/10000, loss: 0.00095986248925328255\n",
      "Episode Reward: 15.0\n",
      "Step 815 (439656) @ Episode 1524/10000, loss: 0.00454124296084046466\n",
      "Episode Reward: 20.0\n",
      "Step 343 (439999) @ Episode 1525/10000, loss: 0.00121539668180048473\n",
      "Copied model parameters to target network.\n",
      "Step 780 (440436) @ Episode 1525/10000, loss: 0.0058775283396244056\n",
      "Episode Reward: 13.0\n",
      "Step 705 (441141) @ Episode 1526/10000, loss: 0.0076761557720601564\n",
      "Episode Reward: 10.0\n",
      "Step 758 (441899) @ Episode 1527/10000, loss: 0.00251872604712843954\n",
      "Episode Reward: 12.0\n",
      "Step 841 (442740) @ Episode 1528/10000, loss: 0.00175970757845789286\n",
      "Episode Reward: 18.0\n",
      "Step 724 (443464) @ Episode 1529/10000, loss: 0.00289275497198104866\n",
      "Episode Reward: 12.0\n",
      "Step 492 (443956) @ Episode 1530/10000, loss: 0.0071129091084003451\n",
      "Episode Reward: 6.0\n",
      "Step 623 (444579) @ Episode 1531/10000, loss: 0.00316167552955448636\n",
      "Episode Reward: 9.0\n",
      "Step 598 (445177) @ Episode 1532/10000, loss: 0.00144282379187643535\n",
      "Episode Reward: 11.0\n",
      "Step 717 (445894) @ Episode 1533/10000, loss: 0.0015749593731015922\n",
      "Episode Reward: 10.0\n",
      "Step 781 (446675) @ Episode 1534/10000, loss: 0.00128103350289165975\n",
      "Episode Reward: 11.0\n",
      "Step 708 (447383) @ Episode 1535/10000, loss: 0.0053720548748970033\n",
      "Episode Reward: 11.0\n",
      "Step 956 (448339) @ Episode 1536/10000, loss: 0.00298474053852260197\n",
      "Episode Reward: 17.0\n",
      "Step 829 (449168) @ Episode 1537/10000, loss: 0.00247904937714338336\n",
      "Episode Reward: 12.0\n",
      "Step 831 (449999) @ Episode 1538/10000, loss: 0.00129301124252378943\n",
      "Copied model parameters to target network.\n",
      "Step 869 (450037) @ Episode 1538/10000, loss: 0.0031636275816708803\n",
      "Episode Reward: 14.0\n",
      "Step 824 (450861) @ Episode 1539/10000, loss: 0.00253202766180038454\n",
      "Episode Reward: 13.0\n",
      "Step 803 (451664) @ Episode 1540/10000, loss: 0.0015962350880727172\n",
      "Episode Reward: 12.0\n",
      "Step 811 (452475) @ Episode 1541/10000, loss: 0.00084097497165203095\n",
      "Episode Reward: 14.0\n",
      "Step 874 (453349) @ Episode 1542/10000, loss: 0.00282862829044461255\n",
      "Episode Reward: 18.0\n",
      "Step 895 (454244) @ Episode 1543/10000, loss: 0.00402729166671633756\n",
      "Episode Reward: 18.0\n",
      "Step 1110 (455354) @ Episode 1544/10000, loss: 0.0011073054047301412\n",
      "Episode Reward: 25.0\n",
      "Step 788 (456142) @ Episode 1545/10000, loss: 0.00489090988412499457\n",
      "Episode Reward: 11.0\n",
      "Step 558 (456700) @ Episode 1546/10000, loss: 0.0016268976032733917\n",
      "Episode Reward: 7.0\n",
      "Step 924 (457624) @ Episode 1547/10000, loss: 0.00552032422274351146\n",
      "Episode Reward: 16.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 990 (458614) @ Episode 1548/10000, loss: 0.00275090662762522735\n",
      "Episode Reward: 14.0\n",
      "Step 663 (459277) @ Episode 1549/10000, loss: 0.0014167523477226496\n",
      "Episode Reward: 9.0\n",
      "Step 722 (459999) @ Episode 1550/10000, loss: 0.0013196641812101007\n",
      "Copied model parameters to target network.\n",
      "Step 922 (460199) @ Episode 1550/10000, loss: 0.0011920932447537784\n",
      "Episode Reward: 19.0\n",
      "Step 846 (461045) @ Episode 1551/10000, loss: 0.0026994105428457265\n",
      "Episode Reward: 27.0\n",
      "Step 1087 (462132) @ Episode 1552/10000, loss: 0.0028742854483425617\n",
      "Episode Reward: 26.0\n",
      "Step 901 (463033) @ Episode 1553/10000, loss: 0.00128104002214968255\n",
      "Episode Reward: 13.0\n",
      "Step 1112 (464145) @ Episode 1554/10000, loss: 0.0019113119924440983\n",
      "Episode Reward: 22.0\n",
      "Step 937 (465082) @ Episode 1555/10000, loss: 0.0011090738698840141\n",
      "Episode Reward: 18.0\n",
      "Step 814 (465896) @ Episode 1556/10000, loss: 0.0026070408057421446\n",
      "Episode Reward: 12.0\n",
      "Step 944 (466840) @ Episode 1557/10000, loss: 0.0014281522016972303\n",
      "Episode Reward: 16.0\n",
      "Step 865 (467705) @ Episode 1558/10000, loss: 0.00208567688241601137\n",
      "Episode Reward: 15.0\n",
      "Step 760 (468465) @ Episode 1559/10000, loss: 0.00266577862203121254\n",
      "Episode Reward: 16.0\n",
      "Step 1004 (469469) @ Episode 1560/10000, loss: 0.0023796968162059784\n",
      "Episode Reward: 19.0\n",
      "Step 530 (469999) @ Episode 1561/10000, loss: 0.00609651114791631726\n",
      "Copied model parameters to target network.\n",
      "Step 910 (470379) @ Episode 1561/10000, loss: 0.0017741499468684196\n",
      "Episode Reward: 20.0\n",
      "Step 825 (471204) @ Episode 1562/10000, loss: 0.0052785924635827546\n",
      "Episode Reward: 13.0\n",
      "Step 1022 (472226) @ Episode 1563/10000, loss: 0.0017421655356884003\n",
      "Episode Reward: 17.0\n",
      "Step 899 (473125) @ Episode 1564/10000, loss: 0.0025342551525682218\n",
      "Episode Reward: 14.0\n",
      "Step 784 (473909) @ Episode 1565/10000, loss: 0.00475198915228247653\n",
      "Episode Reward: 15.0\n",
      "Step 903 (474812) @ Episode 1566/10000, loss: 0.0032188233453780413\n",
      "Episode Reward: 17.0\n",
      "Step 1229 (476041) @ Episode 1567/10000, loss: 0.0015469822101294994\n",
      "Episode Reward: 24.0\n",
      "Step 1134 (477175) @ Episode 1568/10000, loss: 0.0012724789557978514\n",
      "Episode Reward: 23.0\n",
      "Step 800 (477975) @ Episode 1569/10000, loss: 0.00183006585575640265\n",
      "Episode Reward: 15.0\n",
      "Step 709 (478684) @ Episode 1570/10000, loss: 0.0029989860486239195\n",
      "Episode Reward: 12.0\n",
      "Step 650 (479334) @ Episode 1571/10000, loss: 0.0037676743231713774\n",
      "Episode Reward: 10.0\n",
      "Step 665 (479999) @ Episode 1572/10000, loss: 0.0011617336422204971\n",
      "Copied model parameters to target network.\n",
      "Step 744 (480078) @ Episode 1572/10000, loss: 0.0053600333631038672\n",
      "Episode Reward: 15.0\n",
      "Step 1107 (481185) @ Episode 1573/10000, loss: 0.0015407059108838442\n",
      "Episode Reward: 20.0\n",
      "Step 988 (482173) @ Episode 1574/10000, loss: 0.00339567661285400473\n",
      "Episode Reward: 13.0\n",
      "Step 1028 (483201) @ Episode 1575/10000, loss: 0.0021826797164976597\n",
      "Episode Reward: 25.0\n",
      "Step 841 (484042) @ Episode 1576/10000, loss: 0.0017887043068185449\n",
      "Episode Reward: 12.0\n",
      "Step 924 (484966) @ Episode 1577/10000, loss: 0.00966323819011449835\n",
      "Episode Reward: 26.0\n",
      "Step 1139 (486105) @ Episode 1578/10000, loss: 0.0032542473636567593\n",
      "Episode Reward: 21.0\n",
      "Step 1430 (487535) @ Episode 1579/10000, loss: 0.0053599001839756966\n",
      "Episode Reward: 41.0\n",
      "Step 1316 (488851) @ Episode 1580/10000, loss: 0.0012389245675876737\n",
      "Episode Reward: 25.0\n",
      "Step 971 (489822) @ Episode 1581/10000, loss: 0.0012341497931629429\n",
      "Episode Reward: 23.0\n",
      "Step 177 (489999) @ Episode 1582/10000, loss: 0.0034188225399702787\n",
      "Copied model parameters to target network.\n",
      "Step 886 (490708) @ Episode 1582/10000, loss: 0.0022133644670248034\n",
      "Episode Reward: 19.0\n",
      "Step 473 (491181) @ Episode 1583/10000, loss: 0.0027726802509278067\n",
      "Episode Reward: 7.0\n",
      "Step 824 (492005) @ Episode 1584/10000, loss: 0.00277722277678549334\n",
      "Episode Reward: 14.0\n",
      "Step 1165 (493170) @ Episode 1585/10000, loss: 0.0035503017716109753\n",
      "Episode Reward: 32.0\n",
      "Step 908 (494078) @ Episode 1586/10000, loss: 0.01233446784317493467\n",
      "Episode Reward: 20.0\n",
      "Step 859 (494937) @ Episode 1587/10000, loss: 0.00188915990293025977\n",
      "Episode Reward: 22.0\n",
      "Step 1593 (496530) @ Episode 1588/10000, loss: 0.0012898400891572237\n",
      "Episode Reward: 44.0\n",
      "Step 1016 (497546) @ Episode 1589/10000, loss: 0.0051348470151424417\n",
      "Episode Reward: 19.0\n",
      "Step 970 (498516) @ Episode 1590/10000, loss: 0.00237604836001992236\n",
      "Episode Reward: 23.0\n",
      "Step 1031 (499547) @ Episode 1591/10000, loss: 0.0040410421788692474\n",
      "Episode Reward: 23.0\n",
      "Step 452 (499999) @ Episode 1592/10000, loss: 0.0110977096483111382\n",
      "Copied model parameters to target network.\n",
      "Step 991 (500538) @ Episode 1592/10000, loss: 0.00556164002045989366\n",
      "Episode Reward: 31.0\n",
      "Step 1128 (501666) @ Episode 1593/10000, loss: 0.0025694929063320162\n",
      "Episode Reward: 25.0\n",
      "Step 880 (502546) @ Episode 1594/10000, loss: 0.0024621891789138317\n",
      "Episode Reward: 23.0\n",
      "Step 884 (503430) @ Episode 1595/10000, loss: 0.00174220441840589053\n",
      "Episode Reward: 21.0\n",
      "Step 1346 (504776) @ Episode 1596/10000, loss: 0.0034393877722322943\n",
      "Episode Reward: 29.0\n",
      "Step 1201 (505977) @ Episode 1597/10000, loss: 0.0028553509619086986\n",
      "Episode Reward: 26.0\n",
      "Step 866 (506843) @ Episode 1598/10000, loss: 0.00180022628046572276\n",
      "Episode Reward: 18.0\n",
      "Step 1123 (507966) @ Episode 1599/10000, loss: 0.0037237755022943023\n",
      "Episode Reward: 24.0\n",
      "Step 1025 (508991) @ Episode 1600/10000, loss: 0.0025631617754697887\n",
      "Episode Reward: 18.0\n",
      "Step 1008 (509999) @ Episode 1601/10000, loss: 0.0024842559359967718\n",
      "Copied model parameters to target network.\n",
      "Step 1395 (510386) @ Episode 1601/10000, loss: 0.0058399802073836335\n",
      "Episode Reward: 28.0\n",
      "Step 850 (511236) @ Episode 1602/10000, loss: 0.0054694353602826595\n",
      "Episode Reward: 16.0\n",
      "Step 741 (511977) @ Episode 1603/10000, loss: 0.00507282232865691255\n",
      "Episode Reward: 16.0\n",
      "Step 825 (512802) @ Episode 1604/10000, loss: 0.0099237430840730674\n",
      "Episode Reward: 14.0\n",
      "Step 941 (513743) @ Episode 1605/10000, loss: 0.00218626181595027456\n",
      "Episode Reward: 19.0\n",
      "Step 1524 (515267) @ Episode 1606/10000, loss: 0.0011215761769562964\n",
      "Episode Reward: 42.0\n",
      "Step 1135 (516402) @ Episode 1607/10000, loss: 0.0026525068096816541\n",
      "Episode Reward: 29.0\n",
      "Step 1019 (517421) @ Episode 1608/10000, loss: 0.0098271481692790999\n",
      "Episode Reward: 25.0\n",
      "Step 1337 (518758) @ Episode 1609/10000, loss: 0.0079799033701419834\n",
      "Episode Reward: 40.0\n",
      "Step 1126 (519884) @ Episode 1610/10000, loss: 0.0047652940265834333\n",
      "Episode Reward: 24.0\n",
      "Step 115 (519999) @ Episode 1611/10000, loss: 0.0026466001290827999\n",
      "Copied model parameters to target network.\n",
      "Step 783 (520667) @ Episode 1611/10000, loss: 0.00097675994038581853\n",
      "Episode Reward: 17.0\n",
      "Step 979 (521646) @ Episode 1612/10000, loss: 0.0019640964455902576\n",
      "Episode Reward: 22.0\n",
      "Step 961 (522607) @ Episode 1613/10000, loss: 0.0097016980871558195\n",
      "Episode Reward: 19.0\n",
      "Step 1200 (523807) @ Episode 1614/10000, loss: 0.0125157479196786882\n",
      "Episode Reward: 21.0\n",
      "Step 890 (524697) @ Episode 1615/10000, loss: 0.0051984558813273916\n",
      "Episode Reward: 25.0\n",
      "Step 598 (525295) @ Episode 1616/10000, loss: 0.0034369095228612423\n",
      "Episode Reward: 9.0\n",
      "Step 813 (526108) @ Episode 1617/10000, loss: 0.0018752610776573427\n",
      "Episode Reward: 13.0\n",
      "Step 873 (526981) @ Episode 1618/10000, loss: 0.0049380017444491393\n",
      "Episode Reward: 15.0\n",
      "Step 1129 (528110) @ Episode 1619/10000, loss: 0.0016861712792888284\n",
      "Episode Reward: 32.0\n",
      "Step 810 (528920) @ Episode 1620/10000, loss: 0.0024288024287670855\n",
      "Episode Reward: 13.0\n",
      "Step 1045 (529965) @ Episode 1621/10000, loss: 0.0043470659293234354\n",
      "Episode Reward: 18.0\n",
      "Step 34 (529999) @ Episode 1622/10000, loss: 0.0013113160384818912\n",
      "Copied model parameters to target network.\n",
      "Step 1336 (531301) @ Episode 1622/10000, loss: 0.0073895351961255076\n",
      "Episode Reward: 33.0\n",
      "Step 1065 (532366) @ Episode 1623/10000, loss: 0.0051486091688275345\n",
      "Episode Reward: 18.0\n",
      "Step 910 (533276) @ Episode 1624/10000, loss: 0.0085333511233329773\n",
      "Episode Reward: 29.0\n",
      "Step 1017 (534293) @ Episode 1625/10000, loss: 0.0077979201450943954\n",
      "Episode Reward: 22.0\n",
      "Step 1215 (535508) @ Episode 1626/10000, loss: 0.0083029344677925117\n",
      "Episode Reward: 32.0\n",
      "Step 915 (536423) @ Episode 1627/10000, loss: 0.00468721566721797044\n",
      "Episode Reward: 16.0\n",
      "Step 1424 (537847) @ Episode 1628/10000, loss: 0.0011027051368728289\n",
      "Episode Reward: 32.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 875 (538722) @ Episode 1629/10000, loss: 0.0051190154626965526\n",
      "Episode Reward: 22.0\n",
      "Step 1044 (539766) @ Episode 1630/10000, loss: 0.0061134272255003455\n",
      "Episode Reward: 22.0\n",
      "Step 233 (539999) @ Episode 1631/10000, loss: 0.0043812086805701267\n",
      "Copied model parameters to target network.\n",
      "Step 1074 (540840) @ Episode 1631/10000, loss: 0.0029781481716781855\n",
      "Episode Reward: 22.0\n",
      "Step 970 (541810) @ Episode 1632/10000, loss: 0.0032530464231967926\n",
      "Episode Reward: 20.0\n",
      "Step 1033 (542843) @ Episode 1633/10000, loss: 0.0012888102792203426\n",
      "Episode Reward: 16.0\n",
      "Step 862 (543705) @ Episode 1634/10000, loss: 0.0059585357084870346\n",
      "Episode Reward: 21.0\n",
      "Step 1052 (544757) @ Episode 1635/10000, loss: 0.0017602350562810898\n",
      "Episode Reward: 32.0\n",
      "Step 653 (545410) @ Episode 1636/10000, loss: 0.0022568884305655956\n",
      "Episode Reward: 11.0\n",
      "Step 1153 (546563) @ Episode 1637/10000, loss: 0.0161670707166194923\n",
      "Episode Reward: 31.0\n",
      "Step 780 (547343) @ Episode 1638/10000, loss: 0.0038980860263109207\n",
      "Episode Reward: 12.0\n",
      "Step 622 (547965) @ Episode 1639/10000, loss: 0.0069168815389275554\n",
      "Episode Reward: 10.0\n",
      "Step 1262 (549227) @ Episode 1640/10000, loss: 0.0010638857493177056\n",
      "Episode Reward: 32.0\n",
      "Step 772 (549999) @ Episode 1641/10000, loss: 0.0015793808270245796\n",
      "Copied model parameters to target network.\n",
      "Step 1367 (550594) @ Episode 1641/10000, loss: 0.0027016568928956985\n",
      "Episode Reward: 33.0\n",
      "Step 1132 (551726) @ Episode 1642/10000, loss: 0.0089184762910008431\n",
      "Episode Reward: 29.0\n",
      "Step 1165 (552891) @ Episode 1643/10000, loss: 0.0008886532741598785\n",
      "Episode Reward: 32.0\n",
      "Step 1143 (554034) @ Episode 1644/10000, loss: 0.0012561972253024578\n",
      "Episode Reward: 27.0\n",
      "Step 731 (554765) @ Episode 1645/10000, loss: 0.0034259450621902943\n",
      "Episode Reward: 12.0\n",
      "Step 1298 (556063) @ Episode 1646/10000, loss: 0.0045337653718888767\n",
      "Episode Reward: 30.0\n",
      "Step 1083 (557146) @ Episode 1647/10000, loss: 0.0030388201121240854\n",
      "Episode Reward: 25.0\n",
      "Step 1560 (558706) @ Episode 1648/10000, loss: 0.00331409019418060845\n",
      "Episode Reward: 42.0\n",
      "Step 1014 (559720) @ Episode 1649/10000, loss: 0.0046071382239460945\n",
      "Episode Reward: 22.0\n",
      "Step 279 (559999) @ Episode 1650/10000, loss: 0.0112551208585500722\n",
      "Copied model parameters to target network.\n",
      "Step 1107 (560827) @ Episode 1650/10000, loss: 0.0020549059845507145\n",
      "Episode Reward: 29.0\n",
      "Step 1057 (561884) @ Episode 1651/10000, loss: 0.0016049031401053073\n",
      "Episode Reward: 35.0\n",
      "Step 920 (562804) @ Episode 1652/10000, loss: 0.0029899419751018286\n",
      "Episode Reward: 24.0\n",
      "Step 924 (563728) @ Episode 1653/10000, loss: 0.0019506260287016632\n",
      "Episode Reward: 24.0\n",
      "Step 1065 (564793) @ Episode 1654/10000, loss: 0.0010466413805261254\n",
      "Episode Reward: 19.0\n",
      "Step 905 (565698) @ Episode 1655/10000, loss: 0.0019696629606187344\n",
      "Episode Reward: 26.0\n",
      "Step 1299 (566997) @ Episode 1656/10000, loss: 0.0012705086264759302\n",
      "Episode Reward: 42.0\n",
      "Step 854 (567851) @ Episode 1657/10000, loss: 0.0027396925725042825\n",
      "Episode Reward: 24.0\n",
      "Step 919 (568770) @ Episode 1658/10000, loss: 0.0047532632015645524\n",
      "Episode Reward: 18.0\n",
      "Step 970 (569740) @ Episode 1659/10000, loss: 0.0038536048959940675\n",
      "Episode Reward: 22.0\n",
      "Step 259 (569999) @ Episode 1660/10000, loss: 0.0027610831893980503\n",
      "Copied model parameters to target network.\n",
      "Step 1069 (570809) @ Episode 1660/10000, loss: 0.0023060392122715715\n",
      "Episode Reward: 18.0\n",
      "Step 1567 (572376) @ Episode 1661/10000, loss: 0.0014496210496872663\n",
      "Episode Reward: 38.0\n",
      "Step 991 (573367) @ Episode 1662/10000, loss: 0.0008681891486048698\n",
      "Episode Reward: 18.0\n",
      "Step 909 (574276) @ Episode 1663/10000, loss: 0.0036718496121466165\n",
      "Episode Reward: 21.0\n",
      "Step 857 (575133) @ Episode 1664/10000, loss: 0.0083156470209360123\n",
      "Episode Reward: 26.0\n",
      "Step 897 (576030) @ Episode 1665/10000, loss: 0.0010761350858956575\n",
      "Episode Reward: 22.0\n",
      "Step 1249 (577279) @ Episode 1666/10000, loss: 0.0006655630422756076\n",
      "Episode Reward: 33.0\n",
      "Step 804 (578083) @ Episode 1667/10000, loss: 0.0114634968340396886\n",
      "Episode Reward: 15.0\n",
      "Step 1175 (579258) @ Episode 1668/10000, loss: 0.0117456894367933275\n",
      "Episode Reward: 23.0\n",
      "Step 741 (579999) @ Episode 1669/10000, loss: 0.04293592274188995484\n",
      "Copied model parameters to target network.\n",
      "Step 1248 (580506) @ Episode 1669/10000, loss: 0.0049034319818019873\n",
      "Episode Reward: 35.0\n",
      "Step 1059 (581565) @ Episode 1670/10000, loss: 0.0021222741343080997\n",
      "Episode Reward: 29.0\n",
      "Step 699 (582264) @ Episode 1671/10000, loss: 0.0051911184564232833\n",
      "Episode Reward: 11.0\n",
      "Step 1037 (583301) @ Episode 1672/10000, loss: 0.0023456483613699675\n",
      "Episode Reward: 24.0\n",
      "Step 1101 (584402) @ Episode 1673/10000, loss: 0.0012328757438808684\n",
      "Episode Reward: 29.0\n",
      "Step 916 (585318) @ Episode 1674/10000, loss: 0.0048029897734522825\n",
      "Episode Reward: 17.0\n",
      "Step 1394 (586712) @ Episode 1675/10000, loss: 0.0255760457366704944\n",
      "Episode Reward: 29.0\n",
      "Step 1322 (588034) @ Episode 1676/10000, loss: 0.0055584576912224294\n",
      "Episode Reward: 33.0\n",
      "Step 916 (588950) @ Episode 1677/10000, loss: 0.00271789426915347586\n",
      "Episode Reward: 19.0\n",
      "Step 1049 (589999) @ Episode 1678/10000, loss: 0.0024847537279129036\n",
      "Copied model parameters to target network.\n",
      "Step 1215 (590165) @ Episode 1678/10000, loss: 0.0019579394720494747\n",
      "Episode Reward: 26.0\n",
      "Step 994 (591159) @ Episode 1679/10000, loss: 0.0049826116301119339\n",
      "Episode Reward: 21.0\n",
      "Step 931 (592090) @ Episode 1680/10000, loss: 0.0048761330544948585\n",
      "Episode Reward: 15.0\n",
      "Step 772 (592862) @ Episode 1681/10000, loss: 0.01532732881605625277\n",
      "Episode Reward: 15.0\n",
      "Step 763 (593625) @ Episode 1682/10000, loss: 0.0026410233695060015\n",
      "Episode Reward: 11.0\n",
      "Step 992 (594617) @ Episode 1683/10000, loss: 0.0025899324100464582\n",
      "Episode Reward: 27.0\n",
      "Step 944 (595561) @ Episode 1684/10000, loss: 0.0119190104305744173\n",
      "Episode Reward: 31.0\n",
      "Step 1383 (596944) @ Episode 1685/10000, loss: 0.0023378159385174513\n",
      "Episode Reward: 45.0\n",
      "Step 916 (597860) @ Episode 1686/10000, loss: 0.0030006964225322015\n",
      "Episode Reward: 15.0\n",
      "Step 1107 (598967) @ Episode 1687/10000, loss: 0.0016414530109614134\n",
      "Episode Reward: 18.0\n",
      "Step 726 (599693) @ Episode 1688/10000, loss: 0.0015180690679699183\n",
      "Episode Reward: 11.0\n",
      "Step 306 (599999) @ Episode 1689/10000, loss: 0.0057444786652922637\n",
      "Copied model parameters to target network.\n",
      "Step 1486 (601179) @ Episode 1689/10000, loss: 0.0069810240529477655\n",
      "Episode Reward: 32.0\n",
      "Step 1141 (602320) @ Episode 1690/10000, loss: 0.0321169048547744756\n",
      "Episode Reward: 27.0\n",
      "Step 930 (603250) @ Episode 1691/10000, loss: 0.0201204679906368265\n",
      "Episode Reward: 28.0\n",
      "Step 1209 (604459) @ Episode 1692/10000, loss: 0.0019966135732829573\n",
      "Episode Reward: 26.0\n",
      "Step 800 (605259) @ Episode 1693/10000, loss: 0.0040696328505873682\n",
      "Episode Reward: 18.0\n",
      "Step 1558 (606817) @ Episode 1694/10000, loss: 0.0849114507436752311\n",
      "Episode Reward: 38.0\n",
      "Step 1127 (607944) @ Episode 1695/10000, loss: 0.0012252437882125378\n",
      "Episode Reward: 20.0\n",
      "Step 1459 (609403) @ Episode 1696/10000, loss: 0.0017549898475408554\n",
      "Episode Reward: 42.0\n",
      "Step 596 (609999) @ Episode 1697/10000, loss: 0.0025122638326138265\n",
      "Copied model parameters to target network.\n",
      "Step 922 (610325) @ Episode 1697/10000, loss: 0.0049135135486721997\n",
      "Episode Reward: 18.0\n",
      "Step 784 (611109) @ Episode 1698/10000, loss: 0.0009898145217448473\n",
      "Episode Reward: 15.0\n",
      "Step 939 (612048) @ Episode 1699/10000, loss: 0.0109808910638093953\n",
      "Episode Reward: 20.0\n",
      "Step 1408 (613456) @ Episode 1700/10000, loss: 0.0016299888957291842\n",
      "Episode Reward: 41.0\n",
      "Step 1150 (614606) @ Episode 1701/10000, loss: 0.0036555058322846897\n",
      "Episode Reward: 30.0\n",
      "Step 1017 (615623) @ Episode 1702/10000, loss: 0.0019908652175217867\n",
      "Episode Reward: 29.0\n",
      "Step 923 (616546) @ Episode 1703/10000, loss: 0.0011286365333944564\n",
      "Episode Reward: 22.0\n",
      "Step 1277 (617823) @ Episode 1704/10000, loss: 0.00515643600374460265\n",
      "Episode Reward: 25.0\n",
      "Step 1104 (618927) @ Episode 1705/10000, loss: 0.0033577035646885633\n",
      "Episode Reward: 21.0\n",
      "Step 687 (619614) @ Episode 1706/10000, loss: 0.0021393082570284605\n",
      "Episode Reward: 10.0\n",
      "Step 385 (619999) @ Episode 1707/10000, loss: 0.0006632466684095562\n",
      "Copied model parameters to target network.\n",
      "Step 982 (620596) @ Episode 1707/10000, loss: 0.0032791062258183956\n",
      "Episode Reward: 17.0\n",
      "Step 1091 (621687) @ Episode 1708/10000, loss: 0.0028356059920042753\n",
      "Episode Reward: 20.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1338 (623025) @ Episode 1709/10000, loss: 0.0039037824608385563\n",
      "Episode Reward: 31.0\n",
      "Step 1058 (624083) @ Episode 1710/10000, loss: 0.0032761848997324705\n",
      "Episode Reward: 26.0\n",
      "Step 736 (624819) @ Episode 1711/10000, loss: 0.0093408655375242238\n",
      "Episode Reward: 13.0\n",
      "Step 1572 (626391) @ Episode 1712/10000, loss: 0.0011631271336227655\n",
      "Episode Reward: 48.0\n",
      "Step 1731 (628122) @ Episode 1713/10000, loss: 0.0053306547924876212\n",
      "Episode Reward: 48.0\n",
      "Step 1008 (629130) @ Episode 1714/10000, loss: 0.0020680918823927644\n",
      "Episode Reward: 22.0\n",
      "Step 869 (629999) @ Episode 1715/10000, loss: 0.0035674385726451874\n",
      "Copied model parameters to target network.\n",
      "Step 1269 (630399) @ Episode 1715/10000, loss: 0.0117912162095308395\n",
      "Episode Reward: 29.0\n",
      "Step 1004 (631403) @ Episode 1716/10000, loss: 0.0015997511800378564\n",
      "Episode Reward: 25.0\n",
      "Step 1232 (632635) @ Episode 1717/10000, loss: 0.0026881222147494555\n",
      "Episode Reward: 31.0\n",
      "Step 734 (633369) @ Episode 1718/10000, loss: 0.0014454751508310437\n",
      "Episode Reward: 13.0\n",
      "Step 1054 (634423) @ Episode 1719/10000, loss: 0.0034180209040641785\n",
      "Episode Reward: 28.0\n",
      "Step 1380 (635803) @ Episode 1720/10000, loss: 0.00222273683175444666\n",
      "Episode Reward: 32.0\n",
      "Step 1410 (637213) @ Episode 1721/10000, loss: 0.0205675028264522559\n",
      "Episode Reward: 40.0\n",
      "Step 870 (638083) @ Episode 1722/10000, loss: 0.0015776377404108644\n",
      "Episode Reward: 17.0\n",
      "Step 1236 (639319) @ Episode 1723/10000, loss: 0.00349846179597079755\n",
      "Episode Reward: 29.0\n",
      "Step 680 (639999) @ Episode 1724/10000, loss: 0.00539063382893800784\n",
      "Copied model parameters to target network.\n",
      "Step 1066 (640385) @ Episode 1724/10000, loss: 0.0015880779828876257\n",
      "Episode Reward: 17.0\n",
      "Step 786 (641171) @ Episode 1725/10000, loss: 0.0052346526645123969\n",
      "Episode Reward: 14.0\n",
      "Step 1178 (642349) @ Episode 1726/10000, loss: 0.0028654183261096478\n",
      "Episode Reward: 26.0\n",
      "Step 1013 (643362) @ Episode 1727/10000, loss: 0.0015964701306074858\n",
      "Episode Reward: 17.0\n",
      "Step 1374 (644736) @ Episode 1728/10000, loss: 0.0018757516518235207\n",
      "Episode Reward: 36.0\n",
      "Step 1216 (645952) @ Episode 1729/10000, loss: 0.0040854401886463165\n",
      "Episode Reward: 30.0\n",
      "Step 1115 (647067) @ Episode 1730/10000, loss: 0.0015880775172263384\n",
      "Episode Reward: 24.0\n",
      "Step 1318 (648385) @ Episode 1731/10000, loss: 0.0019954550080001354\n",
      "Episode Reward: 31.0\n",
      "Step 1274 (649659) @ Episode 1732/10000, loss: 0.0169143956154584936\n",
      "Episode Reward: 34.0\n",
      "Step 340 (649999) @ Episode 1733/10000, loss: 0.0021133623085916042\n",
      "Copied model parameters to target network.\n",
      "Step 792 (650451) @ Episode 1733/10000, loss: 0.0029276204295456415\n",
      "Episode Reward: 17.0\n",
      "Step 948 (651399) @ Episode 1734/10000, loss: 0.0029135979712009438\n",
      "Episode Reward: 27.0\n",
      "Step 1024 (652423) @ Episode 1735/10000, loss: 0.0021218787878751755\n",
      "Episode Reward: 21.0\n",
      "Step 1308 (653731) @ Episode 1736/10000, loss: 0.0086820190772414235\n",
      "Episode Reward: 29.0\n",
      "Step 808 (654539) @ Episode 1737/10000, loss: 0.0039592417888343338\n",
      "Episode Reward: 28.0\n",
      "Step 964 (655503) @ Episode 1738/10000, loss: 0.0041703288443386555\n",
      "Episode Reward: 16.0\n",
      "Step 1126 (656629) @ Episode 1739/10000, loss: 0.0122905764728784564\n",
      "Episode Reward: 28.0\n",
      "Step 1272 (657901) @ Episode 1740/10000, loss: 0.0018340602982789278\n",
      "Episode Reward: 36.0\n",
      "Step 1124 (659025) @ Episode 1741/10000, loss: 0.0125381248071789741\n",
      "Episode Reward: 21.0\n",
      "Step 952 (659977) @ Episode 1742/10000, loss: 0.0041855461895465853\n",
      "Episode Reward: 26.0\n",
      "Step 22 (659999) @ Episode 1743/10000, loss: 0.0252404455095529567\n",
      "Copied model parameters to target network.\n",
      "Step 703 (660680) @ Episode 1743/10000, loss: 0.0025665841531008483\n",
      "Episode Reward: 14.0\n",
      "Step 1134 (661814) @ Episode 1744/10000, loss: 0.0093373805284500123\n",
      "Episode Reward: 27.0\n",
      "Step 1071 (662885) @ Episode 1745/10000, loss: 0.0040973471477627753\n",
      "Episode Reward: 23.0\n",
      "Step 844 (663729) @ Episode 1746/10000, loss: 0.0023946519941091537\n",
      "Episode Reward: 16.0\n",
      "Step 929 (664658) @ Episode 1747/10000, loss: 0.0027769079897552738\n",
      "Episode Reward: 15.0\n",
      "Step 847 (665505) @ Episode 1748/10000, loss: 0.0041132485494017657\n",
      "Episode Reward: 16.0\n",
      "Step 1026 (666531) @ Episode 1749/10000, loss: 0.0010014867875725033\n",
      "Episode Reward: 21.0\n",
      "Step 936 (667467) @ Episode 1750/10000, loss: 0.0019295501988381147\n",
      "Episode Reward: 21.0\n",
      "Step 1137 (668604) @ Episode 1751/10000, loss: 0.0036533616948872805\n",
      "Episode Reward: 19.0\n",
      "Step 1114 (669718) @ Episode 1752/10000, loss: 0.0017142897704616194\n",
      "Episode Reward: 38.0\n",
      "Step 281 (669999) @ Episode 1753/10000, loss: 0.0030915187671780586\n",
      "Copied model parameters to target network.\n",
      "Step 947 (670665) @ Episode 1753/10000, loss: 0.0022611543536186222\n",
      "Episode Reward: 16.0\n",
      "Step 829 (671494) @ Episode 1754/10000, loss: 0.0030727596022188663\n",
      "Episode Reward: 15.0\n",
      "Step 908 (672402) @ Episode 1755/10000, loss: 0.0073194345459342044\n",
      "Episode Reward: 18.0\n",
      "Step 772 (673174) @ Episode 1756/10000, loss: 0.0063613979145884518\n",
      "Episode Reward: 13.0\n",
      "Step 1026 (674200) @ Episode 1757/10000, loss: 0.0041450322605669513\n",
      "Episode Reward: 27.0\n",
      "Step 1058 (675258) @ Episode 1758/10000, loss: 0.0219420772045850758\n",
      "Episode Reward: 19.0\n",
      "Step 917 (676175) @ Episode 1759/10000, loss: 0.0070601822808384895\n",
      "Episode Reward: 23.0\n",
      "Step 859 (677034) @ Episode 1760/10000, loss: 0.0029864429961889982\n",
      "Episode Reward: 16.0\n",
      "Step 1249 (678283) @ Episode 1761/10000, loss: 0.0030049397610127926\n",
      "Episode Reward: 24.0\n",
      "Step 869 (679152) @ Episode 1762/10000, loss: 0.0025771865621209145\n",
      "Episode Reward: 20.0\n",
      "Step 847 (679999) @ Episode 1763/10000, loss: 0.0018252745503559709\n",
      "Copied model parameters to target network.\n",
      "Step 986 (680138) @ Episode 1763/10000, loss: 0.0065017854794859895\n",
      "Episode Reward: 21.0\n",
      "Step 998 (681136) @ Episode 1764/10000, loss: 0.0021399364341050386\n",
      "Episode Reward: 25.0\n",
      "Step 981 (682117) @ Episode 1765/10000, loss: 0.0019959246274083853\n",
      "Episode Reward: 22.0\n",
      "Step 952 (683069) @ Episode 1766/10000, loss: 0.0055701201781630526\n",
      "Episode Reward: 15.0\n",
      "Step 931 (684000) @ Episode 1767/10000, loss: 0.0363738611340522847\n",
      "Episode Reward: 19.0\n",
      "Step 1021 (685021) @ Episode 1768/10000, loss: 0.0200170762836933146\n",
      "Episode Reward: 21.0\n",
      "Step 801 (685822) @ Episode 1769/10000, loss: 0.0047176829539239418\n",
      "Episode Reward: 12.0\n",
      "Step 940 (686762) @ Episode 1770/10000, loss: 0.0020805639214813715\n",
      "Episode Reward: 14.0\n",
      "Step 773 (687535) @ Episode 1771/10000, loss: 0.0059147388674318795\n",
      "Episode Reward: 18.0\n",
      "Step 1092 (688627) @ Episode 1772/10000, loss: 0.0037350442726165056\n",
      "Episode Reward: 19.0\n",
      "Step 1109 (689736) @ Episode 1773/10000, loss: 0.0067517221905291087\n",
      "Episode Reward: 20.0\n",
      "Step 263 (689999) @ Episode 1774/10000, loss: 0.0033033359795808796\n",
      "Copied model parameters to target network.\n",
      "Step 755 (690491) @ Episode 1774/10000, loss: 0.0034566365648061037\n",
      "Episode Reward: 11.0\n",
      "Step 938 (691429) @ Episode 1775/10000, loss: 0.0088532958179712347\n",
      "Episode Reward: 15.0\n",
      "Step 984 (692413) @ Episode 1776/10000, loss: 0.0017370615387335422\n",
      "Episode Reward: 20.0\n",
      "Step 806 (693219) @ Episode 1777/10000, loss: 0.0032011144794523716\n",
      "Episode Reward: 12.0\n",
      "Step 910 (694129) @ Episode 1778/10000, loss: 0.0030858102254569532\n",
      "Episode Reward: 16.0\n",
      "Step 711 (694840) @ Episode 1779/10000, loss: 0.0040290472097694876\n",
      "Episode Reward: 14.0\n",
      "Step 1574 (696414) @ Episode 1780/10000, loss: 0.0050473115406930455\n",
      "Episode Reward: 37.0\n",
      "Step 1029 (697443) @ Episode 1781/10000, loss: 0.0037168667186051607\n",
      "Episode Reward: 28.0\n",
      "Step 905 (698348) @ Episode 1782/10000, loss: 0.0057979263365268714\n",
      "Episode Reward: 18.0\n",
      "Step 1195 (699543) @ Episode 1783/10000, loss: 0.0185982324182987266\n",
      "Episode Reward: 21.0\n",
      "Step 456 (699999) @ Episode 1784/10000, loss: 0.0012887731427326798\n",
      "Copied model parameters to target network.\n",
      "Step 995 (700538) @ Episode 1784/10000, loss: 0.0041232090443372734\n",
      "Episode Reward: 29.0\n",
      "Step 977 (701515) @ Episode 1785/10000, loss: 0.0076140775345265865\n",
      "Episode Reward: 18.0\n",
      "Step 676 (702191) @ Episode 1786/10000, loss: 0.0019895026925951242\n",
      "Episode Reward: 18.0\n",
      "Step 780 (702971) @ Episode 1787/10000, loss: 0.0023838344495743513\n",
      "Episode Reward: 12.0\n",
      "Step 828 (703799) @ Episode 1788/10000, loss: 0.0046350033953785934\n",
      "Episode Reward: 12.0\n",
      "Step 1385 (705184) @ Episode 1789/10000, loss: 0.0056114215403795247\n",
      "Episode Reward: 30.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1391 (706575) @ Episode 1790/10000, loss: 0.0027190535329282284\n",
      "Episode Reward: 30.0\n",
      "Step 715 (707290) @ Episode 1791/10000, loss: 0.0028423529583960777\n",
      "Episode Reward: 15.0\n",
      "Step 1032 (708322) @ Episode 1792/10000, loss: 0.0061646727845072754\n",
      "Episode Reward: 21.0\n",
      "Step 635 (708957) @ Episode 1793/10000, loss: 0.0235106740146875386\n",
      "Episode Reward: 9.0\n",
      "Step 700 (709657) @ Episode 1794/10000, loss: 0.0115449484437704096\n",
      "Episode Reward: 10.0\n",
      "Step 342 (709999) @ Episode 1795/10000, loss: 0.0043508857488632283\n",
      "Copied model parameters to target network.\n",
      "Step 750 (710407) @ Episode 1795/10000, loss: 0.0031883078627288346\n",
      "Episode Reward: 11.0\n",
      "Step 749 (711156) @ Episode 1796/10000, loss: 0.0230229478329420115\n",
      "Episode Reward: 12.0\n",
      "Step 661 (711817) @ Episode 1797/10000, loss: 0.0166098140180110934\n",
      "Episode Reward: 10.0\n",
      "Step 1006 (712823) @ Episode 1798/10000, loss: 0.0034145091194659474\n",
      "Episode Reward: 16.0\n",
      "Step 943 (713766) @ Episode 1799/10000, loss: 0.0037812206428498035\n",
      "Episode Reward: 15.0\n",
      "Step 1060 (714826) @ Episode 1800/10000, loss: 0.0038110637106001377\n",
      "Episode Reward: 22.0\n",
      "Step 1130 (715956) @ Episode 1801/10000, loss: 0.0043017147108912474\n",
      "Episode Reward: 20.0\n",
      "Step 1266 (717222) @ Episode 1802/10000, loss: 0.0146955372765660296\n",
      "Episode Reward: 38.0\n",
      "Step 1270 (718492) @ Episode 1803/10000, loss: 0.0040904916822910316\n",
      "Episode Reward: 25.0\n",
      "Step 1070 (719562) @ Episode 1804/10000, loss: 0.0062080705538392074\n",
      "Episode Reward: 20.0\n",
      "Step 437 (719999) @ Episode 1805/10000, loss: 0.0071592829190194619\n",
      "Copied model parameters to target network.\n",
      "Step 640 (720202) @ Episode 1805/10000, loss: 0.0047796806320548063\n",
      "Episode Reward: 13.0\n",
      "Step 1299 (721501) @ Episode 1806/10000, loss: 0.0038750884123146534\n",
      "Episode Reward: 32.0\n",
      "Step 1332 (722833) @ Episode 1807/10000, loss: 0.0215086601674556733\n",
      "Episode Reward: 26.0\n",
      "Step 802 (723635) @ Episode 1808/10000, loss: 0.0078605934977531438\n",
      "Episode Reward: 12.0\n",
      "Step 743 (724378) @ Episode 1809/10000, loss: 0.0191965326666831976\n",
      "Episode Reward: 11.0\n",
      "Step 1014 (725392) @ Episode 1810/10000, loss: 0.0028357862029224634\n",
      "Episode Reward: 21.0\n",
      "Step 1001 (726393) @ Episode 1811/10000, loss: 0.0017595263198018074\n",
      "Episode Reward: 19.0\n",
      "Step 954 (727347) @ Episode 1812/10000, loss: 0.0031648078002035618\n",
      "Episode Reward: 23.0\n",
      "Step 1064 (728411) @ Episode 1813/10000, loss: 0.0031312524806708097\n",
      "Episode Reward: 25.0\n",
      "Step 744 (729155) @ Episode 1814/10000, loss: 0.0030660121701657777\n",
      "Episode Reward: 12.0\n",
      "Step 844 (729999) @ Episode 1815/10000, loss: 0.0045209643431007866\n",
      "Copied model parameters to target network.\n",
      "Step 1054 (730209) @ Episode 1815/10000, loss: 0.0022887771483510733\n",
      "Episode Reward: 16.0\n",
      "Step 1165 (731374) @ Episode 1816/10000, loss: 0.0083237327635288241\n",
      "Episode Reward: 23.0\n",
      "Step 1207 (732581) @ Episode 1817/10000, loss: 0.0203910619020462046\n",
      "Episode Reward: 24.0\n",
      "Step 837 (733418) @ Episode 1818/10000, loss: 0.0024633263237774375\n",
      "Episode Reward: 17.0\n",
      "Step 940 (734358) @ Episode 1819/10000, loss: 0.0031228289008140564\n",
      "Episode Reward: 17.0\n",
      "Step 665 (735023) @ Episode 1820/10000, loss: 0.0256147328764200266\n",
      "Episode Reward: 15.0\n",
      "Step 1215 (736238) @ Episode 1821/10000, loss: 0.0098279938101768553\n",
      "Episode Reward: 22.0\n",
      "Step 915 (737153) @ Episode 1822/10000, loss: 0.0032382966019213276\n",
      "Episode Reward: 17.0\n",
      "Step 700 (737853) @ Episode 1823/10000, loss: 0.0023641732987016446\n",
      "Episode Reward: 10.0\n",
      "Step 1128 (738981) @ Episode 1824/10000, loss: 0.0062137409113347538\n",
      "Episode Reward: 26.0\n",
      "Step 1018 (739999) @ Episode 1825/10000, loss: 0.0087568704038858418\n",
      "Copied model parameters to target network.\n",
      "Step 1063 (740044) @ Episode 1825/10000, loss: 0.0029682707972824574\n",
      "Episode Reward: 28.0\n",
      "Step 673 (740717) @ Episode 1826/10000, loss: 0.0056533077731728555\n",
      "Episode Reward: 10.0\n",
      "Step 707 (741424) @ Episode 1827/10000, loss: 0.0075138933025300588\n",
      "Episode Reward: 11.0\n",
      "Step 1027 (742451) @ Episode 1828/10000, loss: 0.0011678194859996438\n",
      "Episode Reward: 20.0\n",
      "Step 940 (743391) @ Episode 1829/10000, loss: 0.0027649863623082638\n",
      "Episode Reward: 23.0\n",
      "Step 1028 (744419) @ Episode 1830/10000, loss: 0.0067986100912094123\n",
      "Episode Reward: 21.0\n",
      "Step 936 (745355) @ Episode 1831/10000, loss: 0.0053360033780336383\n",
      "Episode Reward: 18.0\n",
      "Step 924 (746279) @ Episode 1832/10000, loss: 0.0037928405217826366\n",
      "Episode Reward: 13.0\n",
      "Step 1081 (747360) @ Episode 1833/10000, loss: 0.0073798806406557562\n",
      "Episode Reward: 21.0\n",
      "Step 1060 (748420) @ Episode 1834/10000, loss: 0.0012674588942900336\n",
      "Episode Reward: 29.0\n",
      "Step 978 (749398) @ Episode 1835/10000, loss: 0.0188141018152236946\n",
      "Episode Reward: 19.0\n",
      "Step 601 (749999) @ Episode 1836/10000, loss: 0.0018358411034569144\n",
      "Copied model parameters to target network.\n",
      "Step 845 (750243) @ Episode 1836/10000, loss: 0.0084525672718882565\n",
      "Episode Reward: 14.0\n",
      "Step 970 (751213) @ Episode 1837/10000, loss: 0.0065888785757124423\n",
      "Episode Reward: 20.0\n",
      "Step 1447 (752660) @ Episode 1838/10000, loss: 0.0056635066866874695\n",
      "Episode Reward: 31.0\n",
      "Step 966 (753626) @ Episode 1839/10000, loss: 0.0126597862690687185\n",
      "Episode Reward: 13.0\n",
      "Step 656 (754282) @ Episode 1840/10000, loss: 0.0114794475957751275\n",
      "Episode Reward: 9.0\n",
      "Step 537 (754819) @ Episode 1841/10000, loss: 0.0041029630228877072\n",
      "Episode Reward: 7.0\n",
      "Step 649 (755468) @ Episode 1842/10000, loss: 0.0092452028766274455\n",
      "Episode Reward: 13.0\n",
      "Step 1279 (756747) @ Episode 1843/10000, loss: 0.0100666582584381164\n",
      "Episode Reward: 27.0\n",
      "Step 740 (757487) @ Episode 1844/10000, loss: 0.0127579821273684536\n",
      "Episode Reward: 15.0\n",
      "Step 942 (758429) @ Episode 1845/10000, loss: 0.0057033104822039694\n",
      "Episode Reward: 20.0\n",
      "Step 942 (759371) @ Episode 1846/10000, loss: 0.0029399779159575796\n",
      "Episode Reward: 20.0\n",
      "Step 467 (759838) @ Episode 1847/10000, loss: 0.0045781582593917856\n",
      "Episode Reward: 6.0\n",
      "Step 161 (759999) @ Episode 1848/10000, loss: 0.0029506497085094455\n",
      "Copied model parameters to target network.\n",
      "Step 1077 (760915) @ Episode 1848/10000, loss: 0.0344632044434547473\n",
      "Episode Reward: 25.0\n",
      "Step 908 (761823) @ Episode 1849/10000, loss: 0.0031868554651737213\n",
      "Episode Reward: 19.0\n",
      "Step 703 (762526) @ Episode 1850/10000, loss: 0.0171283483505249023\n",
      "Episode Reward: 21.0\n",
      "Step 1512 (764038) @ Episode 1851/10000, loss: 0.0081168133765459065\n",
      "Episode Reward: 38.0\n",
      "Step 783 (764821) @ Episode 1852/10000, loss: 0.0045236567966639996\n",
      "Episode Reward: 13.0\n",
      "Step 1120 (765941) @ Episode 1853/10000, loss: 0.0077649736776947975\n",
      "Episode Reward: 27.0\n",
      "Step 903 (766844) @ Episode 1854/10000, loss: 0.0081096924841403967\n",
      "Episode Reward: 15.0\n",
      "Step 993 (767837) @ Episode 1855/10000, loss: 0.0070673646405339246\n",
      "Episode Reward: 26.0\n",
      "Step 1318 (769155) @ Episode 1856/10000, loss: 0.0059285582974553115\n",
      "Episode Reward: 30.0\n",
      "Step 844 (769999) @ Episode 1857/10000, loss: 0.0049020121805369858\n",
      "Copied model parameters to target network.\n",
      "Step 1327 (770482) @ Episode 1857/10000, loss: 0.0130885718390345576\n",
      "Episode Reward: 32.0\n",
      "Step 1153 (771635) @ Episode 1858/10000, loss: 0.0031211539171636105\n",
      "Episode Reward: 18.0\n",
      "Step 1034 (772669) @ Episode 1859/10000, loss: 0.0099833868443965914\n",
      "Episode Reward: 18.0\n",
      "Step 1714 (774383) @ Episode 1860/10000, loss: 0.0052974047139286995\n",
      "Episode Reward: 39.0\n",
      "Step 939 (775322) @ Episode 1861/10000, loss: 0.0031487229280173785\n",
      "Episode Reward: 19.0\n",
      "Step 1175 (776497) @ Episode 1862/10000, loss: 0.0051043415442109116\n",
      "Episode Reward: 24.0\n",
      "Step 1440 (777937) @ Episode 1863/10000, loss: 0.0092906169593334278\n",
      "Episode Reward: 28.0\n",
      "Step 598 (778535) @ Episode 1864/10000, loss: 0.0077533773146569735\n",
      "Episode Reward: 8.0\n",
      "Step 1054 (779589) @ Episode 1865/10000, loss: 0.0051517570391297345\n",
      "Episode Reward: 22.0\n",
      "Step 410 (779999) @ Episode 1866/10000, loss: 0.0054821036756038674\n",
      "Copied model parameters to target network.\n",
      "Step 798 (780387) @ Episode 1866/10000, loss: 0.0108901914209127435\n",
      "Episode Reward: 19.0\n",
      "Step 936 (781323) @ Episode 1867/10000, loss: 0.0036306907422840595\n",
      "Episode Reward: 19.0\n",
      "Step 1041 (782364) @ Episode 1868/10000, loss: 0.0027323705144226553\n",
      "Episode Reward: 17.0\n",
      "Step 811 (783175) @ Episode 1869/10000, loss: 0.0060759778134524827\n",
      "Episode Reward: 14.0\n",
      "Step 1004 (784179) @ Episode 1870/10000, loss: 0.0099486159160733224\n",
      "Episode Reward: 25.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1555 (785734) @ Episode 1871/10000, loss: 0.0097231958061456685\n",
      "Episode Reward: 37.0\n",
      "Step 581 (786315) @ Episode 1872/10000, loss: 0.0074401483871042736\n",
      "Episode Reward: 8.0\n",
      "Step 896 (787211) @ Episode 1873/10000, loss: 0.0033885159064084297\n",
      "Episode Reward: 18.0\n",
      "Step 1737 (788948) @ Episode 1874/10000, loss: 0.0090072546154260644\n",
      "Episode Reward: 40.0\n",
      "Step 1051 (789999) @ Episode 1875/10000, loss: 0.0048118671402335175\n",
      "Copied model parameters to target network.\n",
      "Step 1115 (790063) @ Episode 1875/10000, loss: 0.0110495369881391532\n",
      "Episode Reward: 28.0\n",
      "Step 1064 (791127) @ Episode 1876/10000, loss: 0.0137327732518315326\n",
      "Episode Reward: 27.0\n",
      "Step 679 (791806) @ Episode 1877/10000, loss: 0.0083111636340618135\n",
      "Episode Reward: 11.0\n",
      "Step 918 (792724) @ Episode 1878/10000, loss: 0.0182861238718032845\n",
      "Episode Reward: 20.0\n",
      "Step 1023 (793747) @ Episode 1879/10000, loss: 0.0183930993080139165\n",
      "Episode Reward: 24.0\n",
      "Step 844 (794591) @ Episode 1880/10000, loss: 0.0118347331881523135\n",
      "Episode Reward: 13.0\n",
      "Step 903 (795494) @ Episode 1881/10000, loss: 0.0044199922122061254\n",
      "Episode Reward: 23.0\n",
      "Step 1142 (796636) @ Episode 1882/10000, loss: 0.0054902196861803535\n",
      "Episode Reward: 19.0\n",
      "Step 943 (797579) @ Episode 1883/10000, loss: 0.0054792235605418685\n",
      "Episode Reward: 17.0\n",
      "Step 947 (798526) @ Episode 1884/10000, loss: 0.0058206962421536446\n",
      "Episode Reward: 16.0\n",
      "Step 1116 (799642) @ Episode 1885/10000, loss: 0.0146167976781725884\n",
      "Episode Reward: 27.0\n",
      "Step 357 (799999) @ Episode 1886/10000, loss: 0.0130851324647665025\n",
      "Copied model parameters to target network.\n",
      "Step 672 (800314) @ Episode 1886/10000, loss: 0.0115087358281016355\n",
      "Episode Reward: 18.0\n",
      "Step 1134 (801448) @ Episode 1887/10000, loss: 0.0131592657417058948\n",
      "Episode Reward: 22.0\n",
      "Step 864 (802312) @ Episode 1888/10000, loss: 0.0045558717101812367\n",
      "Episode Reward: 16.0\n",
      "Step 1021 (803333) @ Episode 1889/10000, loss: 0.0303606502711772925\n",
      "Episode Reward: 18.0\n",
      "Step 896 (804229) @ Episode 1890/10000, loss: 0.0233436189591884677\n",
      "Episode Reward: 15.0\n",
      "Step 1085 (805314) @ Episode 1891/10000, loss: 0.0078376010060310367\n",
      "Episode Reward: 23.0\n",
      "Step 708 (806022) @ Episode 1892/10000, loss: 0.0086034536361694347\n",
      "Episode Reward: 15.0\n",
      "Step 1157 (807179) @ Episode 1893/10000, loss: 0.0082197654992342458\n",
      "Episode Reward: 27.0\n",
      "Step 1111 (808290) @ Episode 1894/10000, loss: 0.0177417397499084473\n",
      "Episode Reward: 22.0\n",
      "Step 727 (809017) @ Episode 1895/10000, loss: 0.0360632762312889145\n",
      "Episode Reward: 16.0\n",
      "Step 781 (809798) @ Episode 1896/10000, loss: 0.0048061730340123185\n",
      "Episode Reward: 12.0\n",
      "Step 201 (809999) @ Episode 1897/10000, loss: 0.0120058190077543264\n",
      "Copied model parameters to target network.\n",
      "Step 903 (810701) @ Episode 1897/10000, loss: 0.0069061694666743285\n",
      "Episode Reward: 14.0\n",
      "Step 761 (811462) @ Episode 1898/10000, loss: 0.0058001512661576276\n",
      "Episode Reward: 20.0\n",
      "Step 890 (812352) @ Episode 1899/10000, loss: 0.0216666329652071684\n",
      "Episode Reward: 15.0\n",
      "Step 1096 (813448) @ Episode 1900/10000, loss: 0.0033982521854341036\n",
      "Episode Reward: 21.0\n",
      "Step 852 (814300) @ Episode 1901/10000, loss: 0.0271229092031717326\n",
      "Episode Reward: 15.0\n",
      "Step 838 (815138) @ Episode 1902/10000, loss: 0.0220716781914234165\n",
      "Episode Reward: 17.0\n",
      "Step 1233 (816371) @ Episode 1903/10000, loss: 0.0082563599571585662\n",
      "Episode Reward: 30.0\n",
      "Step 740 (817111) @ Episode 1904/10000, loss: 0.0188747458159923555\n",
      "Episode Reward: 12.0\n",
      "Step 1030 (818141) @ Episode 1905/10000, loss: 0.0051760165952146056\n",
      "Episode Reward: 28.0\n",
      "Step 816 (818957) @ Episode 1906/10000, loss: 0.0232446528971195225\n",
      "Episode Reward: 15.0\n",
      "Step 1024 (819981) @ Episode 1907/10000, loss: 0.0150795280933380135\n",
      "Episode Reward: 24.0\n",
      "Step 18 (819999) @ Episode 1908/10000, loss: 0.0037072591949254274\n",
      "Copied model parameters to target network.\n",
      "Step 805 (820786) @ Episode 1908/10000, loss: 0.0051997024565935135\n",
      "Episode Reward: 12.0\n",
      "Step 1317 (822103) @ Episode 1909/10000, loss: 0.0021571689285337925\n",
      "Episode Reward: 29.0\n",
      "Step 1172 (823275) @ Episode 1910/10000, loss: 0.0079660126939415934\n",
      "Episode Reward: 28.0\n",
      "Step 873 (824148) @ Episode 1911/10000, loss: 0.0360129475593566924\n",
      "Episode Reward: 15.0\n",
      "Step 1784 (825932) @ Episode 1912/10000, loss: 0.0391415767371654573\n",
      "Episode Reward: 54.0\n",
      "Step 1393 (827325) @ Episode 1913/10000, loss: 0.0061673177406191834\n",
      "Episode Reward: 34.0\n",
      "Step 1245 (828570) @ Episode 1914/10000, loss: 0.0138858156278729445\n",
      "Episode Reward: 28.0\n",
      "Step 1132 (829702) @ Episode 1915/10000, loss: 0.0115989577025175135\n",
      "Episode Reward: 24.0\n",
      "Step 297 (829999) @ Episode 1916/10000, loss: 0.0276748836040496837\n",
      "Copied model parameters to target network.\n",
      "Step 862 (830564) @ Episode 1916/10000, loss: 0.0055579477921128274\n",
      "Episode Reward: 13.0\n",
      "Step 907 (831471) @ Episode 1917/10000, loss: 0.0783402472734451373\n",
      "Episode Reward: 22.0\n",
      "Step 882 (832353) @ Episode 1918/10000, loss: 0.0359906032681465154\n",
      "Episode Reward: 14.0\n",
      "Step 1107 (833460) @ Episode 1919/10000, loss: 0.0065293358638882645\n",
      "Episode Reward: 22.0\n",
      "Step 938 (834398) @ Episode 1920/10000, loss: 0.0262801107019186026\n",
      "Episode Reward: 23.0\n",
      "Step 1365 (835763) @ Episode 1921/10000, loss: 0.0203587803989648826\n",
      "Episode Reward: 31.0\n",
      "Step 911 (836674) @ Episode 1922/10000, loss: 0.0291082561016082766\n",
      "Episode Reward: 19.0\n",
      "Step 1030 (837704) @ Episode 1923/10000, loss: 0.017218202352523804\n",
      "Episode Reward: 25.0\n",
      "Step 1033 (838737) @ Episode 1924/10000, loss: 0.0087430356070399283\n",
      "Episode Reward: 19.0\n",
      "Step 1005 (839742) @ Episode 1925/10000, loss: 0.0053572217002511024\n",
      "Episode Reward: 26.0\n",
      "Step 257 (839999) @ Episode 1926/10000, loss: 0.0217429827898740777\n",
      "Copied model parameters to target network.\n",
      "Step 940 (840682) @ Episode 1926/10000, loss: 0.0095437122508883485\n",
      "Episode Reward: 20.0\n",
      "Step 1019 (841701) @ Episode 1927/10000, loss: 0.019556181505322456\n",
      "Episode Reward: 24.0\n",
      "Step 716 (842417) @ Episode 1928/10000, loss: 0.0133554488420486455\n",
      "Episode Reward: 11.0\n",
      "Step 674 (843091) @ Episode 1929/10000, loss: 0.0115416925400495535\n",
      "Episode Reward: 21.0\n",
      "Step 892 (843983) @ Episode 1930/10000, loss: 0.0036559361033141613\n",
      "Episode Reward: 19.0\n",
      "Step 1201 (845184) @ Episode 1931/10000, loss: 0.0199466627091169365\n",
      "Episode Reward: 25.0\n",
      "Step 1023 (846207) @ Episode 1932/10000, loss: 0.013128980994224548\n",
      "Episode Reward: 27.0\n",
      "Step 1054 (847261) @ Episode 1933/10000, loss: 0.0082819005474448226\n",
      "Episode Reward: 19.0\n",
      "Step 931 (848192) @ Episode 1934/10000, loss: 0.0209420472383499156\n",
      "Episode Reward: 19.0\n",
      "Step 1807 (849999) @ Episode 1935/10000, loss: 0.0163861848413944245\n",
      "Copied model parameters to target network.\n",
      "Step 1842 (850034) @ Episode 1935/10000, loss: 0.0061054611578583725\n",
      "Episode Reward: 50.0\n",
      "Step 956 (850990) @ Episode 1936/10000, loss: 0.0134513052180409434\n",
      "Episode Reward: 25.0\n",
      "Step 1074 (852064) @ Episode 1937/10000, loss: 0.0119080189615488055\n",
      "Episode Reward: 29.0\n",
      "Step 921 (852985) @ Episode 1938/10000, loss: 0.0398314036428928445\n",
      "Episode Reward: 17.0\n",
      "Step 956 (853941) @ Episode 1939/10000, loss: 0.0103239212185144425\n",
      "Episode Reward: 20.0\n",
      "Step 987 (854928) @ Episode 1940/10000, loss: 0.0227907579392194755\n",
      "Episode Reward: 20.0\n",
      "Step 866 (855794) @ Episode 1941/10000, loss: 0.0103290546685457235\n",
      "Episode Reward: 14.0\n",
      "Step 963 (856757) @ Episode 1942/10000, loss: 0.0092669362202286725\n",
      "Episode Reward: 17.0\n",
      "Step 1168 (857925) @ Episode 1943/10000, loss: 0.0118040433153510176\n",
      "Episode Reward: 23.0\n",
      "Step 881 (858806) @ Episode 1944/10000, loss: 0.0060817813500761995\n",
      "Episode Reward: 16.0\n",
      "Step 1109 (859915) @ Episode 1945/10000, loss: 0.0168637391179800036\n",
      "Episode Reward: 27.0\n",
      "Step 84 (859999) @ Episode 1946/10000, loss: 0.0105372890830039984\n",
      "Copied model parameters to target network.\n",
      "Step 1157 (861072) @ Episode 1946/10000, loss: 0.0136731015518307695\n",
      "Episode Reward: 24.0\n",
      "Step 1162 (862234) @ Episode 1947/10000, loss: 0.0053618750534951696\n",
      "Episode Reward: 30.0\n",
      "Step 1866 (864100) @ Episode 1948/10000, loss: 0.0091737117618322375\n",
      "Episode Reward: 41.0\n",
      "Step 1461 (865561) @ Episode 1949/10000, loss: 0.0097491526976227767\n",
      "Episode Reward: 45.0\n",
      "Step 1219 (866780) @ Episode 1950/10000, loss: 0.0031168342102319875\n",
      "Episode Reward: 36.0\n",
      "Step 1313 (868093) @ Episode 1951/10000, loss: 0.0095080938190221795\n",
      "Episode Reward: 27.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1104 (869197) @ Episode 1952/10000, loss: 0.0156878922134637835\n",
      "Episode Reward: 24.0\n",
      "Step 802 (869999) @ Episode 1953/10000, loss: 0.0066335103474557425\n",
      "Copied model parameters to target network.\n",
      "Step 1110 (870307) @ Episode 1953/10000, loss: 0.0129120619967579844\n",
      "Episode Reward: 20.0\n",
      "Step 799 (871106) @ Episode 1954/10000, loss: 0.0124769955873489386\n",
      "Episode Reward: 20.0\n",
      "Step 1430 (872536) @ Episode 1955/10000, loss: 0.0392404235899448445\n",
      "Episode Reward: 45.0\n",
      "Step 1355 (873891) @ Episode 1956/10000, loss: 0.0063215019181370735\n",
      "Episode Reward: 31.0\n",
      "Step 1367 (875258) @ Episode 1957/10000, loss: 0.0191338881850242675\n",
      "Episode Reward: 29.0\n",
      "Step 1284 (876542) @ Episode 1958/10000, loss: 0.0095322476699948315\n",
      "Episode Reward: 27.0\n",
      "Step 863 (877405) @ Episode 1959/10000, loss: 0.0090195816010236745\n",
      "Episode Reward: 15.0\n",
      "Step 927 (878332) @ Episode 1960/10000, loss: 0.0044681760482490065\n",
      "Episode Reward: 21.0\n",
      "Step 953 (879285) @ Episode 1961/10000, loss: 0.0269709341228008275\n",
      "Episode Reward: 23.0\n",
      "Step 714 (879999) @ Episode 1962/10000, loss: 0.0079575907438993455\n",
      "Copied model parameters to target network.\n",
      "Step 1158 (880443) @ Episode 1962/10000, loss: 0.0109755564481019975\n",
      "Episode Reward: 26.0\n",
      "Step 814 (881257) @ Episode 1963/10000, loss: 0.0076548913493752485\n",
      "Episode Reward: 18.0\n",
      "Step 658 (881915) @ Episode 1964/10000, loss: 0.0135090202093124395\n",
      "Episode Reward: 13.0\n",
      "Step 1603 (883518) @ Episode 1965/10000, loss: 0.0172080341726541525\n",
      "Episode Reward: 40.0\n",
      "Step 1157 (884675) @ Episode 1966/10000, loss: 0.0255189724266529156\n",
      "Episode Reward: 32.0\n",
      "Step 954 (885629) @ Episode 1967/10000, loss: 0.0084064071998000145\n",
      "Episode Reward: 30.0\n",
      "Step 1022 (886651) @ Episode 1968/10000, loss: 0.0066196424886584285\n",
      "Episode Reward: 21.0\n",
      "Step 823 (887474) @ Episode 1969/10000, loss: 0.0165928788483142855\n",
      "Episode Reward: 18.0\n",
      "Step 1295 (888769) @ Episode 1970/10000, loss: 0.0095252133905887675\n",
      "Episode Reward: 26.0\n",
      "Step 709 (889478) @ Episode 1971/10000, loss: 0.0053535969927906996\n",
      "Episode Reward: 12.0\n",
      "Step 521 (889999) @ Episode 1972/10000, loss: 0.0088883060961961755\n",
      "Copied model parameters to target network.\n",
      "Step 903 (890381) @ Episode 1972/10000, loss: 0.0171899888664484026\n",
      "Episode Reward: 15.0\n",
      "Step 1187 (891568) @ Episode 1973/10000, loss: 0.0096838232129812242\n",
      "Episode Reward: 25.0\n",
      "Step 1830 (893398) @ Episode 1974/10000, loss: 0.0098523925989866263\n",
      "Episode Reward: 68.0\n",
      "Step 1038 (894436) @ Episode 1975/10000, loss: 0.0096259359270334245\n",
      "Episode Reward: 19.0\n",
      "Step 1138 (895574) @ Episode 1976/10000, loss: 0.0116931023076176645\n",
      "Episode Reward: 29.0\n",
      "Step 812 (896386) @ Episode 1977/10000, loss: 0.0097731426358222965\n",
      "Episode Reward: 17.0\n",
      "Step 980 (897366) @ Episode 1978/10000, loss: 0.0102276382967829755\n",
      "Episode Reward: 17.0\n",
      "Step 968 (898334) @ Episode 1979/10000, loss: 0.0059179202653467655\n",
      "Episode Reward: 17.0\n",
      "Step 936 (899270) @ Episode 1980/10000, loss: 0.0183115676045417853\n",
      "Episode Reward: 21.0\n",
      "Step 729 (899999) @ Episode 1981/10000, loss: 0.0076714185997843745\n",
      "Copied model parameters to target network.\n",
      "Step 1286 (900556) @ Episode 1981/10000, loss: 0.0138314664363861085\n",
      "Episode Reward: 26.0\n",
      "Step 1158 (901714) @ Episode 1982/10000, loss: 0.0133989583700895315\n",
      "Episode Reward: 22.0\n",
      "Step 945 (902659) @ Episode 1983/10000, loss: 0.0103041334077715874\n",
      "Episode Reward: 13.0\n",
      "Step 1149 (903808) @ Episode 1984/10000, loss: 0.0227357633411884354\n",
      "Episode Reward: 35.0\n",
      "Step 1076 (904884) @ Episode 1985/10000, loss: 0.0070265694521367555\n",
      "Episode Reward: 17.0\n",
      "Step 1106 (905990) @ Episode 1986/10000, loss: 0.0076640122570097455\n",
      "Episode Reward: 23.0\n",
      "Step 1154 (907144) @ Episode 1987/10000, loss: 0.0137450471520423895\n",
      "Episode Reward: 36.0\n",
      "Step 1237 (908381) @ Episode 1988/10000, loss: 0.0066059781238436735\n",
      "Episode Reward: 32.0\n",
      "Step 1496 (909877) @ Episode 1989/10000, loss: 0.0136311184614896775\n",
      "Episode Reward: 47.0\n",
      "Step 122 (909999) @ Episode 1990/10000, loss: 0.0215163733810186454\n",
      "Copied model parameters to target network.\n",
      "Step 1127 (911004) @ Episode 1990/10000, loss: 0.0126955639570951464\n",
      "Episode Reward: 20.0\n",
      "Step 1528 (912532) @ Episode 1991/10000, loss: 0.0117854988202452665\n",
      "Episode Reward: 42.0\n",
      "Step 1089 (913621) @ Episode 1992/10000, loss: 0.0210194773972034453\n",
      "Episode Reward: 18.0\n",
      "Step 1731 (915352) @ Episode 1993/10000, loss: 0.0128533486276865753\n",
      "Episode Reward: 42.0\n",
      "Step 1180 (916532) @ Episode 1994/10000, loss: 0.0053490563295781614\n",
      "Episode Reward: 24.0\n",
      "Step 839 (917371) @ Episode 1995/10000, loss: 0.0118753556162118913\n",
      "Episode Reward: 14.0\n",
      "Step 1040 (918411) @ Episode 1996/10000, loss: 0.0114181647077202812\n",
      "Episode Reward: 21.0\n",
      "Step 1315 (919726) @ Episode 1997/10000, loss: 0.0043017417192459115\n",
      "Episode Reward: 51.0\n",
      "Step 273 (919999) @ Episode 1998/10000, loss: 0.0194415096193552133\n",
      "Copied model parameters to target network.\n",
      "Step 823 (920549) @ Episode 1998/10000, loss: 0.0051146904006600385\n",
      "Episode Reward: 22.0\n",
      "Step 693 (921242) @ Episode 1999/10000, loss: 0.0127567891031503688\n",
      "Episode Reward: 14.0\n",
      "Step 754 (921996) @ Episode 2000/10000, loss: 0.0147519707679748546\n",
      "Episode Reward: 12.0\n",
      "Step 647 (922643) @ Episode 2001/10000, loss: 0.0130502358078956625\n",
      "Episode Reward: 11.0\n",
      "Step 977 (923620) @ Episode 2002/10000, loss: 0.0054614623077213765\n",
      "Episode Reward: 18.0\n",
      "Step 1297 (924917) @ Episode 2003/10000, loss: 0.0083408681675791745\n",
      "Episode Reward: 29.0\n",
      "Step 1070 (925987) @ Episode 2004/10000, loss: 0.0243666842579841685\n",
      "Episode Reward: 18.0\n",
      "Step 848 (926835) @ Episode 2005/10000, loss: 0.0074750347994267944\n",
      "Episode Reward: 11.0\n",
      "Step 873 (927708) @ Episode 2006/10000, loss: 0.0108423288911581045\n",
      "Episode Reward: 22.0\n",
      "Step 863 (928571) @ Episode 2007/10000, loss: 0.0149921830743551254\n",
      "Episode Reward: 16.0\n",
      "Step 1072 (929643) @ Episode 2008/10000, loss: 0.0083863092586398125\n",
      "Episode Reward: 22.0\n",
      "Step 356 (929999) @ Episode 2009/10000, loss: 0.0230994485318660745\n",
      "Copied model parameters to target network.\n",
      "Step 778 (930421) @ Episode 2009/10000, loss: 0.0113161467015743265\n",
      "Episode Reward: 16.0\n",
      "Step 1039 (931460) @ Episode 2010/10000, loss: 0.0078086340799927715\n",
      "Episode Reward: 28.0\n",
      "Step 807 (932267) @ Episode 2011/10000, loss: 0.0114865768700838096\n",
      "Episode Reward: 11.0\n",
      "Step 649 (932916) @ Episode 2012/10000, loss: 0.0173207111656665835\n",
      "Episode Reward: 8.0\n",
      "Step 782 (933698) @ Episode 2013/10000, loss: 0.0105189550668001175\n",
      "Episode Reward: 10.0\n",
      "Step 1227 (934925) @ Episode 2014/10000, loss: 0.0103128254413604745\n",
      "Episode Reward: 33.0\n",
      "Step 764 (935689) @ Episode 2015/10000, loss: 0.0183756016194820475\n",
      "Episode Reward: 16.0\n",
      "Step 961 (936650) @ Episode 2016/10000, loss: 0.0108686937019228945\n",
      "Episode Reward: 18.0\n",
      "Step 894 (937544) @ Episode 2017/10000, loss: 0.0079146064817905433\n",
      "Episode Reward: 19.0\n",
      "Step 1131 (938675) @ Episode 2018/10000, loss: 0.0059803449548780925\n",
      "Episode Reward: 30.0\n",
      "Step 1003 (939678) @ Episode 2019/10000, loss: 0.0054059759713709354\n",
      "Episode Reward: 23.0\n",
      "Step 321 (939999) @ Episode 2020/10000, loss: 0.0061112162657082085\n",
      "Copied model parameters to target network.\n",
      "Step 1041 (940719) @ Episode 2020/10000, loss: 0.006724168546497822\n",
      "Episode Reward: 24.0\n",
      "Step 632 (941351) @ Episode 2021/10000, loss: 0.0085948631167411875\n",
      "Episode Reward: 11.0\n",
      "Step 728 (942079) @ Episode 2022/10000, loss: 0.0077113443985581495\n",
      "Episode Reward: 11.0\n",
      "Step 1308 (943387) @ Episode 2023/10000, loss: 0.0110362302511930475\n",
      "Episode Reward: 37.0\n",
      "Step 694 (944081) @ Episode 2024/10000, loss: 0.0040487125515937805"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-535231217670>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m                                     \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                                     ser_coef=32):\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEpisode Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/secondary_experience_replay/SERI.py\u001b[0m in \u001b[0;36mdeep_q_learning\u001b[0;34m(sess, env, q_estimator, target_estimator, state_processor, num_episodes, experiment_dir, replay_memory_size, replay_memory_init_size, update_target_estimator_every, discount_factor, epsilon_start, epsilon_end, epsilon_decay_steps, batch_size, ser_coef, record_video_every)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;31m# Calculate q values and targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mq_values_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m             targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(\n\u001b[1;32m    298\u001b[0m                 q_values_next, axis=1)\n",
      "\u001b[0;32m~/Documents/secondary_experience_replay/SERI.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sess, s)\u001b[0m\n\u001b[1;32m    435\u001b[0m           \u001b[0maction\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \"\"\"\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_pl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/exp37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/exp37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/exp37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/exp37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/exp37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/exp37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "from SERI import StateProcessor, Estimator, ModelParametersCopier, make_epsilon_greedy_policy, deep_q_learning\n",
    "\n",
    "from reinforcementlearning.lib import plotting\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "env = gym.envs.make(\"Breakout-v0\")\n",
    "\n",
    "VALID_ACTIONS = [0, 1, 2, 3]\n",
    "\n",
    "# training\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments_seri/{}\".format(env.spec.id))\n",
    "\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope=\"q_estimator\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "# State processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "results = []\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                    env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    state_processor=state_processor,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    num_episodes=10000,\n",
    "                                    replay_memory_size=500000,\n",
    "                                    replay_memory_init_size=50000,\n",
    "                                    update_target_estimator_every=10000,\n",
    "                                    epsilon_start=1.0,\n",
    "                                    epsilon_end=0.1,\n",
    "                                    epsilon_decay_steps=500000,\n",
    "                                    discount_factor=0.99,\n",
    "                                    batch_size=40,\n",
    "                                    ser_coef=32):\n",
    "        results.append(stats.episode_rewards[-1])\n",
    "        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = {'Reward':  stats.episode_rewards}\n",
    "df = pd.DataFrame (data)\n",
    "\n",
    "rolling_mean = df.Reward.rolling(window=50).mean()\n",
    "\n",
    "plt.plot(df.index, df.Reward, label='SERI Reward')\n",
    "plt.plot(df.index, rolling_mean, label='SERI MA Reward', color='orange')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "df.to_csv('output_SERI.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
