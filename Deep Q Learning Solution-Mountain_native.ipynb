{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from reinforcementlearning.lib import plotting\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_ACTIONS = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(2,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[2], dtype=tf.uint8)\n",
    "            self.output = self.input_state\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "\n",
    "        # Placeholders for our input\n",
    "        # Our input are 4 grayscale frames of shape 84, 84 each\n",
    "        self.X_pl = tf.placeholder(shape=[None, 2], dtype=tf.float32, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # Fully connected layers\n",
    "        fc1 = tf.contrib.layers.fully_connected(self.X_pl, 16)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        # Calculate the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", self.loss),\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, 4, 84, 84, 1]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "\n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, 4, 84, 84, 1]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/aydarakhmetzyanov/opt/anaconda3/envs/exp37/lib/python3.7/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-14f9ef68b670>:49: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_global_step\n",
      "WARNING:tensorflow:From /Users/aydarakhmetzyanov/opt/anaconda3/envs/exp37/lib/python3.7/site-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "[[0.         0.         0.22390994]\n",
      " [0.         0.         0.22390994]]\n",
      "97.785965\n"
     ]
    }
   ],
   "source": [
    "# For Testing....\n",
    "\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "e = Estimator(scope=\"test\")\n",
    "sp = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Example observation batch\n",
    "    observation = env.reset()\n",
    "    \n",
    "    \n",
    "    observations = np.array([observation] * 2)\n",
    "    \n",
    "    \n",
    "    # Test Prediction\n",
    "    print(e.predict(sess, observations))\n",
    "\n",
    "    # Test training step\n",
    "    y = np.array([10.0, 10.0])\n",
    "    a = np.array([1, 2])\n",
    "    print(e.update(sess, observations, a, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParametersCopier():\n",
    "    \"\"\"\n",
    "    Copy model parameters of one estimator to another.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, estimator1, estimator2):\n",
    "        \"\"\"\n",
    "        Defines copy-work operation graph.  \n",
    "        Args:\n",
    "          estimator1: Estimator to copy the paramters from\n",
    "          estimator2: Estimator to copy the parameters to\n",
    "        \"\"\"\n",
    "        e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "        e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "        e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "        e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "        self.update_ops = []\n",
    "        for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "            op = e2_v.assign(e1_v)\n",
    "            self.update_ops.append(op)\n",
    "            \n",
    "    def make(self, sess):\n",
    "        \"\"\"\n",
    "        Makes copy.\n",
    "        Args:\n",
    "            sess: Tensorflow session instance\n",
    "        \"\"\"\n",
    "        sess.run(self.update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Gamma discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "    \n",
    "    # Make model copier object\n",
    "    estimator_copy = ModelParametersCopier(q_estimator, target_estimator)\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "    \n",
    "    # For 'system/' summaries, usefull to check if currrent process looks healthy\n",
    "    current_process = psutil.Process()\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "\n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator,\n",
    "        len(VALID_ACTIONS))\n",
    "\n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    #state = state_processor.process(sess, state)\n",
    "    #state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "        #next_state = state_processor.process(sess, next_state)\n",
    "        #next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            #state = state_processor.process(sess, state)\n",
    "            #state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        #state = state_processor.process(sess, state)\n",
    "        #state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "\n",
    "            # Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                estimator_copy.make(sess)\n",
    "                print(\"\\nCopied model parameters to target network.\")\n",
    "\n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Take a step\n",
    "            action_probs = policy(sess, state, epsilon)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "            #next_state = state_processor.process(sess, next_state)\n",
    "            #next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "\n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            # Save transition to replay memory\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            # Sample a minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "\n",
    "            # Calculate q values and targets\n",
    "            q_values_next = target_estimator.predict(sess, next_states_batch)\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n",
    "\n",
    "            # Perform gradient descent update\n",
    "            states_batch = np.array(states_batch)\n",
    "            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=epsilon, tag=\"episode/epsilon\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], tag=\"episode/reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], tag=\"episode/length\")\n",
    "        episode_summary.value.add(simple_value=current_process.cpu_percent(), tag=\"system/cpu_usage_percent\")\n",
    "        episode_summary.value.add(simple_value=current_process.memory_percent(memtype=\"vms\"), tag=\"system/v_memeory_usage_percent\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, i_episode)\n",
    "        q_estimator.summary_writer.flush()\n",
    "        \n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay memory...\n",
      "\n",
      "Copied model parameters to target network.\n",
      "Step 49 (9999) @ Episode 51/10000, loss: 0.90173321962356572\n",
      "Copied model parameters to target network.\n",
      "Step 99 (19999) @ Episode 101/10000, loss: 1.001633138656616\n",
      "Copied model parameters to target network.\n",
      "Step 149 (29999) @ Episode 151/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 0 (39999) @ Episode 202/10000, loss: None0\n",
      "Copied model parameters to target network.\n",
      "Step 50 (49999) @ Episode 252/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 100 (59999) @ Episode 302/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 150 (69999) @ Episode 352/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 1 (79999) @ Episode 403/10000, loss: 1.0e0\n",
      "Copied model parameters to target network.\n",
      "Step 51 (89999) @ Episode 453/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 101 (99999) @ Episode 503/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 151 (109999) @ Episode 553/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 2 (119999) @ Episode 604/10000, loss: 1.0e0\n",
      "Copied model parameters to target network.\n",
      "Step 52 (129999) @ Episode 654/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 102 (139999) @ Episode 704/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 152 (149999) @ Episode 754/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 3 (159999) @ Episode 805/10000, loss: 1.0e0\n",
      "Copied model parameters to target network.\n",
      "Step 53 (169999) @ Episode 855/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 103 (179999) @ Episode 905/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 153 (189999) @ Episode 955/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 4 (199999) @ Episode 1006/10000, loss: 1.0e0\n",
      "Copied model parameters to target network.\n",
      "Step 54 (209999) @ Episode 1056/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 104 (219999) @ Episode 1106/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 154 (229999) @ Episode 1156/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 5 (239999) @ Episode 1207/10000, loss: 1.0e0\n",
      "Copied model parameters to target network.\n",
      "Step 55 (249999) @ Episode 1257/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 105 (259999) @ Episode 1307/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 155 (269999) @ Episode 1357/10000, loss: 1.0999673962593079\n",
      "Copied model parameters to target network.\n",
      "Step 6 (279999) @ Episode 1408/10000, loss: 1.0e0\n",
      "Copied model parameters to target network.\n",
      "Step 56 (289999) @ Episode 1458/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 106 (299999) @ Episode 1508/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 156 (309999) @ Episode 1558/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 7 (319999) @ Episode 1609/10000, loss: 1.0e0\n",
      "Copied model parameters to target network.\n",
      "Step 57 (329999) @ Episode 1659/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 107 (339999) @ Episode 1709/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 157 (349999) @ Episode 1759/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 8 (359999) @ Episode 1810/10000, loss: 1.0e0\n",
      "Copied model parameters to target network.\n",
      "Step 58 (369999) @ Episode 1860/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 108 (379999) @ Episode 1910/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 158 (389999) @ Episode 1960/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 9 (399999) @ Episode 2011/10000, loss: 1.0e0\n",
      "Copied model parameters to target network.\n",
      "Step 59 (409999) @ Episode 2061/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 109 (419999) @ Episode 2111/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 159 (429999) @ Episode 2161/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 10 (439999) @ Episode 2212/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 60 (449999) @ Episode 2262/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 110 (459999) @ Episode 2312/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 160 (469999) @ Episode 2362/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 11 (479999) @ Episode 2413/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 61 (489999) @ Episode 2463/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 111 (499999) @ Episode 2513/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 161 (509999) @ Episode 2563/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 12 (519999) @ Episode 2614/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 62 (529999) @ Episode 2664/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 112 (539999) @ Episode 2714/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 162 (549999) @ Episode 2764/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 13 (559999) @ Episode 2815/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 63 (569999) @ Episode 2865/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 113 (579999) @ Episode 2915/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 163 (589999) @ Episode 2965/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 14 (599999) @ Episode 3016/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 64 (609999) @ Episode 3066/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 114 (619999) @ Episode 3116/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 164 (629999) @ Episode 3166/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 15 (639999) @ Episode 3217/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 65 (649999) @ Episode 3267/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 115 (659999) @ Episode 3317/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 165 (669999) @ Episode 3367/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 16 (679999) @ Episode 3418/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 66 (689999) @ Episode 3468/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 116 (699999) @ Episode 3518/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 166 (709999) @ Episode 3568/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 17 (719999) @ Episode 3619/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 67 (729999) @ Episode 3669/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 117 (739999) @ Episode 3719/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 167 (749999) @ Episode 3769/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 18 (759999) @ Episode 3820/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 68 (769999) @ Episode 3870/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 118 (779999) @ Episode 3920/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 168 (789999) @ Episode 3970/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 19 (799999) @ Episode 4021/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 69 (809999) @ Episode 4071/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 119 (819999) @ Episode 4121/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 169 (829999) @ Episode 4171/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 20 (839999) @ Episode 4222/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 70 (849999) @ Episode 4272/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 120 (859999) @ Episode 4322/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 170 (869999) @ Episode 4372/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 21 (879999) @ Episode 4423/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 71 (889999) @ Episode 4473/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 121 (899999) @ Episode 4523/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 171 (909999) @ Episode 4573/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 22 (919999) @ Episode 4624/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 72 (929999) @ Episode 4674/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 122 (939999) @ Episode 4724/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 172 (949999) @ Episode 4774/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 23 (959999) @ Episode 4825/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 73 (969999) @ Episode 4875/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 123 (979999) @ Episode 4925/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 173 (989999) @ Episode 4975/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 24 (999999) @ Episode 5026/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 74 (1009999) @ Episode 5076/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 124 (1019999) @ Episode 5126/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 174 (1029999) @ Episode 5176/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 25 (1039999) @ Episode 5227/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 75 (1049999) @ Episode 5277/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 125 (1059999) @ Episode 5327/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 175 (1069999) @ Episode 5377/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 26 (1079999) @ Episode 5428/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 76 (1089999) @ Episode 5478/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 126 (1099999) @ Episode 5528/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 176 (1109999) @ Episode 5578/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 27 (1119999) @ Episode 5629/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 77 (1129999) @ Episode 5679/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 127 (1139999) @ Episode 5729/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 177 (1149999) @ Episode 5779/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 28 (1159999) @ Episode 5830/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 78 (1169999) @ Episode 5880/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 128 (1179999) @ Episode 5930/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 178 (1189999) @ Episode 5980/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 29 (1199999) @ Episode 6031/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 79 (1209999) @ Episode 6081/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 129 (1219999) @ Episode 6131/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 179 (1229999) @ Episode 6181/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 30 (1239999) @ Episode 6232/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 80 (1249999) @ Episode 6282/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 130 (1259999) @ Episode 6332/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 180 (1269999) @ Episode 6382/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 31 (1279999) @ Episode 6433/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 81 (1289999) @ Episode 6483/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 131 (1299999) @ Episode 6533/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 181 (1309999) @ Episode 6583/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 32 (1319999) @ Episode 6634/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 82 (1329999) @ Episode 6684/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 132 (1339999) @ Episode 6734/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 182 (1349999) @ Episode 6784/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 33 (1359999) @ Episode 6835/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 83 (1369999) @ Episode 6885/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 133 (1379999) @ Episode 6935/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 183 (1389999) @ Episode 6985/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 34 (1399999) @ Episode 7036/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 84 (1409999) @ Episode 7086/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 134 (1419999) @ Episode 7136/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 184 (1429999) @ Episode 7186/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 35 (1439999) @ Episode 7237/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 85 (1449999) @ Episode 7287/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 135 (1459999) @ Episode 7337/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 185 (1469999) @ Episode 7387/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 36 (1479999) @ Episode 7438/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 86 (1489999) @ Episode 7488/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 136 (1499999) @ Episode 7538/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 186 (1509999) @ Episode 7588/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 37 (1519999) @ Episode 7639/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 87 (1529999) @ Episode 7689/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 137 (1539999) @ Episode 7739/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 187 (1549999) @ Episode 7789/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 38 (1559999) @ Episode 7840/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 88 (1569999) @ Episode 7890/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 138 (1579999) @ Episode 7940/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 188 (1589999) @ Episode 7990/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 39 (1599999) @ Episode 8041/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 89 (1609999) @ Episode 8091/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 139 (1619999) @ Episode 8141/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 189 (1629999) @ Episode 8191/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 40 (1639999) @ Episode 8242/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 90 (1649999) @ Episode 8292/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 140 (1659999) @ Episode 8342/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 190 (1669999) @ Episode 8392/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 41 (1679999) @ Episode 8443/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 91 (1689999) @ Episode 8493/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 141 (1699999) @ Episode 8543/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 191 (1709999) @ Episode 8593/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 42 (1719999) @ Episode 8644/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 92 (1729999) @ Episode 8694/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 142 (1739999) @ Episode 8744/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 192 (1749999) @ Episode 8794/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 43 (1759999) @ Episode 8845/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 93 (1769999) @ Episode 8895/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 143 (1779999) @ Episode 8945/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 193 (1789999) @ Episode 8995/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 44 (1799999) @ Episode 9046/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 94 (1809999) @ Episode 9096/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 144 (1819999) @ Episode 9146/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 194 (1829999) @ Episode 9196/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 45 (1839999) @ Episode 9247/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 95 (1849999) @ Episode 9297/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 145 (1859999) @ Episode 9347/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 195 (1869999) @ Episode 9397/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 46 (1879999) @ Episode 9448/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 96 (1889999) @ Episode 9498/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 146 (1899999) @ Episode 9548/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 196 (1909999) @ Episode 9598/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 47 (1919999) @ Episode 9649/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 97 (1929999) @ Episode 9699/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 147 (1939999) @ Episode 9749/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 197 (1949999) @ Episode 9799/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 48 (1959999) @ Episode 9850/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 98 (1969999) @ Episode 9900/10000, loss: 1.00\n",
      "Copied model parameters to target network.\n",
      "Step 148 (1979999) @ Episode 9950/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 198 (1989999) @ Episode 10000/10000, loss: 1.0\n",
      "Copied model parameters to target network.\n",
      "Step 199 (1990000) @ Episode 10000/10000, loss: 1.0"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments_mountain/{}\".format(env.spec.id))\n",
    "\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope=\"q_estimator\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "# State processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                    env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    state_processor=state_processor,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    num_episodes=10000,\n",
    "                                    replay_memory_size=500000,\n",
    "                                    replay_memory_init_size=50000,\n",
    "                                    update_target_estimator_every=10000,\n",
    "                                    epsilon_start=1.0,\n",
    "                                    epsilon_end=0.1,\n",
    "                                    epsilon_decay_steps=500000,\n",
    "                                    discount_factor=0.99,\n",
    "                                    batch_size=64):\n",
    "        \n",
    "        if(stats.episode_rewards[-1]>-200):\n",
    "            print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = {'Reward':  stats.episode_rewards}\n",
    "df = pd.DataFrame (data)\n",
    "\n",
    "rolling_mean = df.Reward.rolling(window=50).mean()\n",
    "\n",
    "plt.plot(df.index, df.Reward, label='DQN Reward')\n",
    "plt.plot(df.index, rolling_mean, label='DQN MA Reward', color='orange')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "df.to_csv('output_DQN_mountain.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
